{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       " <style>\n",
       "    div#notebook-container    { width: 95%; }\n",
       "    div#menubar-container     { width: 65%; }\n",
       "    div#maintoolbar-container { width: 99%; }\n",
       " </style>\n",
       " "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# downloading all modules that we will need for the job\n",
    "# it will be in conda environment|\n",
    "import os\n",
    "\n",
    "# import all required modules\n",
    "import Bio\n",
    "from Bio.PDB import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re  # Regular expression operations\n",
    "import argparse\n",
    "import time\n",
    "import timeit\n",
    "import signal\n",
    "import sys\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import gzip\n",
    "import xml.etree.ElementTree as ET\n",
    "import urllib\n",
    "from datetime import date\n",
    "import math\n",
    "from string import punctuation\n",
    "import platform\n",
    "from platform import python_version\n",
    "from pathlib import Path\n",
    "import tqdm\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "from multiprocessing import Process\n",
    "from multiprocessing import Pool\n",
    "from lxml import html\n",
    "import shutil\n",
    "\n",
    "low_memory=False\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.options.display.max_rows = 10000\n",
    "default_mmCIF_num = 50000\n",
    "### script below can increase width of cells in jupyter-notebook\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data=\"\"\"\n",
    " <style>\n",
    "    div#notebook-container    { width: 95%; }\n",
    "    div#menubar-container     { width: 65%; }\n",
    "    div#maintoolbar-container { width: 99%; }\n",
    " </style>\n",
    " \"\"\"))\n",
    "\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import requests\n",
    "from functools import partial,reduce, cmp_to_key\n",
    "from concurrent.futures import as_completed, ProcessPoolExecutor, ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "def default_path():\n",
    "    default_input_path_to_mmCIF = current_directory + \"/mmCIF\"\n",
    "    default_input_path_to_PDB = current_directory + \"/PDB\"\n",
    "    default_input_path_to_SIFTS = current_directory + \"/SIFTS\"\n",
    "    default_output_path_to_mmCIF = current_directory + \"/output_mmCIF\"\n",
    "    default_output_path_to_PDB = current_directory + \"/output_PDB\"\n",
    "    default_input_path_to_mmCIF_assembly = current_directory + \"/mmCIF_assembly\"\n",
    "    default_input_path_to_PDB_assembly = current_directory + \"/PDB_assembly\"\n",
    "    default_output_path_to_mmCIF_assembly = current_directory + \"/output_mmCIF_assembly\"\n",
    "    default_output_path_to_PDB_assembly = current_directory + \"/output_PDB_assembly\"\n",
    "    return [default_input_path_to_mmCIF, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "            default_output_path_to_mmCIF, default_output_path_to_PDB, default_input_path_to_mmCIF_assembly,\n",
    "            default_input_path_to_PDB_assembly,default_output_path_to_mmCIF_assembly,\n",
    "            default_output_path_to_PDB_assembly]\n",
    "\n",
    "\n",
    "default_paths = default_path()\n",
    "default_input_path_to_mmCIF = default_paths[0]\n",
    "default_input_path_to_PDB = default_paths[1]\n",
    "default_input_path_to_SIFTS = default_paths[2]\n",
    "default_output_path_to_mmCIF = default_paths[3]\n",
    "default_output_path_to_PDB = default_paths[4]\n",
    "default_input_path_to_mmCIF_assembly = default_paths[5]\n",
    "default_input_path_to_PDB_assembly = default_paths[6]\n",
    "default_output_path_to_mmCIF_assembly = default_paths[7]\n",
    "default_output_path_to_PDB_assembly = default_paths[8]\n",
    "gzip_mode = \"on\"\n",
    "exception_AccessionIDs = [\"P42212\", \"Q17104\", \"Q27903\", \"Q93125\", \"P03069\", \"D3DLN9\", \"Q96UT3\", \"P0ABE7\", \"P00192\", \"P76805\", \"Q8XCE3\", \"P00720\", \"Q38170\", \"Q94N07\", \"P0AEX9\", \"P02928\", \"Q2M6S0\"]\n",
    "nproc = None\n",
    "default_mmCIF_num = 50000\n",
    "default_PDB_num = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_what_is_inside(format_to_look_at,\n",
    "                        default_input_path_to_mmCIF=current_directory + \"/mmCIF\",\n",
    "                        default_input_path_to_PDB=current_directory + \"/PDB\",\n",
    "                        default_input_path_to_SIFTS=current_directory + \"/SIFTS\",\n",
    "                        default_output_path_to_mmCIF=current_directory + \"/output_mmCIF\",\n",
    "                        default_output_path_to_PDB=current_directory + \"/output_PDB\",\n",
    "                        default_input_path_to_mmCIF_assembly = current_directory + \"/mmCIF_assembly\",\n",
    "                        default_input_path_to_PDB_assembly = current_directory + \"/PDB_assembly\",\n",
    "                        default_output_path_to_mmCIF_assembly=current_directory + \"/output_mmCIF_assembly\",\n",
    "                        default_output_path_to_PDB_assembly=current_directory + \"/output_PDB_assembly\"):\n",
    "    if format_to_look_at == \"SIFTS\":\n",
    "        if not os.path.exists(default_input_path_to_SIFTS):\n",
    "            os.makedirs(default_input_path_to_SIFTS)\n",
    "        result = [f for f in listdir(default_input_path_to_SIFTS) if isfile(join(default_input_path_to_SIFTS, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"mmCIF\":\n",
    "        if not os.path.exists(default_input_path_to_mmCIF):\n",
    "            os.makedirs(default_input_path_to_mmCIF)\n",
    "        result = [f for f in listdir(default_input_path_to_mmCIF) if isfile(join(default_input_path_to_mmCIF, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"PDB\":\n",
    "        if not os.path.exists(default_input_path_to_PDB):\n",
    "            os.makedirs(default_input_path_to_PDB)\n",
    "        result = [f for f in listdir(default_input_path_to_PDB) if isfile(join(default_input_path_to_PDB, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"output_mmCIF\":\n",
    "        if not os.path.exists(default_output_path_to_mmCIF):\n",
    "            os.makedirs(default_output_path_to_mmCIF)\n",
    "        result = [f for f in listdir(default_output_path_to_mmCIF) if isfile(join(default_output_path_to_mmCIF, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"output_PDB\":\n",
    "        if not os.path.exists(default_output_path_to_PDB):\n",
    "            os.makedirs(default_output_path_to_PDB)\n",
    "        result = [f for f in listdir(default_output_path_to_PDB) if isfile(join(default_output_path_to_PDB, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"mmCIF_assembly\":\n",
    "        if not os.path.exists(default_input_path_to_mmCIF_assembly):\n",
    "            os.makedirs(default_input_path_to_mmCIF_assembly)\n",
    "        result = [f for f in listdir(default_input_path_to_mmCIF_assembly) if isfile(join(default_input_path_to_mmCIF_assembly, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"PDB_assembly\":\n",
    "        if not os.path.exists(default_input_path_to_PDB_assembly):\n",
    "            os.makedirs(default_input_path_to_PDB_assembly)\n",
    "        result = [f for f in listdir(default_input_path_to_PDB_assembly) if isfile(join(default_input_path_to_PDB_assembly, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"output_mmCIF_assembly\":\n",
    "        if not os.path.exists(default_output_path_to_mmCIF_assembly):\n",
    "            os.makedirs(default_output_path_to_mmCIF_assembly)\n",
    "        result = [f for f in listdir(default_output_path_to_mmCIF_assembly) if isfile(join(default_output_path_to_mmCIF_assembly, f))]\n",
    "        return result\n",
    "    if format_to_look_at == \"output_PDB_assembly\":\n",
    "        if not os.path.exists(default_output_path_to_PDB_assembly):\n",
    "            os.makedirs(default_output_path_to_PDB_assembly)\n",
    "        result = [f for f in listdir(default_output_path_to_PDB_assembly) if isfile(join(default_output_path_to_PDB_assembly, f))]\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.download.modules import *\n",
    "from src.download import compressor\n",
    "from src.download.lookfilesinside import look_what_is_inside\n",
    "from src.download.downloadwithThreadPool import url_formation_for_pool, download_with_pool, download_pdb_assemblies_list_with_lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMARK_mmCIF = [\"#\\n\",\n",
    "                \"loop_\\n\",\n",
    "                \"_database_PDB_remark.id       1\\n\",\n",
    "                \"_database_PDB_remark.text\\n\",\n",
    "                \";File processed by PDBrenum: http://dunbrack3.fccc.edu/PDBrenum\\n\",\n",
    "                \"Author sequence numbering is replaced with UniProt numbering according to\\n\",\n",
    "                \"alignment by SIFTS (https://www.ebi.ac.uk/pdbe/docs/sifts/).\\n\",\n",
    "                \"Only chains with UniProt sequences in SIFTS are renumbered.\\n\",\n",
    "                \"Residues in UniProt chains without UniProt residue numbers in SIFTS\\n\",\n",
    "                \"(e.g., sequence tags) are given residue numbers 50000+label_seq_id\\n\",\n",
    "                \"(where label_seq_id is the 1-to-N residue numbering of each chain.\\n\",\n",
    "                \"Ligands are numbered 50000+their residue number in the original file.\\n\",\n",
    "                \"The _poly_seq_scheme table contains a correspondence between the\\n\",\n",
    "                \"1-to-N sequence (seq_id), the new numbering based on UniProt (pdb_seq_num =\\n\",\n",
    "                \"auth_seq_id in the _atom_site records), and the author numbering\\n\",\n",
    "                \"in the original mmCIF file from the PDB (auth_seq_num).\\n\",\n",
    "                \";\\n\",\n",
    "                \"#\\n\"] \n",
    "\n",
    "\n",
    "def try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name):    \n",
    "    mmcif_dict = 0\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            mmcif_dict = Bio.PDB.MMCIF2Dict.MMCIF2Dict(gzip.open(Path(str(default_input_path_to_mmCIF) + \"/\" + mmCIF_name), 'rt'))\n",
    "            break\n",
    "        except EOFError:\n",
    "            os.remove(Path(str(default_input_path_to_mmCIF) + \"/\" + mmCIF_name))\n",
    "            if \"assembly\" in mmCIF_name: \n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF_assembly\", [mmCIF_name])[0])\n",
    "            else:\n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF\", [mmCIF_name])[0])\n",
    "        except ValueError:\n",
    "            os.remove(Path(str(default_input_path_to_mmCIF) + \"/\" + mmCIF_name))\n",
    "            if \"assembly\" in mmCIF_name: \n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF_assembly\", [mmCIF_name])[0])\n",
    "            else:\n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF\", [mmCIF_name])[0])\n",
    "        except OSError:\n",
    "            if \"assembly\" in mmCIF_name: \n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF_assembly\", [mmCIF_name])[0])\n",
    "            else:\n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF\", [mmCIF_name])[0])       \n",
    "    return mmcif_dict\n",
    "\n",
    "def try_SIFTS_tree_parser(default_input_path_to_SIFTS, SIFTS_name):\n",
    "    product_tree_SIFTS = 0\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            handle_SIFTS = gzip.open(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name), 'rt')\n",
    "            product_tree_SIFTS = SIFTS_tree_parser(handle_SIFTS)\n",
    "            break\n",
    "        except EOFError:\n",
    "            os.remove(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name))\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])\n",
    "        except ValueError:\n",
    "            os.remove(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name))\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])     \n",
    "        except OSError:\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])\n",
    "        except:\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])\n",
    "    return product_tree_SIFTS\n",
    "\n",
    "def output_with_this_name_ending(name_ending, path, mmcif_dict, mmCIF_name, gzip_mode=gzip_mode, current_directory=current_directory):\n",
    "    mmCIF_name = mmCIF_name[:mmCIF_name.rfind(\".cif.gz\")]\n",
    "    os.chdir(path)\n",
    "    io = MMCIFIO()\n",
    "    io.set_dict(mmcif_dict)\n",
    "    io.save(mmCIF_name + name_ending)\n",
    "    if gzip_mode == \"on\":\n",
    "        compressor.compress_output_files(mmCIF_name + name_ending, gzip_mode)\n",
    "        os.remove(mmCIF_name + name_ending)\n",
    "    os.chdir(current_directory)\n",
    "\n",
    "    \n",
    "def copy_file(inpath, file_name, outpath, postfix, gzip_mode):\n",
    "    mmCIF_name = file_name[:file_name.rfind(\".cif.gz\")]\n",
    "    absolute_path_in = inpath + \"/\" + file_name\n",
    "    absolute_path_out = outpath + \"/\" + mmCIF_name + postfix\n",
    "    if gzip_mode == \"off\":\n",
    "        with gzip.open(absolute_path_in, 'rb') as f_in:\n",
    "            with open(absolute_path_out[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        shutil.copyfile(absolute_path_in, absolute_path_out)\n",
    "        \n",
    "        \n",
    "def if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message):\n",
    "    strand_id_set = set()\n",
    "    try:\n",
    "        pull_chains_for_chains_count = mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_strand_id\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            pull_chains_for_chains_count = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_strand_id\"]\n",
    "        except KeyError:\n",
    "            pull_chains_for_chains_count = mmcif_dict[\"_atom_site.auth_asym_id\"]\n",
    "\n",
    "    for strand in pull_chains_for_chains_count:\n",
    "        strand_id_set.add(strand)\n",
    "    strand_id_set = list(strand_id_set)\n",
    "    strand_id_set.sort()\n",
    "    for strand in strand_id_set:\n",
    "        count_elements_in_strand = 0\n",
    "        for chain_id in pull_chains_for_chains_count:\n",
    "            if chain_id == strand:\n",
    "                count_elements_in_strand += 1\n",
    "        log_message.append([mmCIF_name[:4], strand, \"-\", \"-\", \"-\", \"-\", count_elements_in_strand, \"0\", \"0\"])\n",
    "    return log_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handling_chain_numbering_clashes(df_PDBe_PDB_UniProt, exception_AccessionIDs):\n",
    "    chains_to_change = set()\n",
    "    chains_to_change_one_to_end = set()\n",
    "    AccessionIDs = set()\n",
    "    chain_AccessionID_dict = dict()\n",
    "\n",
    "    for PDBe_num_UniProt_PDB_accession in df_PDBe_PDB_UniProt[\"Three_Rows_CIF_Num_Uni\"]:\n",
    "        if type(PDBe_num_UniProt_PDB_accession[4]) == float:\n",
    "            continue\n",
    "        chains_to_change.add(PDBe_num_UniProt_PDB_accession[3][2])\n",
    "        chains_to_change_one_to_end.add(PDBe_num_UniProt_PDB_accession[2][2])\n",
    "        AccessionIDs.add(PDBe_num_UniProt_PDB_accession[4])\n",
    "\n",
    "    for chains in chains_to_change:\n",
    "        accessions_in_chain = set()\n",
    "        for PDBe_num_UniProt_PDB_accession in df_PDBe_PDB_UniProt[\"Three_Rows_CIF_Num_Uni\"]:\n",
    "            if chains == PDBe_num_UniProt_PDB_accession[3][2]:\n",
    "                if PDBe_num_UniProt_PDB_accession[4] is not np.nan:\n",
    "                    accessions_in_chain.add(PDBe_num_UniProt_PDB_accession[4])\n",
    "        chain_AccessionID_dict[chains] = accessions_in_chain\n",
    "\n",
    "    tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = list()\n",
    "    combined_tuple_PDBe_UniProt_AccessionID = list()\n",
    "    longest_AccessionID_list = list()\n",
    "    clash = 0\n",
    "\n",
    "    for chain_accession in chain_AccessionID_dict.items():\n",
    "        chains_to_change_for_AccessionID = list()\n",
    "        longest_AccessionID = None\n",
    "        longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = list()\n",
    "        if len(chain_accession[1]) > 1:\n",
    "            for accessions in chain_accession[1]:\n",
    "                tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = list()\n",
    "                target_UniProt_numbers_in_chain = list()\n",
    "                diff_another_UniProt_numbers_in_same_chain = list()\n",
    "                diff_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = list()\n",
    "\n",
    "                for PDBe_num_UniProt_PDB_accession in df_PDBe_PDB_UniProt[\"Three_Rows_CIF_Num_Uni\"]:\n",
    "                    if PDBe_num_UniProt_PDB_accession[4] == accessions and PDBe_num_UniProt_PDB_accession[3][2] == chain_accession[0] and \\\n",
    "                            PDBe_num_UniProt_PDB_accession[4] is not np.nan:\n",
    "                        tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID.append(\n",
    "                            (PDBe_num_UniProt_PDB_accession[0], PDBe_num_UniProt_PDB_accession[2], PDBe_num_UniProt_PDB_accession[4]))\n",
    "                        target_UniProt_numbers_in_chain.append(PDBe_num_UniProt_PDB_accession[2])\n",
    "                        chains_to_change_for_AccessionID.append(PDBe_num_UniProt_PDB_accession[3][2])\n",
    "                    if PDBe_num_UniProt_PDB_accession[4] != accessions and PDBe_num_UniProt_PDB_accession[3][2] == chain_accession[0] and \\\n",
    "                            PDBe_num_UniProt_PDB_accession[4] is not np.nan:\n",
    "                        diff_another_UniProt_numbers_in_same_chain.append(PDBe_num_UniProt_PDB_accession[2])\n",
    "                        diff_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID.append(\n",
    "                            (PDBe_num_UniProt_PDB_accession[0], PDBe_num_UniProt_PDB_accession[2], PDBe_num_UniProt_PDB_accession[4]))\n",
    "\n",
    "                for target_Uni in target_UniProt_numbers_in_chain:\n",
    "                    for diff_Uni in diff_another_UniProt_numbers_in_same_chain:\n",
    "                        if target_Uni[0] == diff_Uni[0]:\n",
    "                            clash = 1\n",
    "\n",
    "                if accessions not in exception_AccessionIDs:\n",
    "                    if len(longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID) < len(\n",
    "                            tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID):\n",
    "                        longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID\n",
    "                        longest_AccessionID = accessions\n",
    "\n",
    "                if longest_AccessionID is None:\n",
    "                    if len(longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID) < len(\n",
    "                            tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID):\n",
    "                        longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID\n",
    "                        longest_AccessionID = accessions\n",
    "\n",
    "            if clash == 1:\n",
    "                combined_tuple_PDBe_UniProt_AccessionID.extend(longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID)\n",
    "                longest_AccessionID_list.append(longest_AccessionID)\n",
    "            else:\n",
    "                combined_tuple_PDBe_UniProt_AccessionID.extend(longest_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID)\n",
    "                combined_tuple_PDBe_UniProt_AccessionID.extend(diff_tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID)\n",
    "        else:\n",
    "            for accessions in chain_accession[1]:\n",
    "                tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID = list()\n",
    "                target_UniProt_numbers_in_chain = list()\n",
    "\n",
    "                for PDBe_num_UniProt_PDB_accession in df_PDBe_PDB_UniProt[\"Three_Rows_CIF_Num_Uni\"]:\n",
    "                    if PDBe_num_UniProt_PDB_accession[4] == accessions and PDBe_num_UniProt_PDB_accession[3][2] == chain_accession[0] and PDBe_num_UniProt_PDB_accession[4] is not np.nan:\n",
    "                        tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID.append((PDBe_num_UniProt_PDB_accession[0], PDBe_num_UniProt_PDB_accession[2], PDBe_num_UniProt_PDB_accession[4]))\n",
    "                        target_UniProt_numbers_in_chain.append(PDBe_num_UniProt_PDB_accession[2])\n",
    "                        chains_to_change_for_AccessionID.append(PDBe_num_UniProt_PDB_accession[3][2])\n",
    "            combined_tuple_PDBe_UniProt_AccessionID.extend(tuple_PDBe_for_UniProt_and_tuple_UniProt_for_AccessionID)\n",
    "\n",
    "    return [chains_to_change, combined_tuple_PDBe_UniProt_AccessionID, AccessionIDs, longest_AccessionID_list, chains_to_change_one_to_end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def renumbered_count_in_chains(chains_to_change_one_to_end, df_PDBe_PDB_UniProt_without_null_index_PDBe, mmCIF_name,\n",
    "                               UniProt_conversion_dict, longest_AccessionID_list):\n",
    "    nothing_changed = 1\n",
    "    chain_total_renum = list()\n",
    "    UniProt_total_renum = list()\n",
    "    renum_for_all_chains = 0\n",
    "    total_renum5000 = 0\n",
    "    chains_to_change = sorted(chains_to_change_one_to_end)\n",
    "    chain_PDBe_PDB = dict()\n",
    "    prot_len = len(df_PDBe_PDB_UniProt_without_null_index_PDBe[\"Three_Rows_CIF_Num_Uni\"])\n",
    "    \n",
    "    for chain in chains_to_change:\n",
    "        total_count_per_chain = 0\n",
    "        renum_for_the_chains = 0\n",
    "        renum5000 = 0\n",
    "        UniProts_set = set()\n",
    "\n",
    "        for PDBe_num_Uni_PDB in df_PDBe_PDB_UniProt_without_null_index_PDBe[\"Three_Rows_CIF_Num_Uni\"]:\n",
    "            if chain == PDBe_num_Uni_PDB[2][2]:\n",
    "                chain_PDBe_PDB[chain] = PDBe_num_Uni_PDB[3][2]\n",
    "                if type(PDBe_num_Uni_PDB[4]) != float:\n",
    "                    UniProts_set.add(PDBe_num_Uni_PDB[4])\n",
    "                total_count_per_chain += 1\n",
    "                if int(PDBe_num_Uni_PDB[1]) > 5000:\n",
    "                    renum5000 += 1\n",
    "                    total_renum5000 += 1\n",
    "                elif PDBe_num_Uni_PDB[1] != PDBe_num_Uni_PDB[3][0]:\n",
    "                    renum_for_all_chains += 1\n",
    "                    renum_for_the_chains += 1\n",
    "\n",
    "        for accession in UniProts_set:\n",
    "            renum_for_accession = 0\n",
    "            coun_accession_len = 0\n",
    "            for PDBe_num_Uni_PDB in df_PDBe_PDB_UniProt_without_null_index_PDBe[\"Three_Rows_CIF_Num_Uni\"]:\n",
    "                if accession == PDBe_num_Uni_PDB[4]:\n",
    "                    if chain == PDBe_num_Uni_PDB[2][2]: \n",
    "                        coun_accession_len +=1\n",
    "                if chain == PDBe_num_Uni_PDB[2][2] and accession == PDBe_num_Uni_PDB[4]:\n",
    "                    if PDBe_num_Uni_PDB[1] != PDBe_num_Uni_PDB[3][0]:\n",
    "                        renum_for_accession += 1\n",
    "\n",
    "            if len(longest_AccessionID_list) != 0:\n",
    "                if accession in longest_AccessionID_list:\n",
    "                    AccessionID_humanread_longest = UniProt_conversion_dict.get(accession)\n",
    "                    chain_total_renum.append(\n",
    "                        [mmCIF_name[:4] + \"*\", chain, chain_PDBe_PDB[chain], accession, AccessionID_humanread_longest, coun_accession_len, total_count_per_chain, renum_for_accession, renum5000])\n",
    "                else:\n",
    "                    AccessionID_humanread = UniProt_conversion_dict.get(accession)\n",
    "                    chain_total_renum.append(\n",
    "                        [mmCIF_name[:4], chain, chain_PDBe_PDB[chain], accession, AccessionID_humanread, coun_accession_len, total_count_per_chain, renum_for_accession, renum5000])\n",
    "            else:\n",
    "                if type(accession) != float:\n",
    "                    AccessionID_humanread = UniProt_conversion_dict.get(accession)\n",
    "                    chain_total_renum.append(\n",
    "                        [mmCIF_name[:4], chain, chain_PDBe_PDB[chain], accession, AccessionID_humanread, coun_accession_len, total_count_per_chain, renum_for_accession, renum5000])\n",
    "\n",
    "    if renum_for_all_chains == 0 and total_renum5000 == 0:\n",
    "        nothing_changed = 0\n",
    "\n",
    "    return [chain_total_renum, nothing_changed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMARK_mmCIF = [\"#\\n\",\n",
    "                \"loop_\\n\",\n",
    "                \"_database_PDB_remark.id       1\\n\",\n",
    "                \"_database_PDB_remark.text\\n\",\n",
    "                \";File processed by PDBrenum: http://dunbrack3.fccc.edu/PDBrenum\\n\",\n",
    "                \"Author sequence numbering is replaced with UniProt numbering according to\\n\",\n",
    "                \"alignment by SIFTS (https://www.ebi.ac.uk/pdbe/docs/sifts/).\\n\",\n",
    "                \"Only chains with UniProt sequences in SIFTS are renumbered.\\n\",\n",
    "                \"Residues in UniProt chains without UniProt residue numbers in SIFTS\\n\",\n",
    "                \"(e.g., sequence tags) are given residue numbers 50000+label_seq_id\\n\",\n",
    "                \"(where label_seq_id is the 1-to-N residue numbering of each chain.\\n\",\n",
    "                \"Ligands are numbered 50000+their residue number in the original file.\\n\",\n",
    "                \"The _poly_seq_scheme table contains a correspondence between the\\n\",\n",
    "                \"1-to-N sequence (seq_id), the new numbering based on UniProt (pdb_seq_num =\\n\",\n",
    "                \"auth_seq_id in the _atom_site records), and the author numbering\\n\",\n",
    "                \"in the original mmCIF file from the PDB (auth_seq_num).\\n\",\n",
    "                \";\\n\",\n",
    "                \"#\\n\"] \n",
    "\n",
    "\n",
    "def try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name):    \n",
    "    mmcif_dict = 0\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            mmcif_dict = Bio.PDB.MMCIF2Dict.MMCIF2Dict(gzip.open(Path(str(default_input_path_to_mmCIF) + \"/\" + mmCIF_name), 'rt'))\n",
    "            break\n",
    "        except EOFError:\n",
    "            os.remove(Path(str(default_input_path_to_mmCIF) + \"/\" + mmCIF_name))\n",
    "            if \"assembly\" in mmCIF_name: \n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF_assembly\", [mmCIF_name])[0])\n",
    "            else:\n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF\", [mmCIF_name])[0])\n",
    "        except ValueError:\n",
    "            os.remove(Path(str(default_input_path_to_mmCIF) + \"/\" + mmCIF_name))\n",
    "            if \"assembly\" in mmCIF_name: \n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF_assembly\", [mmCIF_name])[0])\n",
    "            else:\n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF\", [mmCIF_name])[0])\n",
    "        except OSError:\n",
    "            if \"assembly\" in mmCIF_name: \n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF_assembly\", [mmCIF_name])[0])\n",
    "            else:\n",
    "                download_with_pool(url_formation_for_pool(\"mmCIF\", [mmCIF_name])[0])       \n",
    "    return mmcif_dict\n",
    "\n",
    "def try_SIFTS_tree_parser(default_input_path_to_SIFTS, SIFTS_name):\n",
    "    product_tree_SIFTS = 0\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            handle_SIFTS = gzip.open(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name), 'rt')\n",
    "            product_tree_SIFTS = SIFTS_tree_parser(handle_SIFTS)\n",
    "            break\n",
    "        except EOFError:\n",
    "            os.remove(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name))\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])\n",
    "        except ValueError:\n",
    "            os.remove(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name))\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])     \n",
    "        except OSError:\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])\n",
    "        except:\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name])[0])\n",
    "    return product_tree_SIFTS\n",
    "\n",
    "def output_with_this_name_ending(name_ending, path, mmcif_dict, mmCIF_name, gzip_mode=gzip_mode, current_directory=current_directory):\n",
    "    mmCIF_name = mmCIF_name[:mmCIF_name.rfind(\".cif.gz\")]\n",
    "    os.chdir(path)\n",
    "    io = MMCIFIO()\n",
    "    io.set_dict(mmcif_dict)\n",
    "    io.save(mmCIF_name + name_ending)\n",
    "    if gzip_mode == \"on\":\n",
    "        compressor.compress_output_files(mmCIF_name + name_ending, gzip_mode)\n",
    "        os.remove(mmCIF_name + name_ending)\n",
    "    os.chdir(current_directory)\n",
    "\n",
    "    \n",
    "def copy_file(inpath, file_name, outpath, postfix, gzip_mode):\n",
    "    mmCIF_name = file_name[:file_name.rfind(\".cif.gz\")]\n",
    "    absolute_path_in = inpath + \"/\" + file_name\n",
    "    absolute_path_out = outpath + \"/\" + mmCIF_name + postfix\n",
    "    if gzip_mode == \"off\":\n",
    "        with gzip.open(absolute_path_in, 'rb') as f_in:\n",
    "            with open(absolute_path_out[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        shutil.copyfile(absolute_path_in, absolute_path_out)\n",
    "        \n",
    "        \n",
    "def if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message):\n",
    "    strand_id_set = set()\n",
    "    try:\n",
    "        pull_chains_for_chains_count = mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_strand_id\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            pull_chains_for_chains_count = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_strand_id\"]\n",
    "        except KeyError:\n",
    "            pull_chains_for_chains_count = mmcif_dict[\"_atom_site.auth_asym_id\"]\n",
    "\n",
    "    for strand in pull_chains_for_chains_count:\n",
    "        strand_id_set.add(strand)\n",
    "    strand_id_set = list(strand_id_set)\n",
    "    strand_id_set.sort()\n",
    "    for strand in strand_id_set:\n",
    "        count_elements_in_strand = 0\n",
    "        for chain_id in pull_chains_for_chains_count:\n",
    "            if chain_id == strand:\n",
    "                count_elements_in_strand += 1\n",
    "        log_message.append([mmCIF_name[:4], strand, \"-\", \"-\", \"-\", \"-\", count_elements_in_strand, \"0\", \"0\"])\n",
    "    return log_message\n",
    "\n",
    "\n",
    "def renum_struct_ref_seq_pdbx_auth_seq_align(mmcif_dict):\n",
    "    try:\n",
    "        _struct_ref_seq_pdbx_strand_id = mmcif_dict[\"_struct_ref_seq.pdbx_strand_id\"]\n",
    "\n",
    "        _struct_ref_seq_pdbx_seq_align_beg_ins_code = mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_beg_ins_code\"]\n",
    "        _struct_ref_seq_pdbx_auth_seq_align_beg = mmcif_dict[\"_struct_ref_seq.pdbx_auth_seq_align_beg\"]\n",
    "        _struct_ref_seq_db_align_beg = mmcif_dict[\"_struct_ref_seq.db_align_beg\"]\n",
    "        mmcif_dict[\"_struct_ref_seq.pdbx_auth_seq_align_beg\"] = mmcif_dict[\"_struct_ref_seq.db_align_beg\"]\n",
    "\n",
    "        _struct_ref_seq_pdbx_seq_align_end_ins_code = mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_end_ins_code\"]\n",
    "        _struct_ref_seq_pdbx_auth_seq_align_end = mmcif_dict[\"_struct_ref_seq.pdbx_auth_seq_align_end\"]\n",
    "        _struct_ref_seq_db_align_end = mmcif_dict[\"_struct_ref_seq.db_align_end\"]\n",
    "        mmcif_dict[\"_struct_ref_seq.pdbx_auth_seq_align_end\"] = mmcif_dict[\"_struct_ref_seq.db_align_end\"]\n",
    "\n",
    "        if type(_struct_ref_seq_pdbx_seq_align_beg_ins_code) == str:\n",
    "            if \".\" in _struct_ref_seq_pdbx_seq_align_beg_ins_code:\n",
    "                mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_beg_ins_code\"] = \".\"\n",
    "            else:\n",
    "                mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_beg_ins_code\"] = \"?\"\n",
    "        if type(_struct_ref_seq_pdbx_seq_align_end_ins_code) == str:\n",
    "            if \".\" in _struct_ref_seq_pdbx_seq_align_end_ins_code:\n",
    "                mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_end_ins_code\"] = \".\"\n",
    "            else:\n",
    "                mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_end_ins_code\"] = \"?\"\n",
    "\n",
    "        PDB_ins_code_list = list()\n",
    "        if type(_struct_ref_seq_pdbx_seq_align_beg_ins_code) != str:\n",
    "            if \".\" in _struct_ref_seq_pdbx_seq_align_beg_ins_code:\n",
    "                for _ in range(len(_struct_ref_seq_pdbx_seq_align_beg_ins_code)):\n",
    "                    PDB_ins_code_list.append(\".\")\n",
    "            else:\n",
    "                for _ in range(len(_struct_ref_seq_pdbx_seq_align_beg_ins_code)):\n",
    "                    PDB_ins_code_list.append(\"?\")\n",
    "            mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_beg_ins_code\"] = PDB_ins_code_list\n",
    "            mmcif_dict[\"_struct_ref_seq.pdbx_seq_align_end_ins_code\"] = PDB_ins_code_list\n",
    "\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "def poly_nonpoly_renum(mmcif_dict, df_PDBe_PDB_UniProt, chains_to_change, default_mmCIF_num):\n",
    "    try:\n",
    "        _pdbx_poly_seq_scheme_seq_id = mmcif_dict[\"_pdbx_poly_seq_scheme.seq_id\"]\n",
    "        _pdbx_poly_seq_scheme_asym_id = mmcif_dict[\"_pdbx_poly_seq_scheme.asym_id\"]\n",
    "        _pdbx_poly_seq_scheme_mon_id = mmcif_dict[\"_pdbx_poly_seq_scheme.mon_id\"]\n",
    "\n",
    "        _pdbx_poly_seq_scheme_pdb_seq_num = mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"]\n",
    "        _pdbx_poly_seq_scheme_auth_seq_num = mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"]\n",
    "        _pdbx_poly_seq_scheme_pdb_mon_id = mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_mon_id\"]\n",
    "        _pdbx_poly_seq_scheme_auth_mon_id = mmcif_dict[\"_pdbx_poly_seq_scheme.auth_mon_id\"]\n",
    "        _pdbx_poly_seq_scheme_pdb_strand_id = mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_strand_id\"]\n",
    "        _pdbx_poly_seq_scheme_pdb_ins_code = mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_ins_code\"]\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _pdbx_poly_seq_scheme_seq_id = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.seq_id\"]\n",
    "            _pdbx_poly_seq_scheme_asym_id = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.asym_id\"]\n",
    "            _pdbx_poly_seq_scheme_mon_id = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.mon_id\"]\n",
    "\n",
    "            _pdbx_poly_seq_scheme_pdb_seq_num = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_seq_num\"]\n",
    "            _pdbx_poly_seq_scheme_auth_seq_num = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.auth_seq_num\"]\n",
    "            _pdbx_poly_seq_scheme_pdb_mon_id = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_mon_id\"]\n",
    "            _pdbx_poly_seq_scheme_auth_mon_id = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.auth_mon_id\"]\n",
    "            _pdbx_poly_seq_scheme_pdb_strand_id = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_strand_id\"]\n",
    "            _pdbx_poly_seq_scheme_pdb_ins_code = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_ins_code\"]\n",
    "\n",
    "        except KeyError:\n",
    "            # continue\n",
    "            return 0\n",
    "\n",
    "    if type(_pdbx_poly_seq_scheme_pdb_strand_id) == str:\n",
    "        _pdbx_poly_seq_scheme_pdb_seq_num = [_pdbx_poly_seq_scheme_pdb_seq_num]\n",
    "        _pdbx_poly_seq_scheme_auth_seq_num = [_pdbx_poly_seq_scheme_auth_seq_num]\n",
    "        _pdbx_poly_seq_scheme_pdb_mon_id = [_pdbx_poly_seq_scheme_pdb_mon_id]\n",
    "        _pdbx_poly_seq_scheme_auth_mon_id = [_pdbx_poly_seq_scheme_auth_mon_id]\n",
    "        _pdbx_poly_seq_scheme_pdb_strand_id = [_pdbx_poly_seq_scheme_pdb_strand_id]\n",
    "        _pdbx_poly_seq_scheme_pdb_ins_code = [_pdbx_poly_seq_scheme_pdb_ins_code]\n",
    "\n",
    "    mmCIF_pdbx_poly_seq_scheme_label = list(zip(_pdbx_poly_seq_scheme_seq_id,\n",
    "                                                _pdbx_poly_seq_scheme_mon_id,\n",
    "                                                _pdbx_poly_seq_scheme_asym_id))\n",
    "    mmCIF_pdbx_poly_seq_scheme_pdb = list(zip(_pdbx_poly_seq_scheme_pdb_seq_num,\n",
    "                                              _pdbx_poly_seq_scheme_pdb_mon_id,\n",
    "                                              _pdbx_poly_seq_scheme_pdb_strand_id))\n",
    "    mmCIF_pdbx_poly_seq_scheme_auth = list(zip(_pdbx_poly_seq_scheme_auth_seq_num,\n",
    "                                               _pdbx_poly_seq_scheme_auth_mon_id,\n",
    "                                               _pdbx_poly_seq_scheme_pdb_strand_id))\n",
    "\n",
    "    df_mmCIF_pdbx_poly_seq_scheme = pd.DataFrame(zip(mmCIF_pdbx_poly_seq_scheme_label,\n",
    "                                                     mmCIF_pdbx_poly_seq_scheme_pdb,\n",
    "                                                     mmCIF_pdbx_poly_seq_scheme_auth,\n",
    "                                                     _pdbx_poly_seq_scheme_pdb_ins_code))\n",
    "\n",
    "    df_mmCIF_pdbx_poly_seq_scheme = df_mmCIF_pdbx_poly_seq_scheme.rename(\n",
    "        columns={0: \"_pdbx_poly_seq_scheme_label\", 1: \"pdbx_poly_seq_scheme_pdb\",\n",
    "                 2: \"pdbx_poly_seq_scheme_auth\", 3: \"pdbx_poly_seq_scheme_pdb_ins_code\"})\n",
    "\n",
    "    df_pdbx_poly_seq_scheme_pdb_final = df_mmCIF_pdbx_poly_seq_scheme.merge(\n",
    "        df_PDBe_PDB_UniProt, left_on=\"_pdbx_poly_seq_scheme_label\", right_on=\"PDBe\", how='left')\n",
    "    df_pdbx_poly_seq_scheme_pdb_final[\"PDBe_num_and_chain\"] = df_pdbx_poly_seq_scheme_pdb_final[\n",
    "        \"_pdbx_poly_seq_scheme_label\"].apply(lambda x: (x[0], x[2]))\n",
    "\n",
    "    df_pdbx_poly_seq_scheme_pdb_final[\"PDB_num_and_chain\"] = np.where(\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"pdbx_poly_seq_scheme_pdb_ins_code\"].apply(lambda x: x == \".\"),\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"pdbx_poly_seq_scheme_pdb\"].apply(lambda x: (x[0], x[2])),\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"pdbx_poly_seq_scheme_pdb\"].apply(lambda x: x[0]) +\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"pdbx_poly_seq_scheme_pdb_ins_code\"].apply(lambda x: x) + \",\" +\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"pdbx_poly_seq_scheme_pdb\"].apply(lambda x: x[2]))\n",
    "    df_pdbx_poly_seq_scheme_pdb_final[\"PDB_num_and_chain\"] = df_pdbx_poly_seq_scheme_pdb_final[\"PDB_num_and_chain\"].apply(\n",
    "        lambda x: tuple(x.split(\",\")) if type(x) == str else x)\n",
    "\n",
    "    df_pdbx_poly_seq_scheme_pdb_final[\"Uni_or_50k\"] = np.where(\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"PDB_num_and_chain\"].apply(lambda x: x[1] in chains_to_change),\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"UniProt_50k\"].apply(lambda x: x),\n",
    "        df_pdbx_poly_seq_scheme_pdb_final[\"PDB_num_and_chain\"].apply(lambda x: x[0].strip(re.sub('[0-9\\-\\?\\.]+', '', x[0]))))\n",
    "\n",
    "    try:\n",
    "        mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"]  # check if key exists\n",
    "        mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"] = list(df_pdbx_poly_seq_scheme_pdb_final[\"Uni_or_50k\"].values)\n",
    "        mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"] = _pdbx_poly_seq_scheme_pdb_seq_num \n",
    "    except KeyError:\n",
    "        mmcif_dict[\"_pdbe_orig_poly_seq_scheme.pdb_seq_num\"] = list(df_pdbx_poly_seq_scheme_pdb_final[\"Uni_or_50k\"].values)\n",
    "        mmcif_dict[\"_pdbe_orig_poly_seq_scheme.auth_seq_num\"] = _pdbx_poly_seq_scheme_pdb_seq_num \n",
    "\n",
    "    nonpoly_present = False\n",
    "    try:\n",
    "        _pdbx_nonpoly_scheme_pdb_seq_num = mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_seq_num\"]\n",
    "        _pdbx_nonpoly_scheme_auth_seq_num = mmcif_dict[\"_pdbx_nonpoly_scheme.auth_seq_num\"]\n",
    "        _pdbx_nonpoly_scheme_pdb_mon_id = mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_mon_id\"]\n",
    "        _pdbx_nonpoly_scheme_auth_mon_id = mmcif_dict[\"_pdbx_nonpoly_scheme.auth_mon_id\"]\n",
    "        _pdbx_nonpoly_scheme_pdb_strand_id = mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_strand_id\"]\n",
    "        _pdbx_nonpoly_scheme_asym_id = mmcif_dict[\"_pdbx_nonpoly_scheme.asym_id\"]\n",
    "        dots_for_lable = [\".\" for _ in range(len(_pdbx_nonpoly_scheme_asym_id)) if type(_pdbx_nonpoly_scheme_asym_id) == list]\n",
    "        nonpoly_present = True\n",
    "    except KeyError:\n",
    "        try:\n",
    "            _pdbx_nonpoly_scheme_pdb_seq_num = mmcif_dict[\"_pdbe_orig_nonpoly_scheme.pdb_seq_num\"]\n",
    "            _pdbx_nonpoly_scheme_auth_seq_num = mmcif_dict[\"_pdbe_orig_nonpoly_scheme.auth_seq_num\"]\n",
    "            _pdbx_nonpoly_scheme_pdb_mon_id = mmcif_dict[\"_pdbe_orig_nonpoly_scheme.pdb_mon_id\"]\n",
    "            _pdbx_nonpoly_scheme_auth_mon_id = mmcif_dict[\"_pdbe_orig_nonpoly_scheme.auth_mon_id\"]\n",
    "            _pdbx_nonpoly_scheme_pdb_strand_id = mmcif_dict[\"_pdbe_orig_nonpoly_scheme.pdb_strand_id\"]\n",
    "            _pdbx_nonpoly_scheme_asym_id = mmcif_dict[\"_pdbe_orig_nonpoly_scheme.asym_id\"]\n",
    "            dots_for_lable = [\".\" for _ in range(len(_pdbx_nonpoly_scheme_asym_id)) if type(_pdbx_nonpoly_scheme_asym_id) == list]\n",
    "            nonpoly_present = True\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    if nonpoly_present:\n",
    "        if type(_pdbx_nonpoly_scheme_pdb_strand_id) == str:\n",
    "            _pdbx_nonpoly_scheme_pdb_seq_num = [_pdbx_nonpoly_scheme_pdb_seq_num]\n",
    "            _pdbx_nonpoly_scheme_auth_seq_num = [_pdbx_nonpoly_scheme_auth_seq_num]\n",
    "            _pdbx_nonpoly_scheme_pdb_mon_id = [_pdbx_nonpoly_scheme_pdb_mon_id]\n",
    "            _pdbx_nonpoly_scheme_auth_mon_id = [_pdbx_nonpoly_scheme_auth_mon_id]\n",
    "            _pdbx_nonpoly_scheme_pdb_strand_id = [_pdbx_nonpoly_scheme_pdb_strand_id]\n",
    "            _pdbx_nonpoly_scheme_asym_id = [_pdbx_nonpoly_scheme_asym_id]\n",
    "            dots_for_lable = [\".\"]\n",
    "\n",
    "        mmCIF_pdbx_nonpoly_scheme_pdb = list(zip(_pdbx_nonpoly_scheme_pdb_seq_num,\n",
    "                                                 _pdbx_nonpoly_scheme_pdb_mon_id,\n",
    "                                                 _pdbx_nonpoly_scheme_pdb_strand_id))\n",
    "        mmCIF_pdbx_nonpoly_scheme_auth = list(zip(_pdbx_nonpoly_scheme_auth_seq_num,\n",
    "                                                  _pdbx_nonpoly_scheme_auth_mon_id,\n",
    "                                                  _pdbx_nonpoly_scheme_pdb_strand_id))\n",
    "        mmCIF_pdbx_nonpoly_scheme_label = list(zip(dots_for_lable,\n",
    "                                                   _pdbx_nonpoly_scheme_pdb_mon_id,\n",
    "                                                   _pdbx_nonpoly_scheme_asym_id))\n",
    "\n",
    "        df_mmCIF_pdbx_nonpoly_scheme = pd.DataFrame(zip(mmCIF_pdbx_nonpoly_scheme_pdb,\n",
    "                                                        mmCIF_pdbx_nonpoly_scheme_auth,\n",
    "                                                        mmCIF_pdbx_nonpoly_scheme_label))\n",
    "        df_mmCIF_pdbx_nonpoly_scheme = df_mmCIF_pdbx_nonpoly_scheme.rename(columns={0: \"pdbx_nonpoly_scheme_pdb\",\n",
    "                                                                                    1: \"pdbx_nonpoly_scheme_auth\",\n",
    "                                                                                    2: \"pdbx_nonpoly_scheme_label\"})\n",
    "\n",
    "        df_mmCIF_pdbx_nonpoly_scheme[\"PDB\"] = df_mmCIF_pdbx_nonpoly_scheme[\"pdbx_nonpoly_scheme_pdb\"]\n",
    "        df_mmCIF_pdbx_nonpoly_scheme[\"PDB_num_and_chain\"] = df_mmCIF_pdbx_nonpoly_scheme[\"pdbx_nonpoly_scheme_pdb\"].apply(lambda x: (x[0], x[2]))\n",
    "        df_mmCIF_pdbx_nonpoly_scheme[\"Uni_or_50k\"] = df_mmCIF_pdbx_nonpoly_scheme[\"pdbx_nonpoly_scheme_pdb\"].apply(\n",
    "            lambda x: str(int(x[0]) + default_mmCIF_num + 10000) if x[2] in chains_to_change else x[0])\n",
    "\n",
    "        try:\n",
    "            mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_seq_num\"]  # check if key exists\n",
    "            mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_seq_num\"] = list(df_mmCIF_pdbx_nonpoly_scheme[\"Uni_or_50k\"].values)\n",
    "            mmcif_dict[\"_pdbx_nonpoly_scheme.auth_seq_num\"] = _pdbx_nonpoly_scheme_pdb_seq_num\n",
    "        except KeyError:\n",
    "            try:\n",
    "                mmcif_dict[\"_pdbe_orig_nonpoly_scheme.pdb_seq_num\"] = list(df_mmCIF_pdbx_nonpoly_scheme[\"Uni_or_50k\"].values)\n",
    "                mmcif_dict[\"_pdbe_orig_nonpoly_scheme.auth_seq_num\"] = _pdbx_nonpoly_scheme_pdb_seq_num\n",
    "            except KeyError:\n",
    "                pass\n",
    "\n",
    "        poly_nonpoly_append = df_pdbx_poly_seq_scheme_pdb_final.append(df_mmCIF_pdbx_nonpoly_scheme)\n",
    "        poly_nonpoly_append = poly_nonpoly_append[[\"PDBe\", \"PDB\", \"UniProt\", \"PDBe_num_and_chain\", \"PDB_num_and_chain\", \"AccessionID\", \"Uni_or_50k\"]]\n",
    "    else:\n",
    "        poly_nonpoly_append = df_pdbx_poly_seq_scheme_pdb_final[\n",
    "            [\"PDBe\", \"PDB\", \"UniProt\", \"PDBe_num_and_chain\", \"PDB_num_and_chain\", \"AccessionID\", \"Uni_or_50k\"]]\n",
    "\n",
    "    return poly_nonpoly_append\n",
    "\n",
    "\n",
    "def renumber_tables(formed_columns, mmcif_dict, poly_nonpoly_atom_site, chains_to_change, default_mmCIF_num):\n",
    "    dot_or_question_tuple = (\".\", \"?\")\n",
    "    for n in formed_columns:\n",
    "        auth_comp_id = 0\n",
    "        auth_seq_id = n[0]\n",
    "        auth_asym_id = n[1]\n",
    "        try:\n",
    "            PDB_ins_code = n[2]\n",
    "            if \"ins_code\" not in PDB_ins_code:\n",
    "                auth_comp_id = PDB_ins_code\n",
    "                PDB_ins_code = 0\n",
    "        except IndexError:\n",
    "            PDB_ins_code = 0\n",
    "        try:\n",
    "            if auth_comp_id == 0:\n",
    "                auth_comp_id = n[3]\n",
    "        except IndexError:\n",
    "            auth_comp_id = 0\n",
    "\n",
    "        if \"_pdbx_branch_scheme\" in auth_seq_id:\n",
    "            auth_seq_id = \"_pdbx_branch_scheme.pdb_seq_num\"\n",
    "            auth_asym_id = \"_pdbx_branch_scheme.pdb_asym_id\"\n",
    "\n",
    "        PDB_ins_code_list = list()\n",
    "        # auth_comp_id_list = mmcif_dict[auth_comp_id] #for debug only\n",
    "        auth_seq_id_list = mmcif_dict[auth_seq_id]\n",
    "        auth_asym_id_list = mmcif_dict[auth_asym_id]\n",
    "\n",
    "        if PDB_ins_code == 0:\n",
    "            for _ in range(len(auth_seq_id_list)):\n",
    "                PDB_ins_code_list.append(\"?\")\n",
    "        else:\n",
    "            PDB_ins_code_list = mmcif_dict[PDB_ins_code]\n",
    "\n",
    "        if type(auth_asym_id_list) == str:\n",
    "            # auth_comp_id_list = [auth_comp_id_list] for debug only\n",
    "            auth_seq_id_list = [auth_seq_id_list]\n",
    "            auth_asym_id_list = [auth_asym_id_list]\n",
    "\n",
    "            if PDB_ins_code == 0:\n",
    "                PDB_ins_code_list = [\"?\"]\n",
    "            else:\n",
    "                PDB_ins_code_list = [PDB_ins_code]\n",
    "\n",
    "        if PDB_ins_code != 0:\n",
    "            dot_to_question = list()\n",
    "            for ins_code in mmcif_dict[PDB_ins_code]:\n",
    "                if ins_code == \".\":\n",
    "                    dot_to_question.append(\"?\")\n",
    "                else:\n",
    "                    dot_to_question.append(ins_code)\n",
    "            PDB_ins_code_list = dot_to_question\n",
    "\n",
    "        auth_seq_id_list_zip = list(zip(auth_seq_id_list, auth_asym_id_list))\n",
    "        df_mmCIF_auth_seq_id_list_zip = pd.DataFrame(zip(auth_seq_id_list_zip, PDB_ins_code_list))\n",
    "        df_mmCIF_auth_seq_id_list_zip = df_mmCIF_auth_seq_id_list_zip.rename(columns={0: \"auth_seq_id_list_zip\", 1: \"ins_code\"})\n",
    "\n",
    "        df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code\"] = np.where(df_mmCIF_auth_seq_id_list_zip['ins_code'] != \"?\",\n",
    "                                                                      (df_mmCIF_auth_seq_id_list_zip['auth_seq_id_list_zip'].apply(lambda x: x[0])\n",
    "                                                                       + df_mmCIF_auth_seq_id_list_zip['ins_code'].apply(lambda y: y[0]) + \",\"\n",
    "                                                                       + df_mmCIF_auth_seq_id_list_zip['auth_seq_id_list_zip'].apply(lambda x: x[1])),\n",
    "                                                                      df_mmCIF_auth_seq_id_list_zip['ins_code'])\n",
    "\n",
    "        df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code_cor\"] = np.where(df_mmCIF_auth_seq_id_list_zip['PDB_with_ins_code'] != \"?\",\n",
    "                                                                          df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code\"].apply(\n",
    "                                                                              lambda x: tuple(x.split(\",\"))),\n",
    "                                                                          df_mmCIF_auth_seq_id_list_zip[\"auth_seq_id_list_zip\"])\n",
    "\n",
    "        df_mmCIF_auth_seq_id_list_zip[\"auth_seq_id_list_zip\"] = df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code_cor\"]\n",
    "        df_mmCIF_auth_seq_id_list_zip = df_mmCIF_auth_seq_id_list_zip.drop(columns=[\"PDB_with_ins_code_cor\", \"ins_code\", \"PDB_with_ins_code\"])\n",
    "\n",
    "        df_auth_seq_id_list_zip_final = df_mmCIF_auth_seq_id_list_zip.merge(poly_nonpoly_atom_site, left_on=\"auth_seq_id_list_zip\",\n",
    "                                                                            right_on=\"PDB_num_and_chain\", how='left')\n",
    "\n",
    "        df_auth_seq_id_list_zip_final[\"question_mark\"] = np.where(\n",
    "            df_auth_seq_id_list_zip_final[\"auth_seq_id_list_zip\"].apply(lambda x: x[0] in dot_or_question_tuple),\n",
    "            df_auth_seq_id_list_zip_final[\"auth_seq_id_list_zip\"].apply(lambda x: x[0]),\n",
    "            df_auth_seq_id_list_zip_final[\"Uni_or_50k\"].apply(lambda x: x))\n",
    "        try:\n",
    "            df_auth_seq_id_list_zip_final[\"final\"] = np.where(df_auth_seq_id_list_zip_final[\"question_mark\"].apply(lambda x: type(x) == float),\n",
    "                                                              df_auth_seq_id_list_zip_final[\"auth_seq_id_list_zip\"].apply(\n",
    "                                                                  lambda x: \".\" if x[0] == \".\" else\n",
    "                                                                  \"?\" if x[0] == \"?\" else str(\n",
    "                                                                      int(''.join(filter(str.isdigit, str(x[0])))) + default_mmCIF_num)\n",
    "                                                                  if x[1] in chains_to_change else str(int(''.join(filter(str.isdigit, str(x[0])))))),\n",
    "                                                              df_auth_seq_id_list_zip_final[\"question_mark\"].apply(lambda x: x))\n",
    "        except ValueError:\n",
    "            # print(\"ValueError in table \" + auth_seq_id + \" has non-numeric value point in file \" + mmcif_dict[\"data_\"])\n",
    "            return print(\"ValueError in table \" + auth_seq_id + \" has non-numeric value point in file \" + mmcif_dict[\"data_\"])\n",
    "\n",
    "        df_auth_seq_id_list_zip_final[\"ins_code\"] = df_auth_seq_id_list_zip_final[\"final\"].apply(lambda x: \"?\"\n",
    "        if re.sub('[0-9]+', '', x).strip(\"-\").strip(\".\").strip('?') == \"\"\n",
    "        else re.sub('[0-9]+', '', x).strip(\"-\").strip(\".\").strip('?'))\n",
    "        df_auth_seq_id_list_zip_final[\"final\"] = df_auth_seq_id_list_zip_final[\"final\"].apply(lambda x: x.strip(re.sub('[0-9\\-\\?\\.]+', '', x)))\n",
    "\n",
    "        for num in df_auth_seq_id_list_zip_final[\"final\"]:\n",
    "            if num == \"\":\n",
    "                print(\"Empty str\")\n",
    "            if type(num) == float:\n",
    "                print(\"Float or npNAN\")\n",
    "\n",
    "        # actual replacing auth_num with UniProt_num and of ins_code with '?'\n",
    "\n",
    "        PDB_ins_code_list = list()\n",
    "        if PDB_ins_code != 0:\n",
    "            if \".\" in mmcif_dict[PDB_ins_code]:\n",
    "                for ins in df_auth_seq_id_list_zip_final[\"ins_code\"].values:\n",
    "                    if \"?\" == ins:\n",
    "                        PDB_ins_code_list.append(\".\")\n",
    "                    else:\n",
    "                        PDB_ins_code_list.append(ins)\n",
    "                mmcif_dict[PDB_ins_code] = PDB_ins_code_list\n",
    "            else:\n",
    "                mmcif_dict[PDB_ins_code] = list(df_auth_seq_id_list_zip_final[\"ins_code\"].values)\n",
    "\n",
    "        if \"_pdbx_branch_scheme\" in auth_seq_id:\n",
    "            mmcif_dict[\"_pdbx_branch_scheme.auth_seq_num\"] = list(df_auth_seq_id_list_zip_final[\"final\"].values)\n",
    "        else:\n",
    "            mmcif_dict[auth_seq_id] = list(df_auth_seq_id_list_zip_final[\"final\"].values)\n",
    "\n",
    "    return mmcif_dict\n",
    "\n",
    "\n",
    "def column_formation(mmcif_dict):\n",
    "    mmcif_dict_keys = mmcif_dict.keys()\n",
    "    aut_seq_all_splited = list()\n",
    "    for key in mmcif_dict_keys:\n",
    "        key_dot_splited = key.split(\".\")\n",
    "        for tab_name_col_name in key_dot_splited:\n",
    "            if \"auth_seq\" in tab_name_col_name:\n",
    "                if \"auth_seq_id\" in key:\n",
    "                    aut_seq_all_splited.append(key_dot_splited[:1] + key_dot_splited[1].split(\"auth_seq_id\"))\n",
    "                if \"auth_seq_num\" in key:\n",
    "                    aut_seq_all_splited.append(key_dot_splited[:1] + key_dot_splited[1].split(\"auth_seq_num\"))\n",
    "\n",
    "    totaling_combinations = list()\n",
    "    for table_name_prefix_suffix in aut_seq_all_splited:\n",
    "        combinations = list()\n",
    "        for key in mmcif_dict_keys:\n",
    "            if table_name_prefix_suffix[0] == key.split(\".\")[0]:\n",
    "                # res_num auth_seq_id or auth_seq_num\n",
    "                if table_name_prefix_suffix[1] in key and table_name_prefix_suffix[2] in key \\\n",
    "                        and \"auth_seq_id\" in key or \"auth_seq_num\" in key:\n",
    "                    combinations.append(key)\n",
    "                # chain auth_asym_id or strand_id\n",
    "                if \"assembly\" in mmcif_dict[\"data_\"]:\n",
    "                    if table_name_prefix_suffix[1] in key and table_name_prefix_suffix[2] in key \\\n",
    "                            and \"orig_auth_asym_id\" in key:\n",
    "                        combinations.append(key)\n",
    "                else:\n",
    "                    if table_name_prefix_suffix[1] in key and table_name_prefix_suffix[2] in key \\\n",
    "                            and \"auth_asym_id\" in key or \"strand_id\" in key:\n",
    "                        combinations.append(key)\n",
    "                # ins_code\n",
    "                if table_name_prefix_suffix[1] in key and table_name_prefix_suffix[2] in key \\\n",
    "                        and \"ins_code\" in key:\n",
    "                    combinations.append(key)\n",
    "                # monomer_type or auth_comp_id or auth_mon_id or mon_id for _struct_ref_seq_dif\n",
    "                if table_name_prefix_suffix[1] in key and table_name_prefix_suffix[2] in key \\\n",
    "                        and \"auth_comp_id\" in key or \"auth_mon_id\" in key:\n",
    "                    combinations.append(key)\n",
    "                elif table_name_prefix_suffix[0] == \"_struct_ref_seq_dif\" \\\n",
    "                        and \"mon_id\" in key and \"db_mon_id\" not in key:\n",
    "                    combinations.append(key)\n",
    "\n",
    "        # work assuming all the elements in right order\n",
    "        # and they are not crossing each other\n",
    "        if len(combinations) > 4:\n",
    "            combinations = combinations[:4]\n",
    "\n",
    "        ordered_combination = list()\n",
    "        for name in combinations:\n",
    "            if \"auth_seq\" in name:\n",
    "                ordered_combination.insert(0, name)\n",
    "        for name in combinations:\n",
    "            if \"auth_asym_id\" in name or \"strand_id\" in name:\n",
    "                ordered_combination.insert(1, name)\n",
    "        for name in combinations:\n",
    "            if \"ins_code\" in name:\n",
    "                ordered_combination.insert(2, name)\n",
    "        for name in combinations:\n",
    "            if \"auth_comp_id\" in name or \"mon_id\" in name:\n",
    "                ordered_combination.insert(3, name)\n",
    "\n",
    "        # exceptions\n",
    "        if (  # \"pdbx_unobs_or_zero_occ_residues\" not in ordered_combination[0]\n",
    "                \"nonpoly_scheme\" not in ordered_combination[0]\n",
    "                and \"poly_seq_scheme\" not in ordered_combination[0]\n",
    "                and \"ndb_struct_na_base\" not in ordered_combination[0]):\n",
    "            totaling_combinations.append(ordered_combination)\n",
    "\n",
    "    return totaling_combinations\n",
    "\n",
    "\n",
    "def mmCIF_parser(mmCIF_name, default_input_path_to_mmCIF, df_PDBe_PDB_UniProt_without_null_index_PDBe, default_mmCIF_num, chains_to_change,\n",
    "                 chains_to_change_one_to_end):\n",
    "    mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "    if mmcif_dict == 0:\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        _pdbx_poly_seq_scheme_auth_seq_num_before_change = mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"]\n",
    "    except KeyError:\n",
    "        _pdbx_poly_seq_scheme_auth_seq_num_before_change = mmcif_dict[\"_pdbe_orig_poly_seq_scheme.auth_seq_num\"]\n",
    "        pass\n",
    "\n",
    "    _atom_site_label_comp_id_list = mmcif_dict[\"_atom_site.label_comp_id\"]\n",
    "    _atom_site_label_seq_id_list = mmcif_dict[\"_atom_site.label_seq_id\"]\n",
    "    _atom_site_label_asym_id = mmcif_dict[\"_atom_site.label_asym_id\"]\n",
    "    _atom_site_pdbx_PDB_ins_code = mmcif_dict[\"_atom_site.pdbx_PDB_ins_code\"]\n",
    "\n",
    "    _atom_site_auth_comp_id = mmcif_dict[\"_atom_site.auth_comp_id\"]\n",
    "    _atom_site_auth_seq_id = mmcif_dict[\"_atom_site.auth_seq_id\"]\n",
    "    _atom_site_auth_asym_id = mmcif_dict[\"_atom_site.auth_asym_id\"]\n",
    "    _atom_site_pdbx_formal_charge = mmcif_dict[\"_atom_site.pdbx_formal_charge\"]\n",
    "\n",
    "    final_mmCIF_data_list_of_tuples_just_pdb = list(zip(_atom_site_label_seq_id_list, _atom_site_label_comp_id_list, _atom_site_label_asym_id))\n",
    "    final_mmCIF_data_list_of_tuples_with_auth = list(zip(_atom_site_auth_seq_id, _atom_site_auth_comp_id, _atom_site_auth_asym_id))\n",
    "    final_mmCIF_data_list_of_tuples_for_df = list(\n",
    "        zip(final_mmCIF_data_list_of_tuples_just_pdb, final_mmCIF_data_list_of_tuples_with_auth, _atom_site_pdbx_PDB_ins_code))\n",
    "\n",
    "    df_mmCIF = pd.DataFrame(final_mmCIF_data_list_of_tuples_for_df)\n",
    "    df_mmCIF = df_mmCIF.rename(columns={0: \"One_to_N_mmCIF\", 1: \"auth_mmCIF\", 2: \"ins_code\"})\n",
    "\n",
    "    df_mmCIF[\"One_to_N_mmCIF\"]\n",
    "\n",
    "    df_mmCIF[\"PDBnum_inc_code\"] = np.where(df_mmCIF['ins_code'] != \"?\",\n",
    "                                           (df_mmCIF['auth_mmCIF'].apply(lambda x: x[0]) + df_mmCIF[\"ins_code\"].apply(lambda y: y[0]) + \",\"\n",
    "                                            + df_mmCIF['auth_mmCIF'].apply(lambda x: x[1]) + \",\" + df_mmCIF['auth_mmCIF'].apply(lambda x: x[2])),\n",
    "                                           df_mmCIF[\"ins_code\"])\n",
    "    df_mmCIF[\"PDBnum_inc_code_cor\"] = np.where(df_mmCIF[\"PDBnum_inc_code\"] != \"?\", df_mmCIF[\"PDBnum_inc_code\"].apply(lambda x: tuple(x.split(\",\"))),\n",
    "                                               df_mmCIF[\"auth_mmCIF\"])\n",
    "\n",
    "    df_mmCIF[\"auth_mmCIF\"] = df_mmCIF[\"PDBnum_inc_code_cor\"]\n",
    "    df_mmCIF = df_mmCIF.drop(columns=[\"PDBnum_inc_code_cor\", \"ins_code\", \"PDBnum_inc_code\"])\n",
    "\n",
    "    df_PDBe_PDB_UniProt_without_null_index_PDBe = df_PDBe_PDB_UniProt_without_null_index_PDBe.reset_index()\n",
    "    df_final = df_mmCIF.merge(df_PDBe_PDB_UniProt_without_null_index_PDBe, left_on=\"One_to_N_mmCIF\", right_on=\"PDBe\", how='left')\n",
    "    df_final = df_final.rename(columns={\"PDBe_copy\": \"PDBe\"})\n",
    "    df_final = df_final.drop_duplicates(subset=\"auth_mmCIF\", keep='first')\n",
    "    df_final[\"PDB_num_and_chain\"] = df_final[\"auth_mmCIF\"].apply(lambda x: (x[0], x[2]))\n",
    "    df_final[\"PDBe_num_and_chain\"] = df_final[\"One_to_N_mmCIF\"].apply(lambda x: (x[0], x[2]))\n",
    "\n",
    "    df_final[\"Uni_or_50k_NAN\"] = np.where(df_final[\"One_to_N_mmCIF\"].apply(lambda x: x[0] != \".\"),\n",
    "                                          df_final[\"UniProt_50k\"].apply(lambda x: x),\n",
    "                                          df_final[\"PDB_num_and_chain\"].apply(\n",
    "                                              lambda x: str(int(''.join(filter(str.isdigit, x[0]))) + default_mmCIF_num + 10000)\n",
    "                                              if x[1] in chains_to_change else str(int(''.join(filter(str.isdigit, x[0]))))))\n",
    "    df_final[\"Uni_or_50k\"] = np.where(df_final[\"Uni_or_50k_NAN\"].apply(lambda x: type(x) == float),\n",
    "                                      df_final[\"PDBe_num_and_chain\"].apply(\n",
    "                                          lambda x: \".\" if x[0] == \".\" else str(int(''.join(filter(str.isdigit, x[0]))) + default_mmCIF_num)\n",
    "                                          if x[1] in chains_to_change_one_to_end else str(int(''.join(filter(str.isdigit, x[0]))))),\n",
    "                                      df_final[\"Uni_or_50k_NAN\"].apply(lambda x: x))\n",
    "\n",
    "    df_final_atom_site = df_final[[\"PDBe\", \"PDB\", \"UniProt\", \"PDBe_num_and_chain\", \"PDB_num_and_chain\", \"AccessionID\", \"Uni_or_50k\"]]\n",
    "\n",
    "    return [df_final_atom_site, mmcif_dict]\n",
    "\n",
    "\n",
    "def SIFTS_tree_parser(handle_SIFTS):\n",
    "    tree = ET.parse(handle_SIFTS)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    crossRefDb_list = list()\n",
    "    PDBe_val_tuples_in_list = list()\n",
    "    PDBe_val_tuples_in_list_for_Uni = list()\n",
    "    PDBe_val_tuples_in_list_for_PDB = list()\n",
    "    PDB_val_tuples_in_list = list()\n",
    "    UniProt_val_tuple_in_list = list()\n",
    "    UniProtdbAccessionId_list = list()\n",
    "    UniProt_conversion_dict = dict()\n",
    "    Human_readble_AccessionID_list = list()\n",
    "    details_list = list()\n",
    "\n",
    "    for entity in root:\n",
    "        if entity.tag.endswith(\"entity\"):\n",
    "            entity_chainID_list = list(entity.attrib.items())\n",
    "            if entity_chainID_list[0][0] == \"type\" and entity_chainID_list[0][1] == \"protein\":\n",
    "                for segment in entity:\n",
    "                    for listResidue in segment:\n",
    "                        if listResidue.tag.endswith(\"listMapRegion\"):\n",
    "                            for mapRegion in listResidue:\n",
    "                                for db in mapRegion:\n",
    "                                    dbSourse_Uniprot = list(db.attrib.items())\n",
    "                                    if \"dbSource\" == dbSourse_Uniprot[0][0] and \"UniProt\" == dbSourse_Uniprot[0][1]:\n",
    "                                        if db.text is None:\n",
    "                                            UniProt = dbSourse_Uniprot[2][1]\n",
    "                                        else:\n",
    "                                            Human_readble = db.text\n",
    "                                            UniProt_conversion_dict[UniProt] = Human_readble\n",
    "\n",
    "                        for residue in listResidue:\n",
    "                            key_val_tuples_in_list_parent = list(residue.attrib.items())\n",
    "                            if key_val_tuples_in_list_parent[0][0] == \"dbSource\" and key_val_tuples_in_list_parent[0][1] == \"PDBe\":\n",
    "                                PDBe_val_tuples_in_list.append(\n",
    "                                    (key_val_tuples_in_list_parent[2][1], key_val_tuples_in_list_parent[3][1], entity_chainID_list[1][1]))\n",
    "\n",
    "                                for crossRefDb in residue:\n",
    "                                    if crossRefDb.tag.endswith(\"residueDetail\") == True and crossRefDb.text != \"Not_Observed\":\n",
    "                                        details_list.append(((\"PDBid\", root.get(\"dbAccessionId\")), (\"Annotation:\", crossRefDb.text), (\n",
    "                                        key_val_tuples_in_list_parent[2][1], key_val_tuples_in_list_parent[3][1], entity_chainID_list[1][1])))\n",
    "\n",
    "                                    crossRefDb_list.append(crossRefDb.attrib)\n",
    "                                    key_val_tuples_in_list_child = list(crossRefDb.attrib.items())\n",
    "\n",
    "                                    if key_val_tuples_in_list_child[0][0] == \"dbSource\" and key_val_tuples_in_list_child[0][1] == \"PDB\":\n",
    "                                        PDB_val_tuples_in_list.append((key_val_tuples_in_list_child[3][1], key_val_tuples_in_list_child[4][1],\n",
    "                                                                       key_val_tuples_in_list_child[5][1]))\n",
    "                                        PDBe_val_tuples_in_list_for_PDB.append(\n",
    "                                            (key_val_tuples_in_list_parent[2][1], key_val_tuples_in_list_parent[3][1], entity_chainID_list[1][1]))\n",
    "\n",
    "                                    if key_val_tuples_in_list_child[0][0] == \"dbSource\" and key_val_tuples_in_list_child[0][1] == \"UniProt\":\n",
    "                                        UniProt_val_tuple_in_list.append(\n",
    "                                            (key_val_tuples_in_list_child[3][1], key_val_tuples_in_list_child[4][1], entity_chainID_list[1][1]))\n",
    "                                        PDBe_val_tuples_in_list_for_Uni.append(\n",
    "                                            (key_val_tuples_in_list_parent[2][1], key_val_tuples_in_list_parent[3][1], entity_chainID_list[1][1]))\n",
    "                                        UniProtdbAccessionId_list.append(key_val_tuples_in_list_child[2][1])\n",
    "\n",
    "    tuple_PDBe_for_PDB_and_tuple_PDB = list(zip(PDBe_val_tuples_in_list_for_PDB, PDB_val_tuples_in_list))\n",
    "    tuple_PDBe_for_UniProt_and_tuple_UniProt = list(zip(PDBe_val_tuples_in_list_for_Uni, UniProt_val_tuple_in_list, UniProtdbAccessionId_list))\n",
    "\n",
    "    return [tuple_PDBe_for_PDB_and_tuple_PDB, tuple_PDBe_for_UniProt_and_tuple_UniProt, UniProt_conversion_dict, details_list]\n",
    "\n",
    "\n",
    "def SIFTS_data_parser_for_mmCIF(tuple_PDBe_for_PDB_and_tuple_PDB, tuple_PDBe_for_UniProt_and_tuple_UniProt, default_mmCIF_num,\n",
    "                                chains_to_change=\"all\"):\n",
    "    df_PDBe_UniProt = pd.DataFrame(tuple_PDBe_for_UniProt_and_tuple_UniProt, columns=['PDBe', 'UniProt', \"AccessionID\"])\n",
    "    df_PDBe_UniProt = df_PDBe_UniProt.drop_duplicates(subset=\"PDBe\", keep='first')\n",
    "    df_PDBe_PDB = pd.DataFrame(tuple_PDBe_for_PDB_and_tuple_PDB, columns=['PDBe', 'PDB'])\n",
    "    df_PDBe_PDB = df_PDBe_PDB.drop_duplicates(subset=\"PDBe\", keep='first')\n",
    "\n",
    "    df_PDBe_PDB_UniProt = df_PDBe_PDB.merge(df_PDBe_UniProt, left_on=\"PDBe\", right_on=\"PDBe\", how='left')\n",
    "    df_PDBe_PDB_UniProt['UniProt'] = df_PDBe_PDB_UniProt['UniProt'].replace(np.nan, \"50000\")\n",
    "    df_PDBe_PDB_UniProt[\"Uni_moD\"] = np.where(df_PDBe_PDB_UniProt['UniProt'] != \"50000\", df_PDBe_PDB_UniProt['UniProt'], df_PDBe_PDB_UniProt[\"PDBe\"])\n",
    "    df_PDBe_PDB_UniProt.loc[:, 'new_col_Uni'] = df_PDBe_PDB_UniProt.Uni_moD.map(lambda x: x[0])\n",
    "    df_PDBe_PDB_UniProt[\"UniProt_50k\"] = df_PDBe_PDB_UniProt.new_col_Uni.apply(lambda x: str(int(x) + default_mmCIF_num if type(x) == str else x))\n",
    "    df_PDBe_PDB_UniProt.loc[df_PDBe_PDB_UniProt['UniProt'] != '50000', 'UniProt_50k'] = df_PDBe_PDB_UniProt['new_col_Uni']\n",
    "\n",
    "    Three_Rows_CIF_Num_Uni = []\n",
    "    if chains_to_change == \"all\":\n",
    "        for index, rows in df_PDBe_PDB_UniProt.iterrows():\n",
    "            intermediate_list = [rows.PDBe, rows.UniProt_50k, rows.Uni_moD, rows.PDB, rows.AccessionID]\n",
    "            Three_Rows_CIF_Num_Uni.append(intermediate_list)\n",
    "\n",
    "    else:\n",
    "        for index, rows in df_PDBe_PDB_UniProt.iterrows():\n",
    "            if rows.PDB[2].strip() in chains_to_change:\n",
    "                intermediate_list = [rows.PDBe, rows.UniProt_50k, rows.Uni_moD, rows.PDB, rows.AccessionID]\n",
    "            else:\n",
    "                intermediate_list = [rows.PDBe, rows.PDB[0], rows.Uni_moD, rows.PDB, rows.AccessionID]\n",
    "            Three_Rows_CIF_Num_Uni.append(intermediate_list)\n",
    "\n",
    "    df_PDBe_PDB_UniProt[\"Three_Rows_CIF_Num_Uni\"] = Three_Rows_CIF_Num_Uni\n",
    "    df_PDBe_PDB_UniProt_without_null = df_PDBe_PDB_UniProt[df_PDBe_PDB_UniProt.PDB.map(lambda x: x[0]) != \"null\"]\n",
    "    df_PDBe_PDB_UniProt_without_null_index_PDBe = df_PDBe_PDB_UniProt_without_null.set_index(\"PDBe\")\n",
    "\n",
    "    return [df_PDBe_PDB_UniProt_without_null_index_PDBe, df_PDBe_PDB_UniProt]\n",
    "\n",
    "\n",
    "def master_mmCIF_renumber_function(input_mmCIF_file_were_found, default_input_path_to_mmCIF,\n",
    "                                   default_input_path_to_SIFTS, default_output_path_to_mmCIF,\n",
    "                                   default_mmCIF_num, gzip_mode, exception_AccessionIDs):\n",
    "    input_mmCIF_assembly_files_were_found_list = list()\n",
    "    input_mmCIF_assembly_files_were_found_list.append(input_mmCIF_file_were_found)\n",
    "\n",
    "    for mmCIF_name in input_mmCIF_assembly_files_were_found_list:\n",
    "        log_message = list()\n",
    "        SIFTS_name = mmCIF_name[:4] + \".xml.gz\"\n",
    "\n",
    "        # for no SIFTS _no_SIFTS_out.cif.gz\n",
    "        try:\n",
    "            gzip.open(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name), 'rt')\n",
    "        except FileNotFoundError:\n",
    "            mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "            if mmcif_dict == 0:\n",
    "                continue\n",
    "            copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "            log_message = if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message)\n",
    "            return log_message\n",
    "\n",
    "        # for zerobyte SIFTS _zerobyte_SIFTS_out.cif.gz\n",
    "        if os.path.getsize(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name)) == 0:\n",
    "            mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "            if mmcif_dict == 0:\n",
    "                continue\n",
    "            copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "            log_message = if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message)\n",
    "            return log_message\n",
    "\n",
    "        product_tree_SIFTS = try_SIFTS_tree_parser(default_input_path_to_SIFTS, SIFTS_name)\n",
    "        if product_tree_SIFTS == 0:\n",
    "            continue\n",
    "\n",
    "        tuple_PDBe_for_PDB_and_tuple_PDB = product_tree_SIFTS[0]\n",
    "        tuple_PDBe_for_UniProt_and_tuple_UniProt = product_tree_SIFTS[1]\n",
    "        UniProt_conversion_dict = product_tree_SIFTS[2]\n",
    "\n",
    "        # _no UniProt in SIFTS _no_UniProt_in_SIFTS_out.cif.gz\n",
    "        if tuple_PDBe_for_UniProt_and_tuple_UniProt == list():\n",
    "            mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "            if mmcif_dict == 0:\n",
    "                continue\n",
    "            copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "            log_message = if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message)\n",
    "            return log_message\n",
    "\n",
    "        product_of_SIFTS_data_parser = SIFTS_data_parser_for_mmCIF(tuple_PDBe_for_PDB_and_tuple_PDB, tuple_PDBe_for_UniProt_and_tuple_UniProt,\n",
    "                                                                   default_mmCIF_num, 'all')\n",
    "        df_PDBe_PDB_UniProt = product_of_SIFTS_data_parser[1]\n",
    "\n",
    "        # all good till here\n",
    "        handling_chain_numbering = handling_chain_numbering_clashes(df_PDBe_PDB_UniProt, exception_AccessionIDs)\n",
    "        chains_to_change = handling_chain_numbering[0]\n",
    "        combined_tuple_PDBe_UniProt_AccessionID = handling_chain_numbering[1]\n",
    "        longest_AccessionID_list = handling_chain_numbering[3]\n",
    "        chains_to_change_one_to_end = handling_chain_numbering[4]\n",
    "\n",
    "        product_of_SIFTS_data_parser = SIFTS_data_parser_for_mmCIF(tuple_PDBe_for_PDB_and_tuple_PDB, combined_tuple_PDBe_UniProt_AccessionID,\n",
    "                                                                   default_mmCIF_num, chains_to_change)\n",
    "        df_PDBe_PDB_UniProt_without_null_index_PDBe = product_of_SIFTS_data_parser[0]\n",
    "        df_PDBe_PDB_UniProt = product_of_SIFTS_data_parser[1]\n",
    "\n",
    "        renumbered_count = renumbered_count_in_chains(chains_to_change_one_to_end, df_PDBe_PDB_UniProt_without_null_index_PDBe,\n",
    "                                                      mmCIF_name, UniProt_conversion_dict, longest_AccessionID_list)\n",
    "        chain_total_renum = renumbered_count[0]\n",
    "        nothing_changed = renumbered_count[1]\n",
    "\n",
    "        chain_total_renum.append(nothing_changed)\n",
    "        mod_log_message = chain_total_renum\n",
    "\n",
    "        # for no change needed _no_change_out.cif.gz\n",
    "        if nothing_changed == 0:\n",
    "            copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "            return mod_log_message\n",
    "\n",
    "        product_of_mmCIF_parser = mmCIF_parser(mmCIF_name, default_input_path_to_mmCIF, df_PDBe_PDB_UniProt_without_null_index_PDBe,\n",
    "                                               default_mmCIF_num, chains_to_change, chains_to_change_one_to_end)\n",
    "        df_final_atom_site = product_of_mmCIF_parser[0]\n",
    "        mmcif_dict = product_of_mmCIF_parser[1]\n",
    "\n",
    "        poly_nonpoly_append = poly_nonpoly_renum(mmcif_dict, df_PDBe_PDB_UniProt, chains_to_change, default_mmCIF_num)\n",
    "        poly_nonpoly_atom_site = poly_nonpoly_append.append(df_final_atom_site).drop_duplicates(subset=\"PDB_num_and_chain\", keep='first')\n",
    "\n",
    "        formed_columns = column_formation(mmcif_dict)\n",
    "        renumber_tables(formed_columns, mmcif_dict, poly_nonpoly_atom_site, chains_to_change, default_mmCIF_num)\n",
    "\n",
    "        try:\n",
    "            output_with_this_name_ending(\"_renum.cif\", default_output_path_to_mmCIF, mmcif_dict, mmCIF_name=mmCIF_name,\n",
    "                                         gzip_mode=gzip_mode, current_directory=current_directory)\n",
    "            return mod_log_message\n",
    "        except IndexError:\n",
    "            # 5olg data swapped columns\n",
    "            print(\"IndexError Warning this file is not renumbered:\", mmCIF_name)\n",
    "            copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # # # # troubleshooting\n",
    "# # # some_err_list = list()\n",
    "# # # # # #############################################################################################################################################################################\n",
    "# # # all_keys = set()\n",
    "# # multi_Uni_no_clash = set()\n",
    "# # # for input_mmCIF_file_were_found in tqdm.tqdm(input_mmCIF_files_were_found[481:10000], total=len(input_mmCIF_files_were_found[481:10000]), position=0, leave=True, desc=\"Checking files\"):\n",
    "    \n",
    "    \n",
    "# def master_mmCIF_renumber_function(input_mmCIF_file_were_found, default_input_path_to_mmCIF,\n",
    "#                                    default_input_path_to_SIFTS, default_output_path_to_mmCIF,\n",
    "#                                    default_mmCIF_num, gzip_mode, exception_AccessionIDs):\n",
    "\n",
    "#     input_mmCIF_assembly_files_were_found_list = list()\n",
    "#     input_mmCIF_assembly_files_were_found_list.append(input_mmCIF_file_were_found)\n",
    "\n",
    "#     for mmCIF_name in input_mmCIF_assembly_files_were_found_list:\n",
    "#         log_message = list()\n",
    "#         SIFTS_name = mmCIF_name[:4] + \".xml.gz\"\n",
    "\n",
    "#         # for no SIFTS _no_SIFTS_out.cif.gz\n",
    "#         try:\n",
    "#             gzip.open(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name), 'rt')\n",
    "#         except FileNotFoundError:\n",
    "#             mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "#             if mmcif_dict == 0:\n",
    "#                 continue\n",
    "#             copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "#             log_message = if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message)\n",
    "#             continue\n",
    "#             # return log_message\n",
    "\n",
    "#         # for zerobyte SIFTS _zerobyte_SIFTS_out.cif.gz\n",
    "#         if os.path.getsize(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name)) == 0:\n",
    "#             mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "#             if mmcif_dict == 0:\n",
    "#                 continue\n",
    "#             copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "#             log_message = if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message)\n",
    "#             continue\n",
    "#             # return log_message\n",
    "\n",
    "#         product_tree_SIFTS = try_SIFTS_tree_parser(default_input_path_to_SIFTS, SIFTS_name)\n",
    "#         if product_tree_SIFTS == 0:\n",
    "#             continue\n",
    "\n",
    "#         tuple_PDBe_for_PDB_and_tuple_PDB = product_tree_SIFTS[0]\n",
    "#         tuple_PDBe_for_UniProt_and_tuple_UniProt = product_tree_SIFTS[1]\n",
    "#         UniProt_conversion_dict = product_tree_SIFTS[2]\n",
    "        \n",
    "\n",
    "#         # _no UniProt in SIFTS _no_UniProt_in_SIFTS_out.cif.gz\n",
    "#         if tuple_PDBe_for_UniProt_and_tuple_UniProt == list():\n",
    "#             mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "#             if mmcif_dict == 0:\n",
    "#                 continue\n",
    "#             copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "#             log_message = if_no_SIFTS_data_log(mmCIF_name, mmcif_dict, log_message)\n",
    "#             continue\n",
    "#             # return log_message\n",
    "\n",
    "#         product_of_SIFTS_data_parser = SIFTS_data_parser_for_mmCIF(tuple_PDBe_for_PDB_and_tuple_PDB, tuple_PDBe_for_UniProt_and_tuple_UniProt,\n",
    "#                                                                    default_mmCIF_num, 'all')\n",
    "#         df_PDBe_PDB_UniProt = product_of_SIFTS_data_parser[1]\n",
    "        \n",
    "        \n",
    "\n",
    "#         ### all good till here\n",
    "#         handling_chain_numbering = handling_chain_numbering_clashes(df_PDBe_PDB_UniProt, exception_AccessionIDs)\n",
    "#         chains_to_change = handling_chain_numbering[0]\n",
    "#         combined_tuple_PDBe_UniProt_AccessionID = handling_chain_numbering[1]\n",
    "#         longest_AccessionID_list = handling_chain_numbering[3]\n",
    "#         chains_to_change_one_to_end = handling_chain_numbering[4]\n",
    "\n",
    "#         product_of_SIFTS_data_parser = SIFTS_data_parser_for_mmCIF(tuple_PDBe_for_PDB_and_tuple_PDB, combined_tuple_PDBe_UniProt_AccessionID,\n",
    "#                                                                    default_mmCIF_num, chains_to_change)\n",
    "#         df_PDBe_PDB_UniProt_without_null_index_PDBe = product_of_SIFTS_data_parser[0]\n",
    "#         df_PDBe_PDB_UniProt = product_of_SIFTS_data_parser[1]\n",
    "\n",
    "#         renumbered_count = renumbered_count_in_chains(chains_to_change_one_to_end, df_PDBe_PDB_UniProt_without_null_index_PDBe,\n",
    "#                                                       mmCIF_name, UniProt_conversion_dict, longest_AccessionID_list)\n",
    "#         chain_total_renum = renumbered_count[0]\n",
    "#         nothing_changed = renumbered_count[1]\n",
    "\n",
    "#         chain_total_renum.append(nothing_changed)\n",
    "#         mod_log_message = chain_total_renum\n",
    "\n",
    "#         # for no change needed _no_change_out.cif.gz\n",
    "#         if nothing_changed == 0:\n",
    "#             copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "#             continue\n",
    "#             # return mod_log_message\n",
    "\n",
    "#         product_of_mmCIF_parser = mmCIF_parser(mmCIF_name, default_input_path_to_mmCIF, df_PDBe_PDB_UniProt_without_null_index_PDBe,\n",
    "#                                                default_mmCIF_num, chains_to_change, chains_to_change_one_to_end)\n",
    "#         df_final_atom_site = product_of_mmCIF_parser[0]\n",
    "#         mmcif_dict = product_of_mmCIF_parser[1]\n",
    "\n",
    "#         poly_nonpoly_append = poly_nonpoly_renum(mmcif_dict, df_PDBe_PDB_UniProt, chains_to_change, default_mmCIF_num)\n",
    "#         poly_nonpoly_atom_site = poly_nonpoly_append.append(df_final_atom_site).drop_duplicates(subset=\"PDB_num_and_chain\", keep='first')\n",
    "        \n",
    "        \n",
    "#         formed_columns = column_formation(mmcif_dict)\n",
    "#         # mmcif_dict = renumber_tables(formed_columns, mmcif_dict, poly_nonpoly_atom_site, chains_to_change, default_mmCIF_num)\n",
    "\n",
    "#         try:\n",
    "#             output_with_this_name_ending(\".cif\", default_output_path_to_mmCIF, mmcif_dict, mmCIF_name=mmCIF_name,\n",
    "#                                          gzip_mode=gzip_mode, current_directory=current_directory)\n",
    "#             return mod_log_message\n",
    "#         except IndexError:\n",
    "#             # 5olg data swapped columns\n",
    "#             print(\"IndexError Warning this file is not renumbered:\", mmCIF_name)\n",
    "#             copy_file(default_input_path_to_mmCIF, mmCIF_name, default_output_path_to_mmCIF, \".cif.gz\", gzip_mode)\n",
    "        \n",
    "        \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_writer(resulting):\n",
    "    with open('log_corrected.txt', 'w') as f:\n",
    "        compuni_humanuni_PDBid = list()\n",
    "        pdb_id_set = set()\n",
    "        formated_item = (format(\"SP\", \"<3\") + format(\"PDB_id\", \"<7\")  + format(\"chain_PDB\", \"<12\") + format(\"chain_auth\", \"<12\") + format(\"UniProt\", \"<20\") + format(\"SwissProt\", \"<20\") \n",
    "                         + format(\"uni_len\", \">10\") + format(\"chain_len\", \">10\") + format(\"renum\", \">10\") + format(\"5k_or_50k\", \">10\"))\n",
    "        f.write(\"%s\\n\" % formated_item)\n",
    "\n",
    "        for n in resulting:\n",
    "            if type(n) == list:\n",
    "                for z in n:\n",
    "                    if type(z) == list:\n",
    "                        if z[0][-1] == \"*\":\n",
    "                            try:\n",
    "                                formated_item = (format(\"*\", \"<3\") + format(z[0][:4], \"<7\") + format(z[1], \"<12\") + format(z[2], \"<12\") + format(z[3], \"<20\") + format(z[4], \"<20\") \n",
    "                                + format(z[5], \">10\")  + format(z[6], \">10\") + format(z[7], \">10\")  + format(z[8], \">10\"))\n",
    "                                pdb_id_set.add(z[0][:4])\n",
    "                                compuni_humanuni_PDBid.append((z[3], z[4], z[0][:4]))\n",
    "                            except:\n",
    "                                print(z)\n",
    "                        else:\n",
    "                            try:\n",
    "                                formated_item = (format(\"+\", \"<3\") + format(z[0], \"<7\") + format(z[1], \"<12\") + format(z[2], \"<12\") + format(z[3], \"<20\") + format(z[4], \"<20\") \n",
    "                                + format(z[5], \">10\") + format(z[6], \">10\") + format(z[7], \">10\")  + format(z[8], \">10\"))\n",
    "                                pdb_id_set.add(z[0])\n",
    "                                compuni_humanuni_PDBid.append((z[3], z[4], z[0][:4]))\n",
    "                            except:\n",
    "                                print(z)\n",
    "                        f.write(\"%s\\n\" % formated_item)\n",
    "\n",
    "\n",
    "    uniq_compuni_humanuni_PDBid_translation = set()\n",
    "    for n in compuni_humanuni_PDBid:\n",
    "        if n[0] == \"-\":\n",
    "            continue\n",
    "        uniq_compuni_humanuni_PDBid_translation.add(n)\n",
    "\n",
    "    with open('log_translator.txt', 'w') as filehandle:\n",
    "        for listitem in uniq_compuni_humanuni_PDBid_translation:\n",
    "            filehandle.write(listitem[0] + \" \" + listitem[1] + \" \" + listitem[2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(mmCIF_name)\n",
    "# for n in formed_columns[:2]:\n",
    "#     print(n[0])\n",
    "#     print(len(mmcif_dict[n[0]]))\n",
    "#     print(mmcif_dict[n[0]])\n",
    "#     print(n[2])\n",
    "#     print(len(mmcif_dict[n[2]]))\n",
    "#     print(mmcif_dict[n[2]])\n",
    "#     if len(mmcif_dict[n[0]]) != len(mmcif_dict[n[2]]):\n",
    "#         print(n[0])\n",
    "#     try:\n",
    "#         print(n[2])\n",
    "#         print(mmcif_dict[n[2]])\n",
    "#         for a in mmcif_dict[n[2]]:\n",
    "#             if a != \"?\":\n",
    "#                 print(a)\n",
    "#                 mmcif_dict[n[2]]\n",
    "#     except IndexError:\n",
    "#         pass\n",
    "\n",
    "\n",
    "#     #########################################################################################################################################\n",
    "\n",
    "#     dot_or_question_tuple = (\".\", \"?\")\n",
    "#     for n in formed_columns[11:12]:\n",
    "#         auth_comp_id = 0\n",
    "#         auth_seq_id = n[0]\n",
    "#         auth_asym_id = n[1]\n",
    "#         try:\n",
    "#             PDB_ins_code = n[2]\n",
    "#             if \"ins_code\" not in PDB_ins_code:\n",
    "#                 auth_comp_id = PDB_ins_code\n",
    "#                 PDB_ins_code = 0\n",
    "#         except IndexError:\n",
    "#             PDB_ins_code = 0\n",
    "#         try:\n",
    "#             if auth_comp_id == 0:\n",
    "#                 auth_comp_id = n[3]\n",
    "#         except IndexError:\n",
    "#             auth_comp_id = 0\n",
    "\n",
    "#         if \"_pdbx_branch_scheme\" in auth_seq_id:\n",
    "#             auth_seq_id = \"_pdbx_branch_scheme.pdb_seq_num\"\n",
    "#             auth_asym_id = \"_pdbx_branch_scheme.pdb_asym_id\"\n",
    "\n",
    "#         PDB_ins_code_list = list()\n",
    "#         # auth_comp_id_list = mmcif_dict[auth_comp_id] #for debug only\n",
    "#         auth_seq_id_list = mmcif_dict[auth_seq_id]\n",
    "#         auth_asym_id_list = mmcif_dict[auth_asym_id]\n",
    "\n",
    "#         if PDB_ins_code == 0:\n",
    "#             for _ in range(len(auth_seq_id_list)):\n",
    "#                 PDB_ins_code_list.append(\"?\")\n",
    "#         else:\n",
    "#             PDB_ins_code_list = mmcif_dict[PDB_ins_code]\n",
    "\n",
    "#         if type(auth_asym_id_list) == str:\n",
    "#             # auth_comp_id_list = [auth_comp_id_list] for debug only\n",
    "#             auth_seq_id_list = [auth_seq_id_list]\n",
    "#             auth_asym_id_list = [auth_asym_id_list]\n",
    "\n",
    "#             if PDB_ins_code == 0:\n",
    "#                 PDB_ins_code_list = [\"?\"]\n",
    "#             else:\n",
    "#                 PDB_ins_code_list = [PDB_ins_code]\n",
    "\n",
    "#         if PDB_ins_code != 0:\n",
    "#             dot_to_question = list()\n",
    "#             for ins_code in mmcif_dict[PDB_ins_code]:\n",
    "#                 if ins_code == \".\":\n",
    "#                     dot_to_question.append(\"?\")\n",
    "#                 else:\n",
    "#                     dot_to_question.append(ins_code)\n",
    "#             PDB_ins_code_list = dot_to_question\n",
    "\n",
    "#         auth_seq_id_list_zip = list(zip(auth_seq_id_list, auth_asym_id_list))\n",
    "#         df_mmCIF_auth_seq_id_list_zip = pd.DataFrame(zip(auth_seq_id_list_zip, PDB_ins_code_list))\n",
    "#         df_mmCIF_auth_seq_id_list_zip = df_mmCIF_auth_seq_id_list_zip.rename(columns={0: \"auth_seq_id_list_zip\", 1: \"ins_code\"})\n",
    "\n",
    "#         df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code\"] = np.where(df_mmCIF_auth_seq_id_list_zip['ins_code'] != \"?\",\n",
    "#                                                                       (df_mmCIF_auth_seq_id_list_zip['auth_seq_id_list_zip'].apply(lambda x: x[0])\n",
    "#                                                                        + df_mmCIF_auth_seq_id_list_zip['ins_code'].apply(lambda y: y[0]) + \",\"\n",
    "#                                                                        + df_mmCIF_auth_seq_id_list_zip['auth_seq_id_list_zip'].apply(lambda x: x[1])),\n",
    "#                                                                       df_mmCIF_auth_seq_id_list_zip['ins_code'])\n",
    "\n",
    "#         df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code_cor\"] = np.where(df_mmCIF_auth_seq_id_list_zip['PDB_with_ins_code'] != \"?\",\n",
    "#                                                                           df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code\"].apply(\n",
    "#                                                                               lambda x: tuple(x.split(\",\"))),\n",
    "#                                                                           df_mmCIF_auth_seq_id_list_zip[\"auth_seq_id_list_zip\"])\n",
    "\n",
    "#         df_mmCIF_auth_seq_id_list_zip[\"auth_seq_id_list_zip\"] = df_mmCIF_auth_seq_id_list_zip[\"PDB_with_ins_code_cor\"]\n",
    "#         df_mmCIF_auth_seq_id_list_zip = df_mmCIF_auth_seq_id_list_zip.drop(columns=[\"PDB_with_ins_code_cor\", \"ins_code\", \"PDB_with_ins_code\"])\n",
    "\n",
    "#         df_auth_seq_id_list_zip_final = df_mmCIF_auth_seq_id_list_zip.merge(poly_nonpoly_atom_site, left_on=\"auth_seq_id_list_zip\",\n",
    "#                                                                             right_on=\"PDB_num_and_chain\", how='left')\n",
    "\n",
    "#         df_auth_seq_id_list_zip_final[\"question_mark\"] = np.where(\n",
    "#             df_auth_seq_id_list_zip_final[\"auth_seq_id_list_zip\"].apply(lambda x: x[0] in dot_or_question_tuple),\n",
    "#             df_auth_seq_id_list_zip_final[\"auth_seq_id_list_zip\"].apply(lambda x: x[0]),\n",
    "#             df_auth_seq_id_list_zip_final[\"Uni_or_50k\"].apply(lambda x: x))\n",
    "#         try:\n",
    "#             df_auth_seq_id_list_zip_final[\"final\"] = np.where(df_auth_seq_id_list_zip_final[\"question_mark\"].apply(lambda x: type(x) == float),\n",
    "#                                                               df_auth_seq_id_list_zip_final[\"auth_seq_id_list_zip\"].apply(\n",
    "#                                                                   lambda x: \".\" if x[0] == \".\" else\n",
    "#                                                                   \"?\" if x[0] == \"?\" else str(\n",
    "#                                                                       int(''.join(filter(str.isdigit, str(x[0])))) + default_mmCIF_num)\n",
    "#                                                                   if x[1] in chains_to_change else str(int(''.join(filter(str.isdigit, str(x[0])))))),\n",
    "#                                                               df_auth_seq_id_list_zip_final[\"question_mark\"].apply(lambda x: x))\n",
    "#         except ValueError:\n",
    "#             pass\n",
    "#             #return print(\"ValueError in table \" + auth_seq_id + \" has non-numeric value point in file \" + mmcif_dict[\"data_\"])\n",
    "\n",
    "#         df_auth_seq_id_list_zip_final[\"ins_code\"] = df_auth_seq_id_list_zip_final[\"final\"].apply(lambda x: \"?\"\n",
    "#         if re.sub('[0-9]+', '', x).strip(\"-\").strip(\".\").strip('?') == \"\"\n",
    "#         else re.sub('[0-9]+', '', x).strip(\"-\").strip(\".\").strip('?'))\n",
    "#         df_auth_seq_id_list_zip_final[\"final\"] = df_auth_seq_id_list_zip_final[\"final\"].apply(lambda x: x.strip(re.sub('[0-9\\-\\?\\.]+', '', x)))\n",
    "\n",
    "#         for num in df_auth_seq_id_list_zip_final[\"final\"]:\n",
    "#             if num == \"\":\n",
    "#                 print(\"Empty str\")\n",
    "#             if type(num) == float:\n",
    "#                 print(\"Float or npNAN\")\n",
    "\n",
    "#         # actual replacing auth_num with UniProt_num and of ins_code with '?'\n",
    "\n",
    "# #         PDB_ins_code_list = list()\n",
    "# #         if PDB_ins_code != 0:\n",
    "# #             if \".\" in mmcif_dict[PDB_ins_code]:\n",
    "# #                 for ins in df_auth_seq_id_list_zip_final[\"ins_code\"].values:\n",
    "# #                     if \"?\" == ins:\n",
    "# #                         PDB_ins_code_list.append(\".\")\n",
    "# #                     else:\n",
    "# #                         PDB_ins_code_list.append(ins)\n",
    "# #                 mmcif_dict[PDB_ins_code] = PDB_ins_code_list\n",
    "# #             else:\n",
    "# #                 mmcif_dict[PDB_ins_code] = list(df_auth_seq_id_list_zip_final[\"ins_code\"].values)\n",
    "\n",
    "# #         if \"_pdbx_branch_scheme\" in auth_seq_id:\n",
    "# #             mmcif_dict[\"_pdbx_branch_scheme.auth_seq_num\"] = list(df_auth_seq_id_list_zip_final[\"final\"].values)\n",
    "# #         else:\n",
    "# #             mmcif_dict[auth_seq_id] = list(df_auth_seq_id_list_zip_final[\"final\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_assemblies(mmCIF_assembly, default_output_path_to_mmCIF_assembly):\n",
    "    output_mmCIF_assembly_files_were_found_list = list()\n",
    "    output_mmCIF_assembly_files_were_found_list.append(mmCIF_assembly)\n",
    "    new_order_with_remark = 0\n",
    "    \n",
    "    for name in output_mmCIF_assembly_files_were_found_list:\n",
    "        not_gzip = 1\n",
    "        try:\n",
    "            list_of_lines_from_assembly_file = gzip.open(\n",
    "                Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), 'rt').readlines()\n",
    "        except OSError:\n",
    "            # maybe not archived\n",
    "            try:\n",
    "                list_of_lines_from_assembly_file = open(\n",
    "                    Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), 'rt').readlines()\n",
    "                not_gzip = 0\n",
    "            except Exception:\n",
    "                # broken archive\n",
    "                os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "                continue\n",
    "        except Exception:\n",
    "            # broken archive\n",
    "            os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            continue\n",
    "\n",
    "        # check if file startswith \"_atom_site\" table at the beginning\n",
    "        try:\n",
    "            if \"_atom_site\" in list_of_lines_from_assembly_file[3] and \"loop_\" in list_of_lines_from_assembly_file[2]:\n",
    "                pass\n",
    "            else:\n",
    "                if \"renum\" in name:\n",
    "                    for line in list_of_lines_from_assembly_file:\n",
    "                        if line.startswith(\"_entry.id\"):\n",
    "                            new_order_with_remark = (list_of_lines_from_assembly_file[:(list_of_lines_from_assembly_file.index(line)) + 1] \n",
    "                                                     + REMARK_mmCIF \n",
    "                                                     + list_of_lines_from_assembly_file[(list_of_lines_from_assembly_file.index(line)) + 2:])\n",
    "                    if new_order_with_remark == 0:\n",
    "                        new_order_with_remark = list_of_lines_from_assembly_file\n",
    "                        print(name)\n",
    "\n",
    "                    if not_gzip != 0:\n",
    "                        with gzip.open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), \"wt\") as gzip_out:\n",
    "                            for listitem in new_order_with_remark:\n",
    "                                gzip_out.write(listitem)\n",
    "\n",
    "                    else:\n",
    "                        with open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), \"wt\") as file_out:\n",
    "                            for listitem in new_order_with_remark:\n",
    "                                file_out.write(listitem)\n",
    "                                \n",
    "                else:\n",
    "                    new_order_with_remark = list_of_lines_from_assembly_file\n",
    "                    if not_gzip != 0:\n",
    "                        with gzip.open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name.split(\".\")[0] + \"_renum.cif.gz\"), \"wt\") as gzip_out:\n",
    "                            for listitem in new_order_with_remark:\n",
    "                                gzip_out.write(listitem)\n",
    "                        os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "                    else:\n",
    "                        with open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name.split(\".\")[0] + \"_renum.cif\"), \"wt\") as file_out:\n",
    "                            for listitem in new_order_with_remark:\n",
    "                                file_out.write(listitem)\n",
    "                        os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "                    \n",
    "                return name\n",
    "\n",
    "        \n",
    "        except IndexError:\n",
    "            # empty file\n",
    "            os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            new_order_for_assembly_file = (list_of_lines_from_assembly_file[:1]\n",
    "                                           + list_of_lines_from_assembly_file[list_of_lines_from_assembly_file.index(\"#\\n\", 2):]\n",
    "                                           + list_of_lines_from_assembly_file[2:list_of_lines_from_assembly_file.index(\"#\\n\", 2)]\n",
    "                                           + [\"#\\n\"])\n",
    "            \n",
    "            if \"renum\" in name:\n",
    "                for line in new_order_for_assembly_file:\n",
    "                    if line.startswith(\"_entry.id\"):\n",
    "                        new_order_with_remark = (new_order_for_assembly_file[:(new_order_for_assembly_file.index(line)) + 1] \n",
    "                                                 + REMARK_mmCIF \n",
    "                                                 + new_order_for_assembly_file[(new_order_for_assembly_file.index(line)) + 2:])\n",
    "\n",
    "                if new_order_with_remark != 0:\n",
    "                    if not_gzip != 0:\n",
    "                        with gzip.open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), \"wt\") as gzip_out:\n",
    "                            for listitem in new_order_with_remark:\n",
    "                                gzip_out.write(listitem)\n",
    "\n",
    "                    else:\n",
    "                        with open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), \"wt\") as file_out:\n",
    "                            for listitem in new_order_with_remark:\n",
    "                                file_out.write(listitem)\n",
    "            else:\n",
    "                new_order_with_remark = new_order_for_assembly_file\n",
    "                if not_gzip != 0:\n",
    "                    with gzip.open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name.split(\".\")[0] + \"_renum.cif.gz\"), \"wt\") as gzip_out:\n",
    "                        for listitem in new_order_with_remark:\n",
    "                            gzip_out.write(listitem)\n",
    "                    os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "                else:\n",
    "                    with open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name.split(\".\")[0] + \"_renum.cif\"), \"wt\") as file_out:\n",
    "                        for listitem in new_order_with_remark:\n",
    "                            file_out.write(listitem)\n",
    "                    os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            return name\n",
    "\n",
    "        except ValueError:\n",
    "            # file isn't complete\n",
    "            os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            \n",
    "\n",
    "\n",
    "def ProcessPool_run_renum_mmCIF(format_mmCIF, mmCIF_to_renumber, default_input_path_to_mmCIF,\n",
    "                                default_input_path_to_SIFTS, default_output_path_to_mmCIF, default_mmCIF_num,\n",
    "                                gzip_mode, exception_AccessionIDs, nproc):\n",
    "    first_res = 0\n",
    "\n",
    "    for i in range(3):\n",
    "        if not os.path.exists(default_output_path_to_mmCIF):\n",
    "            os.makedirs(default_output_path_to_mmCIF)\n",
    "\n",
    "        # renumber loop\n",
    "        resulting = list()\n",
    "        executor = ProcessPoolExecutor(max_workers=nproc)\n",
    "        partial_master_mmCIF_renumber_function = partial(master_mmCIF_renumber_function,\n",
    "                                                         default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "                                                         default_input_path_to_SIFTS=default_input_path_to_SIFTS,\n",
    "                                                         default_output_path_to_mmCIF=default_output_path_to_mmCIF,\n",
    "                                                         default_mmCIF_num=default_mmCIF_num, gzip_mode=gzip_mode,\n",
    "                                                         exception_AccessionIDs=exception_AccessionIDs)\n",
    "        jobs = [executor.submit(partial_master_mmCIF_renumber_function, mmCIF_files) for mmCIF_files in mmCIF_to_renumber]\n",
    "        for job in tqdm.tqdm(as_completed(jobs), total=len(jobs), miniters=1, position=0,\n",
    "                             leave=True, desc=\"Renumbering \" + format_mmCIF + \" files\"):\n",
    "            result = job.result()\n",
    "            if result is None:\n",
    "                continue\n",
    "            resulting.append(result)\n",
    "\n",
    "        if i == 0:\n",
    "            first_res = resulting\n",
    "\n",
    "        if format_mmCIF == \"mmCIF_assembly\":\n",
    "            output_mmCIF = look_what_is_inside('output_mmCIF_assembly', default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF)\n",
    "        else:\n",
    "            output_mmCIF = look_what_is_inside('output_mmCIF', default_output_path_to_mmCIF=default_output_path_to_mmCIF)\n",
    "\n",
    "        # checker loop\n",
    "        check_list = list()\n",
    "        executor = ProcessPoolExecutor(max_workers=nproc)\n",
    "        partial_reform_assembly = partial(check_assemblies, default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF)\n",
    "        jobs = [executor.submit(partial_reform_assembly, assembly_files) for assembly_files in output_mmCIF]\n",
    "        for job in tqdm.tqdm(as_completed(jobs), total=len(jobs), miniters=1, position=0,\n",
    "                             leave=True, desc=\"Checking \" + format_mmCIF + \" files\"):\n",
    "            resultus = job.result()\n",
    "            check_list.append(resultus)\n",
    "\n",
    "        if format_mmCIF == \"mmCIF_assembly\":\n",
    "            output_mmCIF = look_what_is_inside('output_mmCIF_assembly', default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF)\n",
    "        else:\n",
    "            output_mmCIF = look_what_is_inside('output_mmCIF', default_output_path_to_mmCIF=default_output_path_to_mmCIF)\n",
    "        \n",
    "        output_mmCIF_4char = set()\n",
    "        for n in output_mmCIF:\n",
    "            output_mmCIF_4char.add(n[:4])\n",
    "            \n",
    "        if len(check_list) <= len(output_mmCIF):\n",
    "            break\n",
    "        else:\n",
    "            new_round_mmCIF_to_renumber = set()\n",
    "            for n in mmCIF_to_renumber:\n",
    "                if n[:4] in output_mmCIF_4char:\n",
    "                    continue\n",
    "                else:\n",
    "                    new_round_mmCIF_to_renumber.add(n)\n",
    "            mmCIF_to_renumber = new_round_mmCIF_to_renumber        \n",
    "\n",
    "    return first_res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renumbering mmCIF_assembly files: 100%|██████████| 100/100 [00:18<00:00,  5.36it/s]\n",
      "Checking mmCIF_assembly files: 100%|██████████| 265343/265343 [57:09<00:00, 77.37it/s]  \n",
      "Renumbering mmCIF_assembly files: 0it [00:00, ?it/s]\n",
      "Checking mmCIF_assembly files: 100%|██████████| 265293/265293 [56:20<00:00, 78.48it/s]  \n"
     ]
    }
   ],
   "source": [
    "input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF_assembly\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    resulting1 = ProcessPool_run_renum_mmCIF(\"mmCIF_assembly\", input_mmCIF_files_were_found[:100], default_input_path_to_mmCIF_assembly,\n",
    "                                             default_input_path_to_SIFTS, default_output_path_to_mmCIF_assembly, default_mmCIF_num,\n",
    "                                             gzip_mode, exception_AccessionIDs, nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renumbering mmCIF files:  45%|████▍     | 79296/176507 [2:38:19<3:25:34,  7.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ValueError in table _pdbx_refine_tls_group.end_auth_seq_id has non-numeric value point in file 5OLG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renumbering mmCIF files: 100%|██████████| 176507/176507 [5:53:43<00:00,  8.32it/s]   \n",
      "Checking mmCIF files: 100%|██████████| 176507/176507 [39:52<00:00, 73.77it/s]  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF\")\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     resulting2 = ProcessPool_run_renum_mmCIF(\"mmCIF\", input_mmCIF_files_were_found, default_input_path_to_mmCIF,\n",
    "#                                              default_input_path_to_SIFTS, default_output_path_to_mmCIF, default_mmCIF_num,\n",
    "#                                              gzip_mode, exception_AccessionIDs, nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'resulting' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-1f384bf73e79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresulting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'resulting' is not defined"
     ]
    }
   ],
   "source": [
    "resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_writer(resulting):\n",
    "    with open('log_corrected.txt', 'w') as f:\n",
    "        compuni_humanuni_PDBid = list()\n",
    "        pdb_id_set = set()\n",
    "        formated_item = (format(\"SP\", \"<3\") + format(\"PDB_id\", \"<7\")  + format(\"chain_PDB\", \"<12\") + format(\"chain_auth\", \"<12\") + format(\"UniProt\", \"<20\") + format(\"SwissProt\", \"<20\") \n",
    "                         + format(\"uni_len\", \">10\") + format(\"chain_len\", \">10\") + format(\"renum\", \">10\") + format(\"5k_or_50k\", \">10\"))\n",
    "        f.write(\"%s\\n\" % formated_item)\n",
    "\n",
    "        for n in resulting:\n",
    "            for z in n:\n",
    "                if type(z) == list:\n",
    "                    if z[0][-1] == \"*\":\n",
    "                        try:\n",
    "                            formated_item = (format(\"*\", \"<3\") + format(z[0][:4], \"<7\") + format(z[1], \"<12\") + format(z[2], \"<12\") + format(z[3], \"<20\") + format(z[4], \"<20\") \n",
    "                            + format(z[5], \">10\")  + format(z[6], \">10\") + format(z[7], \">10\")  + format(z[8], \">10\"))\n",
    "                            pdb_id_set.add(z[0][:4])\n",
    "                            compuni_humanuni_PDBid.append((z[3], z[4], z[0][:4]))\n",
    "                        except:\n",
    "                            print(z)\n",
    "                    else:\n",
    "                        try:\n",
    "                            formated_item = (format(\"+\", \"<3\") + format(z[0], \"<7\") + format(z[1], \"<12\") + format(z[2], \"<12\") + format(z[3], \"<20\") + format(z[4], \"<20\") \n",
    "                            + format(z[5], \">10\") + format(z[6], \">10\") + format(z[7], \">10\")  + format(z[8], \">10\"))\n",
    "                            pdb_id_set.add(z[0])\n",
    "                            compuni_humanuni_PDBid.append((z[3], z[4], z[0][:4]))\n",
    "                        except:\n",
    "                            print(z)\n",
    "                    f.write(\"%s\\n\" % formated_item)\n",
    "\n",
    "\n",
    "    uniq_compuni_humanuni_PDBid_translation = set()\n",
    "    for n in compuni_humanuni_PDBid:\n",
    "        if n[0] == \"-\":\n",
    "            continue\n",
    "        uniq_compuni_humanuni_PDBid_translation.add(n)\n",
    "\n",
    "    with open('log_translator.txt', 'w') as filehandle:\n",
    "        for listitem in uniq_compuni_humanuni_PDBid_translation:\n",
    "            filehandle.write(listitem[0] + \" \" + listitem[1] + \" \" + listitem[2] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ProcessPool_run_renum(format_to_download=\"mmCIF\", input_mmCIF_files_were_found=(),\n",
    "#               default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "#               default_input_path_to_SIFTS=default_input_path_to_SIFTS, \n",
    "#               default_output_path_to_mmCIF=default_output_path_to_mmCIF, \n",
    "#               default_mmCIF_num=default_mmCIF_num, gzip_mode=gzip_mode):\n",
    "    \n",
    "#     resulting = list()\n",
    "#     executor = ProcessPoolExecutor()\n",
    "#     partial_master_mmCIF_renumber_function = partial(master_mmCIF_renumber_function,\n",
    "#                                                      default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "#                                                      default_input_path_to_SIFTS=default_input_path_to_SIFTS, \n",
    "#                                                      default_output_path_to_mmCIF=default_output_path_to_mmCIF, \n",
    "#                                                      default_mmCIF_num=default_mmCIF_num, gzip_mode=gzip_mode)\n",
    "    \n",
    "#     jobs = [executor.submit(partial_master_mmCIF_renumber_function, mmCIF_files) for mmCIF_files in input_mmCIF_files_were_found]\n",
    "#     for job in tqdm.tqdm(as_completed(jobs), total=len(jobs), position=0, leave=True, desc=\"Renumbering \"+format_to_download+\" files\"):\n",
    "#         resultus = job.result()\n",
    "#         if resultus is not None:\n",
    "#             resulting.append(resultus)\n",
    "    \n",
    "#     return resulting\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_mmCIF_files_were_found =look_what_is_inside(\"mmCIF_assembly\")\n",
    "# default_output_path_to_mmCIF = default_output_path_to_mmCIF_assembly\n",
    "# default_input_path_to_mmCIF = default_input_path_to_mmCIF_assembly\n",
    "\n",
    "# input_mmCIF_files_were_found =look_what_is_inside(\"mmCIF\")\n",
    "# default_input_path_to_mmCIF = current_directory + \"/mmCIF\"\n",
    "# default_output_path_to_mmCIF = current_directory + \"/output_mmCIF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for asymmetric_unit\n",
    "# input_mmCIF_files_were_found = look_what_is_inside('mmCIF', default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "# if not os.path.exists(default_output_path_to_mmCIF):\n",
    "#     os.makedirs(default_output_path_to_mmCIF)\n",
    "# if __name__ == '__main__':\n",
    "#     resulting = ProcessPool_run_renum(\"mmCIF\", input_mmCIF_files_were_found,\n",
    "#                           default_input_path_to_mmCIF,\n",
    "#                           default_input_path_to_SIFTS,\n",
    "#                           default_output_path_to_mmCIF,\n",
    "#                           default_mmCIF_num, gzip_mode)\n",
    "#for assembly\n",
    "# input_mmCIF_assembly_files_were_found = look_what_is_inside('mmCIF_assembly',default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "# if not os.path.exists(default_output_path_to_mmCIF):\n",
    "#     os.makedirs(default_output_path_to_mmCIF)\n",
    "    \n",
    "# if __name__ == '__main__':\n",
    "#     resulting2 = ProcessPool_run_renum(\"mmCIF_assembly\", input_mmCIF_assembly_files_were_found[207294:207295],\n",
    "#                           default_input_path_to_mmCIF_assembly,\n",
    "#                           default_input_path_to_SIFTS,\n",
    "#                           default_output_path_to_mmCIF_assembly,\n",
    "#                           default_mmCIF_num,gzip_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'5a51.cif.gz'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_assemblies(mmCIF_assembly, default_output_path_to_mmCIF_assembly):\n",
    "    output_mmCIF_assembly_files_were_found_list = list()\n",
    "    output_mmCIF_assembly_files_were_found_list.append(mmCIF_assembly)\n",
    "    for name in output_mmCIF_assembly_files_were_found_list:\n",
    "        not_gzip = 1\n",
    "        try:\n",
    "            list_of_lines_from_assembly_file = gzip.open(\n",
    "                Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), 'rt').readlines()\n",
    "        except OSError:\n",
    "            # maybe not archived\n",
    "            try:\n",
    "                list_of_lines_from_assembly_file = open(\n",
    "                    Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), 'rt').readlines()\n",
    "                not_gzip = 0\n",
    "            except Exception:\n",
    "                # broken archive\n",
    "                os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "                continue\n",
    "        except Exception:\n",
    "            # broken archive\n",
    "            os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            continue\n",
    "        \n",
    "        # check if file startswith \"_atom_site\" table at the beginning\n",
    "        try:\n",
    "            if \"_atom_site\" in list_of_lines_from_assembly_file[3] and \"loop_\" in list_of_lines_from_assembly_file[2]:\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "        except IndexError:\n",
    "            # empty file\n",
    "            os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            new_order_for_assembly_file = list()\n",
    "            listitem = (list_of_lines_from_assembly_file[:1]\n",
    "                        + list_of_lines_from_assembly_file[list_of_lines_from_assembly_file.index(\"#\\n\", 2):]\n",
    "                        + list_of_lines_from_assembly_file[2:list_of_lines_from_assembly_file.index(\"#\\n\", 2)]\n",
    "                        + [\"#\\n\"])\n",
    "\n",
    "            if not_gzip != 0:\n",
    "                with gzip.open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), \"wt\") as gzip_out:\n",
    "                    for listitem in new_order_for_assembly_file:\n",
    "                        gzip_out.write(listitem)\n",
    "            else:\n",
    "                with open(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name), \"wt\") as file_out:\n",
    "                    for listitem in new_order_for_assembly_file:\n",
    "                        file_out.write(listitem)\n",
    "\n",
    "        except ValueError:\n",
    "            # file isn't complete\n",
    "            os.remove(Path(str(default_output_path_to_mmCIF_assembly) + \"/\" + name))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "def ProcessPool_run_reform_assembly(default_output_path_to_mmCIF_assembly, current_directory):\n",
    "    output_mmCIF_assembly = look_what_is_inside('output_mmCIF_assembly', default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF_assembly)\n",
    "    assembly_list = list()\n",
    "    for assembly in output_mmCIF_assembly:\n",
    "        if \"assembly\" in assembly:\n",
    "            assembly_list.append(assembly)\n",
    "    output_mmCIF_assembly = assembly_list\n",
    "\n",
    "    os.chdir(default_output_path_to_mmCIF_assembly)\n",
    "    resulting = list()\n",
    "    executor = ProcessPoolExecutor()\n",
    "    partial_reform_assembly = partial(check_assemblies, default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF_assembly)\n",
    "\n",
    "    jobs = [executor.submit(partial_reform_assembly, assembly_files) for assembly_files in output_mmCIF_assembly]\n",
    "    for job in tqdm.tqdm(as_completed(jobs), total=len(jobs), position=0, leave=True, desc=\"Checking assembly files\"):\n",
    "        resultus = job.result()\n",
    "        resulting.append(resultus)\n",
    "\n",
    "    os.chdir(current_directory)\n",
    "    return resulting\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking assembly files: 100%|██████████| 263532/263532 [35:13<00:00, 124.71it/s] \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    resulting3 = ProcessPool_run_reform_assembly(default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF_assembly,\n",
    "                                                 current_directory=current_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_mmCIF_assembly = look_what_is_inside('output_mmCIF_assembly', default_output_path_to_mmCIF_assembly=default_output_path_to_mmCIF_assembly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get partial_occupancy or ins_code\n",
    "# input_mmCIF_files_were_found =look_what_is_inside(\"mmCIF\")\n",
    "# default_input_path_to_mmCIF = current_directory + \"/mmCIF\"\n",
    "# default_output_path_to_mmCIF = current_directory + \"/output_mmCIF\"\n",
    "\n",
    "# ### partial_occupancy or ins_code catcher\n",
    "# def master_mmCIF_renumber_function(input_mmCIF_files_were_found, default_input_path_to_mmCIF,\n",
    "#                                    default_input_path_to_SIFTS, default_output_path_to_mmCIF,\n",
    "#                                    default_mmCIF_num, gzip_mode):\n",
    "\n",
    "#     input_mmCIF_assembly_files_were_found_list = list()\n",
    "#     input_mmCIF_assembly_files_were_found_list.append(input_mmCIF_files_were_found)\n",
    "\n",
    "#     for mmCIF_name in input_mmCIF_assembly_files_were_found_list:\n",
    "        \n",
    "#         mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "#         if mmcif_dict == 0:\n",
    "#             continue\n",
    "#         ### ins_code_catcher\n",
    "# #         non_dot_ins_code = 0\n",
    "# #         try:\n",
    "# #             for n in mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_ins_code\"]:\n",
    "# #                 if \".\" != n:\n",
    "# #                     non_dot_ins_code = (\"pdb_ins_code:\", mmCIF_name)\n",
    "# #                     return (non_dot_ins_code)\n",
    "# #         except KeyError:\n",
    "# #             pass\n",
    "        \n",
    "#         ### partial_occupancy_catcher\n",
    "#         non_whole_occupancy = 0\n",
    "#         try:\n",
    "#             for n in mmcif_dict[\"_atom_site.occupancy\"]:\n",
    "#                 if n[0] != \"1\":\n",
    "#                     non_whole_occupancy = (\"partial_occupancy:\", mmCIF_name)\n",
    "#                     return (non_whole_occupancy)\n",
    "#         except KeyError:\n",
    "#             pass\n",
    "\n",
    "# def ProcessPool_run_renum(format_to_download=\"mmCIF\", input_mmCIF_files_were_found=(),\n",
    "#               default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "#               default_input_path_to_SIFTS=default_input_path_to_SIFTS, \n",
    "#               default_output_path_to_mmCIF=default_output_path_to_mmCIF, \n",
    "#               default_mmCIF_num=default_mmCIF_num, gzip_mode=gzip_mode):\n",
    "    \n",
    "#     resulting = list()\n",
    "#     executor = ProcessPoolExecutor()\n",
    "#     partial_master_mmCIF_renumber_function = partial(master_mmCIF_renumber_function,\n",
    "#                                                      default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "#                                                      default_input_path_to_SIFTS=default_input_path_to_SIFTS, \n",
    "#                                                      default_output_path_to_mmCIF=default_output_path_to_mmCIF, \n",
    "#                                                      default_mmCIF_num=default_mmCIF_num, gzip_mode=gzip_mode)\n",
    "    \n",
    "#     jobs = [executor.submit(partial_master_mmCIF_renumber_function, mmCIF_files) for mmCIF_files in input_mmCIF_files_were_found]\n",
    "#     for job in tqdm.tqdm(as_completed(jobs), total=len(jobs), position=0, leave=True, desc=\"Renumbering \"+format_to_download+\" files\"):\n",
    "#         resultus = job.result()\n",
    "#         if resultus != None:\n",
    "#             resulting.append(resultus)\n",
    "    \n",
    "#     return resulting\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     resulting = ProcessPool_run_renum(\"mmCIF\", input_mmCIF_files_were_found,\n",
    "#                           default_input_path_to_mmCIF,\n",
    "#                           default_input_path_to_SIFTS,\n",
    "#                           default_output_path_to_mmCIF,\n",
    "#                           default_mmCIF_num, gzip_mode)\n",
    "    \n",
    "# with open('partial_occupancy' + '.txt', 'w') as filehandle:\n",
    "#     for banch in resulting:\n",
    "#         filehandle.write('%s\\n' % list(banch))\n",
    "        \n",
    "# with open('pdb_ins_code' + '.txt', 'w') as filehandle:\n",
    "#     for banch in resulting:\n",
    "#         filehandle.write('%s\\n' % list(banch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation\n",
    "\n",
    "# add_set_of_annotation = ['Isoform beta',\n",
    "#  'Cloning site residue',\n",
    "#  'Gst tag',\n",
    "#  'Propionation',\n",
    "#  'Initial methionine',\n",
    "#  'See remark9 99',\n",
    "#  'Linker',\n",
    "#  'Amidation',\n",
    "#  'Oxidized cys',\n",
    "#  'Missing from gb',\n",
    "#  'Polymorphism',\n",
    "#  'Chromophor; rem 999',\n",
    "#  'Strain difference',\n",
    "#  'Intrachain his tag',\n",
    "#  'Mod. residue/cloning artifact',\n",
    "#  'Differences in map',\n",
    "#  'E',\n",
    "#  'Initiating residue',\n",
    "#  'Myc tag',\n",
    "#  'Modified tyr',\n",
    "#  'Variation',\n",
    "#  'Possible isoform',\n",
    "#  'Substitution',\n",
    "#  'Random mutagenesis',\n",
    "#  'See remrak 999',\n",
    "#  'Lys tag',\n",
    "#  'Gap in sws p00734',\n",
    "#  'Chromophore',\n",
    "#  'Expression tag',\n",
    "#  'N-acetylation',\n",
    "#  'Initiating mse',\n",
    "#  'Modified',\n",
    "#  'helix',\n",
    "#  'Chromophore; rem 999',\n",
    "#  'Methylated asn',\n",
    "#  'Variant see remark 999',\n",
    "#  'Chemical modification',\n",
    "#  'Somatic variant',\n",
    "#  'Benzoylation',\n",
    "#  'Autophosphorylation',\n",
    "#  'Deletion',\n",
    "#  'See remerk 999',\n",
    "#  'Allele',\n",
    "#  'Myristoylated',\n",
    "#  'Modified chromophore',\n",
    "#  'Conflict',\n",
    "#  'See remark 999; engineered',\n",
    "#  'See sequence details',\n",
    "#  'Gpgs tag',\n",
    "#  'Insertion',\n",
    "#  'Insertion; see remark 999',\n",
    "#  '?',\n",
    "#  'Sequencing error',\n",
    "#  'strand',\n",
    "#  'Missing in sws',\n",
    "#  'Initiating met',\n",
    "#  'Variant',\n",
    "#  'Thrombin cleavage site',\n",
    "#  'Protease cleavage site',\n",
    "#  'Frameshift error',\n",
    "#  'Methylation',\n",
    "#  'Microheterogeneity; see remark 999',\n",
    "#  'Natural variant',\n",
    "#  'See remark 999',\n",
    "#  'Isoform',\n",
    "#  'Formylation',\n",
    "#  'Polymorphic variant',\n",
    "#  'Gcn4 tag',\n",
    "#  'Detection tag',\n",
    "#  'Variant strain',\n",
    "#  'Initiator n-formyl-met',\n",
    "#  'Modified amino acid',\n",
    "#  'Allelic variant',\n",
    "#  'Modification',\n",
    "#  'Modified residues',\n",
    "#  'Myc epitope',\n",
    "#  'loop',\n",
    "#  'Kt3 tag',\n",
    "#  'Cleavage site',\n",
    "#  'See remark 400',\n",
    "#  'Chromophore; see remark 999',\n",
    "#  'Correction',\n",
    "#  'Initiator methionine',\n",
    "#  'Sequence correction',\n",
    "#  'Polymorphic site',\n",
    "#  'T',\n",
    "#  'See sequence_details',\n",
    "#  'Modified residue',\n",
    "#  'Engineered mutation',\n",
    "#  'Hydroxylation',\n",
    "#  'Remark 999',\n",
    "#  'H',\n",
    "#  'Microheterogeneity',\n",
    "#  'Acetylation',\n",
    "#  'Initiating methionine',\n",
    "#  'New isozyme',\n",
    "#  'See reamrk 999',\n",
    "#  'Cloning artifact',\n",
    "#  'Prescission site',\n",
    "#  'Strain',\n",
    "#  'D-configuration',\n",
    "#  'Myristoylation',\n",
    "#  'Modified initiating methionine',\n",
    "#  'Signal peptide',\n",
    "#  'Phosphorylation']\n",
    "\n",
    "# None_type_list = list()\n",
    "# for annotation in add_set_of_annotation:\n",
    "#     try:\n",
    "#         with open(annotation + '.txt', 'w') as filehandle:\n",
    "#             for banch in resulting:\n",
    "#                 try:\n",
    "#                     for li in banch:\n",
    "#                         try:\n",
    "#                             if li[1][0] == \"Annotation:\" and li[1][1] == annotation:\n",
    "#                                 filehandle.write('%s\\n' % list(li))\n",
    "#                         except IndexError:\n",
    "#                             pass\n",
    "#                 except TypeError:\n",
    "#                     None_type_list.append(banch)\n",
    "                    \n",
    "#     except FileNotFoundError:\n",
    "#         pass\n",
    "    \n",
    "# with open(\"all_annotation\" + '.txt', 'w') as filehandle:\n",
    "#     for banch in resulting:\n",
    "#         try:\n",
    "#             for li in banch:\n",
    "#                 try:\n",
    "#                     if li[1][0] == \"Annotation:\":\n",
    "#                         filehandle.write('%s\\n' % list(li))\n",
    "#                 except IndexError:\n",
    "#                     pass\n",
    "#         except TypeError:\n",
    "#             pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A) no sifts file available (we don't renumber):  3659\n",
      "B) sifts file available but no Uniprot information (we don't renumber):  5919\n",
      "C) sifts available with UniProt information (we try to renumber):  166929\n",
      "\n",
      "\n",
      "These are mutually exclusive sets. So A+B+C = N, the number of entries in PDB. N total:  176507\n",
      "We can divide C into:\n",
      "\n",
      "\n",
      "D) no changes in numbering at all: 75359\n",
      "E) NumChanges>0 but only changes that are 50000+SeqResNum:  23448\n",
      "F) NumChanges>0 but only changes due to UniProt:  49422\n",
      "G) NumChanges>0 with changes due to UniProt and 50000+SeqResNum:  18700\n"
     ]
    }
   ],
   "source": [
    "### for the analysis of the Numbers_on_Numbers\n",
    "# sums_50k = 0\n",
    "# sums_renum = 0\n",
    "# new_sums_list = list()\n",
    "# for n in resulting2:\n",
    "#     for z in n:\n",
    "#         try:\n",
    "#             sums_50k += int(z[-1])\n",
    "#             sums_renum += int(z[-2])\n",
    "#             PDBid = z[0]\n",
    "#         except TypeError:\n",
    "#             new_sums_list.append([PDBid, sums_renum, sums_50k])\n",
    "#             sums_50k = 0\n",
    "#             sums_renum = 0\n",
    "\n",
    "# both = set()\n",
    "# Uni = set()\n",
    "# _50k = set()\n",
    "# nothing = set()\n",
    "\n",
    "# for n in new_sums_list:\n",
    "#     if n[1] == 0 and n[2] == 0:\n",
    "#         nothing.add(n[0])\n",
    "#     if n[1] != 0 and n[2] == 0:\n",
    "#         Uni.add(n[0])\n",
    "#     if n[1] == 0 and n[2] != 0:\n",
    "#         _50k.add(n[0])\n",
    "#     if n[1] != 0 and n[2] != 0:\n",
    "#         both.add(n[0])\n",
    "\n",
    "\n",
    "# D = (len(nothing))\n",
    "# E = (len(_50k))\n",
    "# F = (len(Uni))\n",
    "# G = (len(both))\n",
    "\n",
    "# C = D + E + F + G\n",
    "# A = len(resulting2) - len(look_what_is_inside(\"SIFTS\"))\n",
    "# B = len(resulting2) - C - A\n",
    "\n",
    "# print(\"A) no sifts file available (we don't renumber): \", A)\n",
    "# print(\"B) sifts file available but no Uniprot information (we don't renumber): \", B)\n",
    "# print(\"C) sifts available with UniProt information (we try to renumber): \", C)\n",
    "\n",
    "# print(\"\\n\")\n",
    "# print(\"These are mutually exclusive sets. So A+B+C = N, the number of entries in PDB. N total: \", len(resulting2))\n",
    "# print(\"We can divide C into:\")\n",
    "# print(\"\\n\")\n",
    "\n",
    "# print(\"D) no changes in numbering at all:\", D)\n",
    "# print(\"E) NumChanges>0 but only changes that are 50000+SeqResNum: \", E)\n",
    "# print(\"F) NumChanges>0 but only changes due to UniProt: \", F)\n",
    "# print(\"G) NumChanges>0 with changes due to UniProt and 50000+SeqResNum: \", G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_PDBe_PDB_UniProt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-a8e074550adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_PDBe_PDB_UniProt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_PDBe_PDB_UniProt' is not defined"
     ]
    }
   ],
   "source": [
    "df_PDBe_PDB_UniProt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### cases when pdb_seq_num diff from auth_seq_num\n",
    "# cases_of_diff_in_poly_seq_scheme = set()\n",
    "# cases_of_diff_in_nonpoly_scheme = set()\n",
    "# for input_file in tqdm.tqdm(input_mmCIF_files_were_found, total=len(input_mmCIF_files_were_found), position=0, leave=True, desc=\"Checking files\"):\n",
    "\n",
    "#     input_mmCIF_assembly_files_were_found_list = list()\n",
    "#     input_mmCIF_assembly_files_were_found_list.append(input_file)\n",
    "    \n",
    "#     for mmCIF_name in input_mmCIF_assembly_files_were_found_list:\n",
    "#         mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "#         if mmcif_dict == 0:\n",
    "#             continue\n",
    "#     try:       \n",
    "#         for n in range(len(mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"])-1):\n",
    "#             if mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"][n] != mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"][n] and mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"][n] != \"?\":\n",
    "#                 return mmCIF_name\n",
    "#                 # cases_of_diff_in_poly_seq_scheme.add(mmCIF_name)\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "            \n",
    "#     try:\n",
    "#         for n in range(len(mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_seq_num\"])-1):\n",
    "#             if mmcif_dict[\"_pdbx_nonpoly_scheme.pdb_seq_num\"][n] != mmcif_dict[\"_pdbx_nonpoly_scheme.auth_seq_num\"][n] and mmcif_dict[\"_pdbx_nonpoly_scheme.auth_seq_num\"][n] != \"?\":\n",
    "#                 cases_of_diff_in_nonpoly_scheme.add(mmCIF_name)\n",
    "#     except KeyError:\n",
    "#         pass\n",
    "\n",
    "# def master_mmCIF_renumber_function(input_mmCIF_files_were_found, default_input_path_to_mmCIF,\n",
    "#                                    default_input_path_to_SIFTS, default_output_path_to_mmCIF,\n",
    "#                                    default_mmCIF_num, gzip_mode):\n",
    "\n",
    "#     input_mmCIF_assembly_files_were_found_list = list()\n",
    "#     input_mmCIF_assembly_files_were_found_list.append(input_mmCIF_files_were_found)\n",
    "    \n",
    "#     for mmCIF_name in input_mmCIF_assembly_files_were_found_list:\n",
    "#         mmcif_dict = try_MMCIF2Dict(default_input_path_to_mmCIF, mmCIF_name)\n",
    "#         if mmcif_dict == 0:\n",
    "#             continue\n",
    "#     try:\n",
    "#         mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"]\n",
    "# #         for n in range(len(mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"])-1):\n",
    "# #             if mmcif_dict[\"_pdbx_poly_seq_scheme.pdb_seq_num\"][n] != mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"][n] and mmcif_dict[\"_pdbx_poly_seq_scheme.auth_seq_num\"][n] != \"?\":\n",
    "# #                 pass\n",
    "#                 # cases_of_diff_in_poly_seq_scheme.add(mmCIF_name)\n",
    "#     except KeyError:\n",
    "#         return mmCIF_name\n",
    "\n",
    "\n",
    "# # # nonpoly_diff = resulting\n",
    "# with open('without_poly_scheme' + '.txt', 'w') as filehandle:\n",
    "#     for banch in resulting:\n",
    "#         filehandle.write('%s\\n' % banch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################PDB format renum#####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.download.modules import *\n",
    "from src.renum.shared.handling_chain_numbering_clashes import handling_chain_numbering_clashes\n",
    "from src.renum.shared.SIFTS_tree_parser import SIFTS_tree_parser\n",
    "from src.renum.shared.renumbered_count_in_chains import renumbered_count_in_chains\n",
    "from src.download.downloadwithThreadPool import download_with_pool, url_formation_for_pool\n",
    "PDBrenum_REMARK = [\n",
    "    \"REMARK   0  File processed by PDBrenum: http://dunbrack3.fccc.edu/PDBrenum      \",\n",
    "    \"REMARK   0  Author sequence numbering is replaced with UniProt numbering        \",\n",
    "    \"REMARK   0  according to alignment by SIFTS                                     \",\n",
    "    \"REMARK   0  (https://www.ebi.ac.uk/pdbe/docs/sifts/).                           \",                 \n",
    "    \"REMARK   0  Only chains with UniProt sequences in SIFTS are renumbered.         \",\n",
    "    \"REMARK   0  Residues in UniProt chains without UniProt residue numbers in SIFTS \",\n",
    "    \"REMARK   0  (e.g., sequence tags) are given residue numbers 5000+label_seq_id   \",\n",
    "    \"REMARK   0  (where label_seq_id is the 1-to-N residue numbering of each chain.  \",\n",
    "    \"REMARK   0  Ligands are numbered 5000+their residue number in the original      \",\n",
    "    \"REMARK   0  file. The _poly_seq_scheme table contains a correspondence between  \",\n",
    "    \"REMARK   0  the 1-to-N sequence (seq_id), the new numbering based on UniProt    \",\n",
    "    \"REMARK   0  (pdb_seq_num = auth_seq_id in the _atom_site records), and          \",\n",
    "    \"REMARK   0  the author numbering in the original mmCIF file from the PDB        \",\n",
    "    \"REMARK   0  (auth_seq_num).                                                     \"]\n",
    "\n",
    "def SIFTS_data_parser_for_PDB(tuple_PDBe_for_PDB_and_tuple_PDB, tuple_PDBe_for_UniProt_and_tuple_UniProt,\n",
    "                              default_PDB_num, chains_to_change=\"all\"):\n",
    "    df_PDBe_UniProt = pd.DataFrame(tuple_PDBe_for_UniProt_and_tuple_UniProt, columns=['PDBe', 'UniProt', \"AccessionID\"])\n",
    "    df_PDBe_UniProt = df_PDBe_UniProt.drop_duplicates(subset=\"PDBe\", keep='first')\n",
    "    df_PDBe_PDB = pd.DataFrame(tuple_PDBe_for_PDB_and_tuple_PDB, columns=['PDBe', 'PDB'])\n",
    "    df_PDBe_PDB = df_PDBe_PDB.drop_duplicates(subset=\"PDBe\", keep='first')\n",
    "\n",
    "    df_PDBe_PDB_UniProt = df_PDBe_PDB.merge(df_PDBe_UniProt, left_on=\"PDBe\", right_on=\"PDBe\", how='left')\n",
    "    df_PDBe_PDB_UniProt['UniProt'] = df_PDBe_PDB_UniProt['UniProt'].replace(np.nan, \"5000\")\n",
    "    df_PDBe_PDB_UniProt[\"Uni_moD\"] = np.where(df_PDBe_PDB_UniProt['UniProt'] != \"5000\", df_PDBe_PDB_UniProt['UniProt'], df_PDBe_PDB_UniProt[\"PDBe\"])\n",
    "    df_PDBe_PDB_UniProt.loc[:, 'new_col_Uni'] = df_PDBe_PDB_UniProt.Uni_moD.map(lambda x: x[0])\n",
    "    df_PDBe_PDB_UniProt[\"UniProt_5k\"] = df_PDBe_PDB_UniProt.new_col_Uni.apply(lambda x: (int(x) + default_PDB_num if type(x) == str else x))\n",
    "    df_PDBe_PDB_UniProt.loc[df_PDBe_PDB_UniProt['UniProt'] != '5000', 'UniProt_5k'] = df_PDBe_PDB_UniProt['new_col_Uni']\n",
    "\n",
    "    Three_Rows_CIF_Num_Uni = []\n",
    "    if chains_to_change == \"all\":\n",
    "        for index, rows in df_PDBe_PDB_UniProt.iterrows():\n",
    "            intermediate_list = [rows.PDBe, rows.UniProt_5k, rows.Uni_moD, rows.PDB, rows.AccessionID]\n",
    "            Three_Rows_CIF_Num_Uni.append(intermediate_list)\n",
    "\n",
    "    else:\n",
    "        for index, rows in df_PDBe_PDB_UniProt.iterrows():\n",
    "            if rows.PDB[2].strip() in chains_to_change:\n",
    "                intermediate_list = [rows.PDBe, rows.UniProt_5k, rows.Uni_moD, rows.PDB, rows.AccessionID]\n",
    "            else:\n",
    "                intermediate_list = [rows.PDBe, rows.PDB[0], rows.Uni_moD, rows.PDB, rows.AccessionID]\n",
    "            Three_Rows_CIF_Num_Uni.append(intermediate_list)\n",
    "\n",
    "    df_PDBe_PDB_UniProt[\"Three_Rows_CIF_Num_Uni\"] = Three_Rows_CIF_Num_Uni\n",
    "    df_PDBe_PDB_UniProt_without_null = df_PDBe_PDB_UniProt[df_PDBe_PDB_UniProt.PDB.map(lambda x: x[0]) != \"null\"]\n",
    "    df_PDBe_PDB_UniProt_without_null_index_PDBe = df_PDBe_PDB_UniProt_without_null.set_index(\"PDBe\")\n",
    "\n",
    "    return [df_PDBe_PDB_UniProt_without_null_index_PDBe, df_PDBe_PDB_UniProt]\n",
    "\n",
    "\n",
    "def try_SIFTS_tree_parser(default_input_path_to_SIFTS, SIFTS_name):\n",
    "    product_tree_SIFTS = 0\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            product_tree_SIFTS = SIFTS_tree_parser(\n",
    "                gzip.open(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name), 'rt'))\n",
    "            \n",
    "            break\n",
    "        except EOFError:\n",
    "            os.remove(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name))\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name], default_input_path_to_SIFTS=default_input_path_to_SIFTS)[0],\n",
    "                               default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        except ValueError:\n",
    "            os.remove(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name))\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name], default_input_path_to_SIFTS=default_input_path_to_SIFTS)[0],\n",
    "                               default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        except OSError:\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name], default_input_path_to_SIFTS=default_input_path_to_SIFTS)[0],\n",
    "                               default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        except:\n",
    "            download_with_pool(url_formation_for_pool(\"SIFTS\", [SIFTS_name], default_input_path_to_SIFTS=default_input_path_to_SIFTS)[0],\n",
    "                               default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    return product_tree_SIFTS\n",
    "\n",
    "\n",
    "def try_PDB(default_input_path_to_PDB, PDB):\n",
    "    split = 0\n",
    "\n",
    "    for _ in range(3):\n",
    "        try:\n",
    "            split = gzip.open(Path(str(default_input_path_to_PDB) + \"/\" + PDB), 'rt').read().splitlines()\n",
    "            break\n",
    "        except EOFError:\n",
    "            try:\n",
    "                re.search('\\.pdb(.*).gz', PDB).group(1)\n",
    "                os.remove(Path(str(default_input_path_to_PDB) + \"/\" + PDB))\n",
    "                download_with_pool(url_formation_for_pool(\"PDB_assembly\", [PDB], default_input_path_to_PDB_assembly=default_input_path_to_PDB)[0],\n",
    "                                   default_input_path_to_PDB_assembly=default_input_path_to_PDB)\n",
    "            except AttributeError:\n",
    "                os.remove(Path(str(default_input_path_to_PDB) + \"/\" + PDB))\n",
    "                download_with_pool(url_formation_for_pool(\"PDB\", [PDB], default_input_path_to_PDB=default_input_path_to_PDB)[0],\n",
    "                                   default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "\n",
    "        except ValueError:\n",
    "            try:\n",
    "                re.search('\\.pdb(.*).gz', PDB).group(1)\n",
    "                os.remove(Path(str(default_input_path_to_PDB) + \"/\" + PDB))\n",
    "                download_with_pool(url_formation_for_pool(\"PDB_assembly\", [PDB], default_input_path_to_PDB_assembly=default_input_path_to_PDB)[0],\n",
    "                                   default_input_path_to_PDB_assembly=default_input_path_to_PDB)\n",
    "            except AttributeError:\n",
    "                os.remove(Path(str(default_input_path_to_PDB) + \"/\" + PDB))\n",
    "                download_with_pool(url_formation_for_pool(\"PDB\", [PDB], default_input_path_to_PDB=default_input_path_to_PDB)[0],\n",
    "                                   default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        except OSError:\n",
    "            try:\n",
    "                re.search('\\.pdb(.*).gz', PDB).group(1)\n",
    "                download_with_pool(url_formation_for_pool(\"PDB_assembly\", [PDB], default_input_path_to_PDB_assembly=default_input_path_to_PDB)[0],\n",
    "                                   default_input_path_to_PDB_assembly=default_input_path_to_PDB)\n",
    "            except AttributeError:\n",
    "                download_with_pool(url_formation_for_pool(\"PDB\", [PDB], default_input_path_to_PDB=default_input_path_to_PDB)[0],\n",
    "                                   default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "    return split\n",
    "\n",
    "\n",
    "def if_no_SIFTS_data_log_for_PDB(default_input_path_to_PDB, PDB_id, PDB):\n",
    "    split = try_PDB(default_input_path_to_PDB, PDB)\n",
    "    res_number_name_chainID_from_PDB_tuple = list()\n",
    "    chains_set = set()\n",
    "    log_message = list()\n",
    "\n",
    "    for n in split:\n",
    "        if n.startswith(\"ATOM\") or n.startswith(\"TER\") or n.startswith(\"ANISOU\") or n.startswith(\"ANISOU\") or n.startswith(\"SIGUIJ\"):\n",
    "            res_number_name_chainID_from_PDB_tuple.append((n[22:27].strip(\" \"), n[17:20], n[21]))\n",
    "            chains_set.add(n[21])\n",
    "\n",
    "    if len(res_number_name_chainID_from_PDB_tuple) == 0:\n",
    "        log_message.append([PDB_id, \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\", \"-\"])\n",
    "        return log_message\n",
    "\n",
    "    df_mmCIF = pd.DataFrame(list(zip(res_number_name_chainID_from_PDB_tuple, res_number_name_chainID_from_PDB_tuple)))\n",
    "    df_mmCIF = df_mmCIF.rename(columns={0: \"PDB_old\", 1: \"PDB_old_copy\"})\n",
    "    df_mmCIF = df_mmCIF.set_index(\"PDB_old\")\n",
    "    df_mmCIF = df_mmCIF.drop_duplicates()\n",
    "\n",
    "    for chain in chains_set:\n",
    "        count_res_in_chain = 0\n",
    "        for resnum_resname_chain in df_mmCIF.PDB_old_copy:\n",
    "            if chain == resnum_resname_chain[2]:\n",
    "                count_res_in_chain += 1\n",
    "        log_message.append([PDB_id, \"-\", chain, \"-\", \"-\", len(df_mmCIF), \"-\", count_res_in_chain, \"0\", \"0\"])\n",
    "    return log_message\n",
    "\n",
    "\n",
    "def copy_file(inpath, file_name, outpath, postfix, gzip_mode):\n",
    "    if file_name.endswith(\".ent.gz\") and file_name.startswith(\"pdb\"):\n",
    "        PDB_id = file_name[3:file_name.rfind(\".ent.gz\")]\n",
    "    else:\n",
    "        PDB_id = file_name[:4]\n",
    "\n",
    "    absolute_path_in = inpath + \"/\" + file_name\n",
    "    absolute_path_out = outpath + \"/\" + PDB_id + postfix\n",
    "    if gzip_mode == \"off\":\n",
    "        with gzip.open(absolute_path_in, 'rb') as f_in:\n",
    "            with open(absolute_path_out, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "    else:\n",
    "        shutil.copyfile(absolute_path_in, absolute_path_out + \".gz\")\n",
    "\n",
    "\n",
    "def PDB_parser(split, df_PDBe_PDB_UniProt_without_null_index_PDBe, default_PDB_num):\n",
    "    res_number_name_chainID_from_PDB_tuple = list()\n",
    "    num_ins_code_name_chain_HETATM = list()\n",
    "    missing_res_remark_465 = list()\n",
    "    skipper_for_remark_465 = True\n",
    "    Num_is_Too_Big = True\n",
    "\n",
    "    for n in split:\n",
    "        if n.startswith(\"ATOM\") or n.startswith(\"TER\") or n.startswith(\"ANISOU\") or n.startswith(\"ANISOU\") or n.startswith(\"SIGUIJ\"):\n",
    "            res_number_name_chainID_from_PDB_tuple.append((n[22:27].strip(\" \"), n[17:20], n[21]))\n",
    "        if n.startswith(\"HETATM\"):\n",
    "            num_ins_code_name_chain_HETATM.append((n[22:27].strip(\" \"), n[17:20], n[21]))\n",
    "\n",
    "        if n.startswith(\"REMARK 465\"):\n",
    "            if not skipper_for_remark_465:\n",
    "                missing_res_remark_465.append((n[20:27].strip(\" \"), n[15:18], n[19]))\n",
    "            if n[15:27] == \"RES C SSSEQI\":\n",
    "                skipper_for_remark_465 = False\n",
    "\n",
    "    df_mmCIF = pd.DataFrame(list(zip(res_number_name_chainID_from_PDB_tuple, res_number_name_chainID_from_PDB_tuple)))\n",
    "    df_mmCIF = df_mmCIF.rename(columns={0: \"PDB_old\", 1: \"PDB_old_copy\"})\n",
    "    df_mmCIF = df_mmCIF.set_index(\"PDB_old\")\n",
    "    df_mmCIF = df_mmCIF.drop_duplicates()\n",
    "\n",
    "    df_final = df_mmCIF.merge(df_PDBe_PDB_UniProt_without_null_index_PDBe, left_on=\"PDB_old_copy\", right_on=\"PDB\", how='left')\n",
    "    df_final['Uni_moD'] = df_final['Uni_moD'].replace(np.nan, \"5000\")\n",
    "    df_final[\"Uni_moD\"] = np.where(df_final['Uni_moD'] != \"5000\", df_final['Uni_moD'], df_final[\"PDB_old_copy\"])\n",
    "    df_final.loc[:, 'new_col_Uni'] = df_final.Uni_moD.map(lambda x: x[0])\n",
    "    df_final[\"UniProt_5k\"] = df_final.new_col_Uni.apply(lambda x: (int(x) + default_PDB_num if x.isdigit() else x))\n",
    "    df_final.loc[df_final['UniProt'] != '5000', 'UniProt_5k'] = df_final['new_col_Uni']\n",
    "\n",
    "    Three_Rows_CIF_Num_Uni = list()\n",
    "    for index, rows in df_final.iterrows():\n",
    "        intermediate_list = [rows.PDB_old_copy, rows.UniProt_5k, rows.Uni_moD]\n",
    "        if type(rows.UniProt_5k) == int:\n",
    "            if len(str(rows.UniProt_5k)) > 4:\n",
    "                Num_is_Too_Big = False\n",
    "        else:\n",
    "            if len(rows.UniProt_5k) > 4:\n",
    "                Num_is_Too_Big = False\n",
    "        Three_Rows_CIF_Num_Uni.append(intermediate_list)\n",
    "\n",
    "    df_final[\"Three_Rows_CIF_Num_Uni\"] = Three_Rows_CIF_Num_Uni\n",
    "    df_final_index_PDBe = df_final.set_index(\"PDB_old_copy\")\n",
    "    df_final_index_PDBe_drop_NAN = df_final_index_PDBe.dropna(subset=['PDB'])\n",
    "    pd_series_index_PDBe = df_final_index_PDBe[\"Three_Rows_CIF_Num_Uni\"]\n",
    "\n",
    "    PDB_str = df_final_index_PDBe_drop_NAN.PDB.map(\n",
    "        lambda x: x[1] + \"{:>2}\".format(x[2]) + \"{:>4}\".format(x[0]) + \" \" if x[0].isdigit() else x[1] + \"{:>2}\".format(x[2]) + \"{:>5}\".format(x[0]))\n",
    "    df_final_index_PDBe_drop_NAN = df_final_index_PDBe_drop_NAN.merge(PDB_str.rename('PDB_str'), left_index=True, right_index=True)\n",
    "    df_final_poly_corrected = df_final_index_PDBe_drop_NAN.drop(columns=['UniProt', 'AccessionID', \"new_col_Uni\", \"UniProt_5k\", \"Uni_moD\"])\n",
    "    renum_str = df_final_poly_corrected.PDB.map(lambda x: x[1] + \"{:>2}\".format(x[2])) + df_final_poly_corrected.Three_Rows_CIF_Num_Uni.map(\n",
    "        lambda x: \"{:>4}\".format(str(int(x[1])))) + \" \"\n",
    "    df_final_poly_corrected = df_final_poly_corrected.merge(renum_str.rename('renum_str'), left_index=True, right_index=True)\n",
    "    df_final_poly_corrected = df_final_poly_corrected.reset_index(drop=True)\n",
    "\n",
    "    return [pd_series_index_PDBe, num_ins_code_name_chain_HETATM, df_final_poly_corrected, missing_res_remark_465, Num_is_Too_Big]\n",
    "\n",
    "\n",
    "def non_poly_num(pd_series_index_PDBe, num_ins_code_name_chain_HETATM):\n",
    "    working_range_list = list()\n",
    "    for n in range(1, 10000):\n",
    "        working_range_list.append(n)\n",
    "\n",
    "    chain_and_number = list()\n",
    "    for n in pd_series_index_PDBe:\n",
    "        chain_and_number.append((n[0][2], n[1]))\n",
    "\n",
    "    chain_label = chain_and_number[0][0]\n",
    "    numbers_per_chain = list()\n",
    "    chain_label_with_numbers_per_chain = list()\n",
    "    for n in chain_and_number:\n",
    "        if chain_label == n[0]:\n",
    "            d = str(n[1])\n",
    "            n_numeric = ''.join(d for d in d if d.isdigit())\n",
    "            numbers_per_chain.append(int(n_numeric))\n",
    "        else:\n",
    "            numbers_per_chain = list(np.unique(numbers_per_chain))\n",
    "            chain_label_with_numbers_per_chain.append((chain_label, numbers_per_chain))\n",
    "            numbers_per_chain = list()\n",
    "            chain_label = n[0]\n",
    "            if chain_label == n[0]:\n",
    "                d = str(n[1])\n",
    "                n_numeric = ''.join(d for d in d if d.isdigit())\n",
    "                numbers_per_chain.append(int(n_numeric))\n",
    "\n",
    "    numbers_per_chain = list(np.unique(numbers_per_chain))\n",
    "    chain_label_with_numbers_per_chain.append((chain_label, numbers_per_chain))\n",
    "\n",
    "    available_numbers_for_chains = list()\n",
    "    for n in chain_label_with_numbers_per_chain:\n",
    "        for num in n[1]:\n",
    "            if num in working_range_list:\n",
    "                working_range_list.remove(num)\n",
    "\n",
    "        available_numbers_for_chains.append((n[0], working_range_list))\n",
    "        working_range_list = list()\n",
    "        for n_ in range(1, 10000):\n",
    "            working_range_list.append(n_)\n",
    "\n",
    "    chain_and_num_available = list()\n",
    "    for n in available_numbers_for_chains:\n",
    "        for num in n[1]:\n",
    "            chain_and_num_available.append((n[0], num))\n",
    "\n",
    "    numbers_from_num_ins_code_name_chain_HETATM = list()\n",
    "    chain_from_num_ins_code_name_chain_HETATM = list()\n",
    "    for n in num_ins_code_name_chain_HETATM:\n",
    "        numbers_from_num_ins_code_name_chain_HETATM.append(n[0])\n",
    "        chain_from_num_ins_code_name_chain_HETATM.append(n[2])\n",
    "\n",
    "    df_nonpoly = pd.DataFrame(\n",
    "        list(zip(num_ins_code_name_chain_HETATM, chain_from_num_ins_code_name_chain_HETATM, numbers_from_num_ins_code_name_chain_HETATM)), columns=[\n",
    "            'PDB', \"PDB_chain\", \"numbers\"])\n",
    "    df_nonpoly_dropped_dup = df_nonpoly.drop_duplicates(subset=\"PDB\", keep='first')\n",
    "    df_nonpoly_dropped_dup_sorted = df_nonpoly_dropped_dup.sort_values([\"PDB_chain\", \"numbers\"], ascending=(True, True)).reset_index(drop=True)\n",
    "    small_ref_table = df_nonpoly_dropped_dup_sorted.set_index([\"PDB_chain\", \"PDB\"]).count(level=\"PDB_chain\")\n",
    "\n",
    "    all_nonpoly_chains = list()\n",
    "    for n in small_ref_table.index:\n",
    "        all_nonpoly_chains.append(n)\n",
    "\n",
    "    checked_chains_list = list()\n",
    "    for n in chain_and_num_available:\n",
    "        checked_chains_list.append(n[0])\n",
    "    checked_chains_list_uniq = list(np.unique(checked_chains_list))\n",
    "\n",
    "    available_numbers_to_chains = list()\n",
    "    for n in all_nonpoly_chains:\n",
    "        if n not in checked_chains_list_uniq:\n",
    "            for z in range(1, 10000):\n",
    "                available_numbers_to_chains.append((n, z))\n",
    "\n",
    "    chain_and_num_available.extend(available_numbers_to_chains)\n",
    "\n",
    "    df_chain_and_num_available = pd.DataFrame(chain_and_num_available, columns=['available_chain', \"available_number\"])\n",
    "    df_chain_and_num_available_sorted = df_chain_and_num_available.drop_duplicates(\n",
    "        subset=[\"available_chain\", \"available_number\"], keep=\"first\").sort_values(\n",
    "        [\"available_chain\", \"available_number\"], ascending=(True, False)).reset_index(drop=True)\n",
    "\n",
    "    df_for_nonpoly_replace = pd.DataFrame(list(), columns=['available_chain', \"available_number\"])\n",
    "    for n in df_nonpoly_dropped_dup_sorted.set_index([\"PDB_chain\", \"PDB\"]).count(level=\"PDB_chain\").index:\n",
    "        temporal_df_for_addition_of_available_num = df_chain_and_num_available_sorted.where(\n",
    "            df_chain_and_num_available_sorted['available_chain'] == n).dropna()[0:(small_ref_table[\"numbers\"][n])]\n",
    "        df_for_nonpoly_replace = df_for_nonpoly_replace.append(temporal_df_for_addition_of_available_num, ignore_index=True)\n",
    "\n",
    "    df_final_nonpoly = pd.merge(left=df_nonpoly_dropped_dup_sorted, right=df_for_nonpoly_replace, left_index=True, right_index=True)\n",
    "\n",
    "    df_final_nonpoly.loc[:, 'PDB_str'] = df_final_nonpoly.PDB.map(\n",
    "        lambda x: x[1] + \"{:>2}\".format(x[2]) + \"{:>4}\".format(x[0]) + \" \" if x[0].isdigit() else x[1] + \"{:>2}\".format(x[2]) + \"{:>5}\".format(x[0]))\n",
    "    df_final_nonpoly_corrected = df_final_nonpoly.drop(columns=['PDB_chain', 'numbers', \"available_chain\"])\n",
    "    df_final_nonpoly_corrected.loc[:, 'renum_str'] = df_final_nonpoly_corrected.PDB.map(\n",
    "        lambda x: x[1] + \"{:>2}\".format(x[2])) + df_final_nonpoly_corrected.available_number.map(lambda x: \"{:>4}\".format(str(int(x)))) + \" \"\n",
    "\n",
    "    return df_final_nonpoly_corrected\n",
    "\n",
    "\n",
    "def remark_465(missing_res_remark_465, df_PDBe_PDB_UniProt):\n",
    "    df_PDBe_PDB_UniProt_nulls = df_PDBe_PDB_UniProt.loc[df_PDBe_PDB_UniProt['PDB'].apply(lambda x: x[0] == \"null\")]\n",
    "    df_PDBe_PDB_UniProt_nulls = df_PDBe_PDB_UniProt_nulls.reset_index(drop=True)\n",
    "\n",
    "    df_mmCIF_remark_465 = pd.DataFrame(list(zip(missing_res_remark_465)))\n",
    "    df_mmCIF_remark_465 = df_mmCIF_remark_465.rename(columns={0: \"PDB_old\"})\n",
    "    df_mmCIF_remark_465 = df_mmCIF_remark_465.drop_duplicates()\n",
    "\n",
    "    df_remark_465_final = df_mmCIF_remark_465.merge(df_PDBe_PDB_UniProt_nulls, left_index=True, right_index=True)\n",
    "    df_remark_465_final.loc[:, 'PDB_str'] = df_remark_465_final.PDB_old.map(\n",
    "        lambda x: x[1] + \"{:>2}\".format(x[2]) + \"{:>6}\".format(x[0]) + \" \" if x[0].isdigit() else x[1] + \"{:>2}\".format(x[2]) + \"{:>5}\".format(x[0]))\n",
    "\n",
    "    df_final_poly_remark_465_corrected = df_remark_465_final.drop(columns=['UniProt', 'AccessionID', \"new_col_Uni\", \"UniProt_5k\", \"Uni_moD\"])\n",
    "    df_final_poly_remark_465_corrected.loc[:, 'renum_str'] = df_final_poly_remark_465_corrected.PDB_old.map(\n",
    "        lambda x: x[1] + \"{:>2}\".format(x[2])) + df_final_poly_remark_465_corrected.Three_Rows_CIF_Num_Uni.map(\n",
    "        lambda x: \"{:>6}\".format(str(int(x[1])))) + \" \"\n",
    "    df_final_poly_remark_465_corrected = df_final_poly_remark_465_corrected.reset_index(drop=True)\n",
    "\n",
    "    return df_final_poly_remark_465_corrected\n",
    "\n",
    "\n",
    "def final_dict_formation(df_final_poly_corrected, df_final_nonpoly_corrected, final_remark_465, chains_to_change):\n",
    "    all_data_df = df_final_poly_corrected.append(df_final_nonpoly_corrected, ignore_index=True, sort=False)\n",
    "    all_data_df = all_data_df.append(final_remark_465, ignore_index=True, sort=False)\n",
    "    all_data_df_drop_dup = all_data_df.drop_duplicates(subset=\"PDB_str\", keep='first')\n",
    "    not_in_chains_to_change = all_data_df_drop_dup.PDB.map(lambda x: x if x[2] in chains_to_change else \"not_in_chains_to_change\")\n",
    "    all_data_merged_not_in_chain_to_change = all_data_df_drop_dup.merge(not_in_chains_to_change.rename('not_in_chains_to_change'),\n",
    "                                                                        left_index=True, right_index=True)\n",
    "    all_data_df_drop_dup_drop_chains = all_data_merged_not_in_chain_to_change[\n",
    "        all_data_merged_not_in_chain_to_change.not_in_chains_to_change != 'not_in_chains_to_change']\n",
    "\n",
    "    different_indent_PDB_str = list()\n",
    "    for n in all_data_df_drop_dup_drop_chains[\"PDB_str\"]:\n",
    "        if \"-\" in n:\n",
    "            n = n[:5] + n[6:] + \" \"\n",
    "        different_indent_PDB_str.append(n)\n",
    "        different_indent_PDB_str.append(n[:3] + \" \" + n[3:])  # present HET\n",
    "        different_indent_PDB_str.append(n[:5] + \" \" + n[5:])  # present most common\n",
    "        different_indent_PDB_str.append(n[:5] + \"  \" + n[5:])  # present REMARK 500\n",
    "\n",
    "    different_indent_renum_str = list()\n",
    "    for n in all_data_df_drop_dup_drop_chains[\"renum_str\"]:\n",
    "        different_indent_renum_str.append(n)\n",
    "        different_indent_renum_str.append(n[:3] + \" \" + n[3:])  # present HET\n",
    "        different_indent_renum_str.append(n[:5] + \" \" + n[5:])  # present most common\n",
    "        different_indent_renum_str.append(n[:5] + \"  \" + n[5:])  # present REMARK 500\n",
    "\n",
    "    dict_for_replacement = dict(zip(different_indent_PDB_str, different_indent_renum_str))\n",
    "    return dict_for_replacement\n",
    "\n",
    "\n",
    "def replace_all(lines, dict_for_replacement):\n",
    "    location_of_the_value = 0\n",
    "    for key, value in dict_for_replacement.items():\n",
    "        if key in lines:\n",
    "            if location_of_the_value == lines.find(key):\n",
    "                continue\n",
    "            lines = lines.replace(key, value)\n",
    "            location_of_the_value = lines.find(value)\n",
    "    return lines\n",
    "\n",
    "\n",
    "def master_PDB_renumber_function(input_PDB_files_were_found, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "                                 default_output_path_to_PDB, default_PDB_num, gzip_mode, exception_AccessionIDs):\n",
    "    if not os.path.exists(default_output_path_to_PDB):\n",
    "        os.makedirs(default_output_path_to_PDB)\n",
    "\n",
    "    input_PDB_files_were_found_list = list()\n",
    "    input_PDB_files_were_found_list.append(input_PDB_files_were_found)\n",
    "\n",
    "    for PDB in input_PDB_files_were_found_list:\n",
    "\n",
    "        try:\n",
    "            assembly_num = re.search('\\.pdb(.*).gz', PDB).group(1)\n",
    "            SIFTS_name = PDB[:4] + \".xml.gz\"\n",
    "            PDB_id = PDB[:4]\n",
    "        except AttributeError:\n",
    "            assembly_num = \"\"\n",
    "            SIFTS_name = PDB[3:7] + \".xml.gz\"\n",
    "            PDB_id = PDB[3:7]\n",
    "\n",
    "        # for no corresponding SIFTS\n",
    "        try:\n",
    "            gzip.open(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name), 'rt')\n",
    "        except FileNotFoundError:\n",
    "            copy_file(default_input_path_to_PDB, PDB, default_output_path_to_PDB, \"_renum.pdb\" + assembly_num, gzip_mode)\n",
    "            log_message = if_no_SIFTS_data_log_for_PDB(default_input_path_to_PDB, PDB_id, PDB)\n",
    "            return log_message\n",
    "\n",
    "        # for zero byte SIFTS\n",
    "        if os.path.getsize(Path(str(default_input_path_to_SIFTS) + \"/\" + SIFTS_name)) == 0:\n",
    "            copy_file(default_input_path_to_PDB, PDB, default_output_path_to_PDB, \"_renum.pdb\" + assembly_num, gzip_mode)\n",
    "            log_message = if_no_SIFTS_data_log_for_PDB(default_input_path_to_PDB, PDB_id, PDB)\n",
    "            return log_message\n",
    "\n",
    "        product_tree_SIFTS = try_SIFTS_tree_parser(default_input_path_to_SIFTS, SIFTS_name)\n",
    "        if product_tree_SIFTS == 0:\n",
    "            continue\n",
    "\n",
    "        tuple_PDBe_for_PDB_and_tuple_PDB = product_tree_SIFTS[0]\n",
    "        tuple_PDBe_for_UniProt_and_tuple_UniProt = product_tree_SIFTS[1]\n",
    "        UniProt_conversion_dict = product_tree_SIFTS[2]\n",
    "\n",
    "        # for no UniProt in SIFTS\n",
    "        if len(tuple_PDBe_for_UniProt_and_tuple_UniProt) == 0:\n",
    "            copy_file(default_input_path_to_PDB, PDB, default_output_path_to_PDB, \"_renum.pdb\" + assembly_num, gzip_mode)\n",
    "            log_message = if_no_SIFTS_data_log_for_PDB(default_input_path_to_PDB, PDB_id, PDB)\n",
    "            return log_message\n",
    "\n",
    "        split = try_PDB(default_input_path_to_PDB, PDB)\n",
    "        if split == 0:\n",
    "            continue\n",
    "\n",
    "        product_of_SIFTS_data_parser = SIFTS_data_parser_for_PDB(tuple_PDBe_for_PDB_and_tuple_PDB,\n",
    "                                                                 tuple_PDBe_for_UniProt_and_tuple_UniProt,\n",
    "                                                                 default_PDB_num, 'all')\n",
    "        df_PDBe_PDB_UniProt = product_of_SIFTS_data_parser[1]\n",
    "\n",
    "        handling_chain_numbering = handling_chain_numbering_clashes(df_PDBe_PDB_UniProt, exception_AccessionIDs)\n",
    "        chains_to_change = handling_chain_numbering[0]\n",
    "        combined_tuple_PDBe_UniProt_AccessionID = handling_chain_numbering[1]\n",
    "        longest_AccessionID_list = handling_chain_numbering[3]\n",
    "        chains_to_change_one_to_end = handling_chain_numbering[4]\n",
    "\n",
    "        product_of_SIFTS_data_parser = SIFTS_data_parser_for_PDB(tuple_PDBe_for_PDB_and_tuple_PDB, combined_tuple_PDBe_UniProt_AccessionID,\n",
    "                                                                 default_PDB_num, chains_to_change)\n",
    "        df_PDBe_PDB_UniProt_without_null_index_PDBe = product_of_SIFTS_data_parser[0]\n",
    "        df_PDBe_PDB_UniProt = product_of_SIFTS_data_parser[1]\n",
    "\n",
    "        renumbered_count = renumbered_count_in_chains(chains_to_change_one_to_end, df_PDBe_PDB_UniProt_without_null_index_PDBe,\n",
    "                                                      PDB_id, UniProt_conversion_dict, longest_AccessionID_list)\n",
    "        chain_total_renum = renumbered_count[0]\n",
    "        nothing_changed = renumbered_count[1]\n",
    "\n",
    "        chain_total_renum.append(nothing_changed)\n",
    "        mod_log_message = chain_total_renum\n",
    "\n",
    "        if nothing_changed == 0:\n",
    "            copy_file(default_input_path_to_PDB, PDB, default_output_path_to_PDB, \"_renum.pdb\" + assembly_num, gzip_mode)\n",
    "            return mod_log_message\n",
    "\n",
    "        parsed_PDB = PDB_parser(split, df_PDBe_PDB_UniProt_without_null_index_PDBe, default_PDB_num)\n",
    "        pd_series_index_PDBe = parsed_PDB[0]\n",
    "        num_ins_code_name_chain_HETATM = parsed_PDB[1]\n",
    "        df_final_poly_corrected = parsed_PDB[2]\n",
    "        missing_res_remark_465 = parsed_PDB[3]\n",
    "        Num_is_Too_Big = parsed_PDB[4]\n",
    "\n",
    "        # when numbers get too big\n",
    "        if not Num_is_Too_Big:\n",
    "            copy_file(default_input_path_to_PDB, PDB, default_output_path_to_PDB, \"_renum.pdb\" + assembly_num, gzip_mode)\n",
    "            return mod_log_message\n",
    "\n",
    "        df_final_nonpoly_corrected = non_poly_num(pd_series_index_PDBe, num_ins_code_name_chain_HETATM)\n",
    "        if len(missing_res_remark_465) != 0:\n",
    "            final_remark_465 = remark_465(missing_res_remark_465, df_PDBe_PDB_UniProt)\n",
    "        else:\n",
    "            final_remark_465 = None\n",
    "\n",
    "        dict_for_replacement = final_dict_formation(df_final_poly_corrected, df_final_nonpoly_corrected, final_remark_465, chains_to_change)\n",
    "        \n",
    "\n",
    "        outF = open(Path(str(default_output_path_to_PDB) + \"/\" + PDB_id + \"_renum.pdb\" + assembly_num), \"w\")\n",
    "        # PDBrenum REMARK 0 insert \n",
    "        start_remark_0 = 0\n",
    "        for lines in split:\n",
    "            if lines.startswith(\"HEADER\"):\n",
    "                start_remark_0 += 1\n",
    "            else:\n",
    "                split = split[:start_remark_0]+ PDBrenum_REMARK +split[start_remark_0:]\n",
    "                break\n",
    "                \n",
    "        for lines in split:\n",
    "            lines = replace_all(lines, dict_for_replacement)\n",
    "            outF.write(lines)\n",
    "            outF.write(\"\\n\")\n",
    "        outF.close()\n",
    "\n",
    "        if gzip_mode == \"on\":\n",
    "            with open(Path(str(default_output_path_to_PDB) + \"/\" + PDB_id + \"_renum.pdb\" + assembly_num), 'rb') as f_in:\n",
    "                with gzip.open(Path(str(default_output_path_to_PDB) + \"/\" + PDB_id + \"_renum.pdb\" + assembly_num + \".gz\"), 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            os.remove(Path(str(default_output_path_to_PDB) + \"/\" + PDB_id + \"_renum.pdb\" + assembly_num))\n",
    "\n",
    "        return mod_log_message\n",
    "\n",
    "\n",
    "def ProcessPool_run_renum_PDB(format_to_download, input_PDB_files_were_found, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "                              default_output_path_to_PDB, default_PDB_num, gzip_mode, exception_AccessionIDs, nproc):\n",
    "    if not os.path.exists(default_output_path_to_PDB):\n",
    "        os.makedirs(default_output_path_to_PDB)\n",
    "\n",
    "    resulting = list()\n",
    "    executor = ProcessPoolExecutor(max_workers=nproc)\n",
    "    partial_master_PDB_renumber_function = partial(master_PDB_renumber_function,\n",
    "                                                   default_input_path_to_PDB=default_input_path_to_PDB,\n",
    "                                                   default_input_path_to_SIFTS=default_input_path_to_SIFTS,\n",
    "                                                   default_output_path_to_PDB=default_output_path_to_PDB,\n",
    "                                                   default_PDB_num=default_PDB_num, gzip_mode=gzip_mode,\n",
    "                                                   exception_AccessionIDs=exception_AccessionIDs)\n",
    "\n",
    "    jobs = [executor.submit(partial_master_PDB_renumber_function, pdb_files) for pdb_files in input_PDB_files_were_found]\n",
    "    with tqdm.tqdm(total=len(jobs), position=0, leave=True, desc=\"Renumbering \" + format_to_download + \" files\") as pbar:\n",
    "        for job in as_completed(jobs):\n",
    "            result = job.result()\n",
    "            resulting.append(result)\n",
    "            pbar.update(1)\n",
    "\n",
    "    return resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Renumbering PDB_assembly files: 100%|██████████| 252365/252365 [23:52:50<00:00,  2.94it/s]    \n"
     ]
    }
   ],
   "source": [
    "input_PDB_assembly_files_were_found = look_what_is_inside(\"PDB_assembly\")\n",
    "if __name__ == '__main__':\n",
    "    ProcessPool_run_renum_PDB(\"PDB_assembly\", input_PDB_assembly_files_were_found, default_input_path_to_PDB_assembly, default_input_path_to_SIFTS,\n",
    "                              default_output_path_to_PDB_assembly, default_PDB_num, gzip_mode, exception_AccessionIDs, nproc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProcessPool_run_renum_PDB(format_to_download=\"PDB\", input_PDB_files_were_found=(),\n",
    "                              default_input_path_to_PDB=default_input_path_to_PDB, \n",
    "                              default_input_path_to_SIFTS=default_input_path_to_SIFTS, \n",
    "                              default_output_path_to_PDB=default_output_path_to_PDB, \n",
    "                              default_PDB_num=default_PDB_num, gzip_mode=gzip_mode):\n",
    "    if not os.path.exists(default_output_path_to_PDB):\n",
    "        os.makedirs(default_output_path_to_PDB)\n",
    "    \n",
    "    resulting = list()\n",
    "    executor = ProcessPoolExecutor()\n",
    "    partial_master_PDB_renumber_function = partial(master_PDB_renumber_function, \n",
    "                                                   default_input_path_to_PDB=default_input_path_to_PDB, \n",
    "                                                   default_input_path_to_SIFTS=default_input_path_to_SIFTS, \n",
    "                                                   default_output_path_to_PDB=default_output_path_to_PDB, \n",
    "                                                   default_PDB_num=default_PDB_num, gzip_mode=gzip_mode)\n",
    "    \n",
    "    jobs = [executor.submit(partial_master_PDB_renumber_function, pdb_files) for pdb_files in input_PDB_files_were_found]\n",
    "    for job in tqdm.tqdm(as_completed(jobs), total=len(jobs), position=0, leave=True, desc=\"Renumbering \"+format_to_download+\" files\"):\n",
    "        resultus = job.result()\n",
    "        resulting.append(resultus)\n",
    "    \n",
    "    return resulting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_PDB_files_were_found = look_what_is_inside(\"PDB\")\n",
    "# if __name__ == '__main__':\n",
    "#     resulting2 = ProcessPool_run_renum_PDB(\"PDB\", input_PDB_files_were_found,\n",
    "#                                           default_input_path_to_PDB, \n",
    "#                                           default_input_path_to_SIFTS,\n",
    "#                                           default_output_path_to_PDB,\n",
    "#                                           default_PDB_num, gzip_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_PDB_assembly_files_were_found = look_what_is_inside(\"PDB_assembly\")\n",
    "if __name__ == '__main__':\n",
    "    resulting = ProcessPool_run_renum_PDB(\"PDB_assembly\", input_PDB_assembly_files_were_found,\n",
    "                                          default_input_path_to_PDB_assembly, \n",
    "                                          default_input_path_to_SIFTS,\n",
    "                                          default_output_path_to_PDB_assembly,\n",
    "                                          default_PDB_num, gzip_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def log_translator_reader(list_of_uni, mod=\"AccessionId\"):\n",
    "#     if mod == \"Human_readble_UniProt\":\n",
    "#         index_to_look = 1\n",
    "#     else:\n",
    "#         index_to_look = 0\n",
    "        \n",
    "#     target_pdb = set()\n",
    "#     if type(list_of_uni) is str:\n",
    "#         list_of_uni = [list_of_uni]\n",
    "\n",
    "#     with open('log_translator.txt', 'r') as filehandle:\n",
    "#         for n in filehandle.readlines():\n",
    "#             for uni in list_of_uni:\n",
    "#                 if n.split()[index_to_look] == uni:\n",
    "#                     target_pdb.add(n.split()[2])\n",
    "#     return(target_pdb)\n",
    "\n",
    "# log_translator_reader([\"GGACT_HUMAN\",\"NBS1_SCHPO\"], \"Human_readble_UniProt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_PDB_entries_with_renum_0 = set()\n",
    "all_data = set()\n",
    "with open('log_PDBrenum.txt', 'rt') as f:\n",
    "    for n in f.readlines():\n",
    "        # print(n.split())\n",
    "        all_data.add(n.split()[1])\n",
    "        if n.split()[-2] == \"0\":\n",
    "             # print(n.split())\n",
    "            set_of_PDB_entries_with_renum_0.add(n.split()[1])\n",
    "print(\"total:\", len(all_data))    \n",
    "print(\"zero_renum:\", len(set_of_PDB_entries_with_renum_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################DOWNLOADER################################\n",
    "from src.download.modules import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###read latest catalog and return list of all file names\n",
    "def latest_catalog_reader():\n",
    "    files_of_current_directory = os.listdir(current_directory)\n",
    "    paths_to_ls_lR = list()\n",
    "    paths_to_xml = list()\n",
    "\n",
    "    for n in files_of_current_directory:\n",
    "        if n.startswith('ls-lR'):\n",
    "            n = current_directory + \"/\" + n + \"/\" + 'ls-lR'\n",
    "            paths_to_ls_lR.append(n)\n",
    "        if n.startswith('xml'):\n",
    "            n = current_directory + \"/\" + n + \"/\" + 'xml'\n",
    "            paths_to_xml.append(n)\n",
    "\n",
    "    paths_to_ls_lR_sorted = sorted(paths_to_ls_lR, reverse=True)\n",
    "    paths_to_xml_sorted = sorted(paths_to_xml, reverse=True)\n",
    "\n",
    "    try:\n",
    "        path_to_the_latest_list = paths_to_ls_lR_sorted[0]\n",
    "\n",
    "        df_catalog_the_latest_listing = pd.read_csv(path_to_the_latest_list,\n",
    "                                                    names=[\"1\", \"2\", \"3\", \"4\", \"Data_size\", \"Month\", \"Day\", \"Time\",\n",
    "                                                           \"file_name\", \"10\", \"file_names_path\"], sep=\"\\s+\",\n",
    "                                                    low_memory=False)\n",
    "\n",
    "        # mmCIF\n",
    "        df_catalog_the_latest_mmCIF_listing_dropna = df_catalog_the_latest_listing.dropna()\n",
    "        df_catalog_the_latest_mmCIF_listing_dropna_cif_gz = df_catalog_the_latest_mmCIF_listing_dropna[\n",
    "            df_catalog_the_latest_mmCIF_listing_dropna['file_name'].str.endswith('cif.gz')]\n",
    "        df_catalog_the_latest_mmCIF_listing_dropna_cif_gz_34kb = df_catalog_the_latest_mmCIF_listing_dropna_cif_gz[\n",
    "            df_catalog_the_latest_mmCIF_listing_dropna_cif_gz.Data_size == 34.0]\n",
    "\n",
    "        all_mmCIF_files = list()\n",
    "        for n in df_catalog_the_latest_mmCIF_listing_dropna_cif_gz_34kb[\"file_name\"]:\n",
    "            all_mmCIF_files.append(n)\n",
    "\n",
    "        # PDB\n",
    "        df_catalog_the_latest_PDB_listing_dropna = df_catalog_the_latest_listing.dropna()\n",
    "        df_catalog_the_latest_PDB_listing_dropna_ent_gz = df_catalog_the_latest_PDB_listing_dropna[\n",
    "            df_catalog_the_latest_PDB_listing_dropna['file_name'].str.endswith('ent.gz')]\n",
    "        df_catalog_the_latest_PDB_listing_dropna_ent_gz_34kb = df_catalog_the_latest_PDB_listing_dropna_ent_gz[\n",
    "            df_catalog_the_latest_PDB_listing_dropna_ent_gz.Data_size == 35.0]\n",
    "\n",
    "        all_PDB_files = list()\n",
    "        for n in df_catalog_the_latest_PDB_listing_dropna_ent_gz_34kb[\"file_name\"]:\n",
    "            all_PDB_files.append(n)\n",
    "\n",
    "        # SIFTS\n",
    "        path_to_the_latest_list = paths_to_xml_sorted[0]\n",
    "\n",
    "        df_catalog_the_latest_listing = pd.read_csv(path_to_the_latest_list,\n",
    "                                                    names=[\"1\", \"2\", \"3\", \"4\", \"Data_size\", \"Month\", \"Day\", \"Time\",\n",
    "                                                           \"file_name\", \"10\", \"file_names_path\"], sep=\"\\s+\",\n",
    "                                                    low_memory=False)\n",
    "\n",
    "        df_catalog_the_latest_SIFTS_listing_dropna = df_catalog_the_latest_listing.dropna()\n",
    "        df_catalog_the_latest_SIFTS_listing_dropna_xml_gz = df_catalog_the_latest_SIFTS_listing_dropna[\n",
    "            df_catalog_the_latest_SIFTS_listing_dropna['file_name'].str.endswith('xml.gz')]\n",
    "        df_catalog_the_latest_SIFTS_listing_dropna_cif_gz_34kb = df_catalog_the_latest_SIFTS_listing_dropna_xml_gz[\n",
    "            df_catalog_the_latest_SIFTS_listing_dropna_xml_gz.Data_size == 27.0]\n",
    "\n",
    "        all_SIFTS_files = list()\n",
    "        for n in df_catalog_the_latest_SIFTS_listing_dropna_cif_gz_34kb[\"file_name\"]:\n",
    "            all_SIFTS_files.append(n)\n",
    "\n",
    "    except IndexError:\n",
    "        print(\"Sorry, nothing to read from. Try catalog_downloader() command first.\")\n",
    "        all_mmCIF_files = None\n",
    "        all_PDB_files = None\n",
    "        all_SIFTS_files = None\n",
    "\n",
    "    return [all_mmCIF_files, all_PDB_files, all_SIFTS_files]\n",
    "\n",
    "def my_hook(t):\n",
    "    last_b = [0]\n",
    "\n",
    "    def update_to(b=1, b_size=1, t_size=None):\n",
    "        if t_size is not None:\n",
    "            t.total = t_size\n",
    "        t.update((b - last_b[0]) * b_size)\n",
    "        last_b[0] = b\n",
    "    return update_to\n",
    "\n",
    "\n",
    "def downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes):\n",
    "    socket.setdefaulttimeout(300)\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            last_slash = ftp_to_download.rsplit('/', 1)[-1]\n",
    "            with tqdm.tqdm(unit=\"B\", unit_scale=True, desc=\"Downloading mmCIF/SIFTS catalogs \" + last_slash, position=0, leave=True) as t:\n",
    "                reporthook = my_hook(t)\n",
    "                urllib.request.urlretrieve(ftp_to_download, Path(str(where_the_file_goes) + \"/\" + last_slash), reporthook=reporthook)\n",
    "            break\n",
    "        except Exception:\n",
    "            time.sleep(1)\n",
    "\n",
    "\n",
    "def catalog_downloader():\n",
    "    \"\"\"PDB ls-lR catalog\"\"\"\n",
    "    ftp_for_all_mmCIF_and_PDB = \"https://files.wwpdb.org/pub/pdb/holdings/current_file_holdings.json.gz\"\n",
    "\n",
    "    ftp_to_download = ftp_for_all_mmCIF_and_PDB\n",
    "    last_slash = ftp_to_download.rsplit('/', 1)[-1]\n",
    "    today_date = date.today()\n",
    "    today_date_str = today_date.strftime(\"_%Y_%m_%d\")\n",
    "\n",
    "    where_the_file_goes = current_directory + \"/\" + last_slash + today_date_str\n",
    "    if not os.path.exists(where_the_file_goes):\n",
    "        os.makedirs(where_the_file_goes)\n",
    "        downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes)\n",
    "    else:\n",
    "        if not os.path.isfile(where_the_file_goes + \"/ls-lR\"):\n",
    "            downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes)\n",
    "\n",
    "    # downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes)\n",
    "\n",
    "    # reading txt file SIFTS parent catalog and creating pandas df out of it\n",
    "    df_catalog_listing_everything = pd.read_csv(Path(str(where_the_file_goes) + \"/\" + last_slash),\n",
    "                                                names=[\"1\", \"2\", \"3\", \"4\", \"Data_size\", \"Month\", \"Day\", \"Time\",\n",
    "                                                       \"file_name\", \"10\", \"file_names_path\"], sep=\"\\s+\",\n",
    "                                                low_memory=False)\n",
    "\n",
    "    # Dropping all unnecessary rows leaving only files with 'cif.gz' endings\n",
    "    df_mmCIF_catalog_dropna = df_catalog_listing_everything.dropna()\n",
    "    df_mmCIF_catalog_dropna_cif_gz = df_mmCIF_catalog_dropna[df_mmCIF_catalog_dropna['file_name'].str.endswith('cif.gz')]\n",
    "    df_mmCIF_catalog_dropna_cif_gz_34kb = df_mmCIF_catalog_dropna_cif_gz[df_mmCIF_catalog_dropna_cif_gz.Data_size == 34.0]\n",
    "\n",
    "    # Dropping all unnecessary rows leaving only files with 'ent.gz' endings\n",
    "    df_PDB_catalog_dropna = df_catalog_listing_everything.dropna()\n",
    "    df_PDB_catalog_dropna_ent_gz = df_PDB_catalog_dropna[df_PDB_catalog_dropna['file_name'].str.endswith('ent.gz')]\n",
    "    df_PDB_catalog_dropna_ent_gz_35kb = df_PDB_catalog_dropna_ent_gz[df_PDB_catalog_dropna_ent_gz.Data_size == 35.0]\n",
    "\n",
    "    # creating lists of the mmCIF file_names\n",
    "    list_of_mmCIF_cif_gz_file_names = list()\n",
    "    for mmCIF_file_name in df_mmCIF_catalog_dropna_cif_gz_34kb[\"file_name\"]:\n",
    "        list_of_mmCIF_cif_gz_file_names.append(mmCIF_file_name)\n",
    "\n",
    "    # creating lists of the PDB file_names\n",
    "    list_of_PDB_ent_gz_file_names = list()\n",
    "    for PDB_file_name in df_PDB_catalog_dropna_ent_gz_35kb[\"file_name\"]:\n",
    "        list_of_PDB_ent_gz_file_names.append(PDB_file_name)\n",
    "\n",
    "    \"\"\"SIFTS xml catalog\"\"\"\n",
    "    ftp_all_SIFTS = \"https://ftp.ebi.ac.uk/pub/databases/msd/sifts/xml\"\n",
    "    ftp_to_download = ftp_all_SIFTS\n",
    "    last_slash = ftp_to_download.rsplit('/', 1)[-1]\n",
    "\n",
    "    where_the_file_goes = current_directory + \"/\" + last_slash + today_date_str\n",
    "    if not os.path.exists(where_the_file_goes):\n",
    "        os.makedirs(where_the_file_goes)\n",
    "        downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes)\n",
    "    else:\n",
    "        if not os.path.isfile(where_the_file_goes + \"/xml\"):\n",
    "            downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes)\n",
    "\n",
    "    # downloader_for_catalog_with_urllib(ftp_to_download, where_the_file_goes)\n",
    "\n",
    "    # reading txt file SIFTS parent catalog and creating pandas df out of it\n",
    "    df_catalog_listing_everything = pd.read_csv(Path(str(where_the_file_goes) + \"/\" + last_slash),\n",
    "                                                names=[\"1\", \"2\", \"3\", \"4\", \"Data_size\", \"Month\", \"Day\", \"Time\",\n",
    "                                                       \"file_name\", \"10\", \"file_names_path\"], sep=\"\\s+\",\n",
    "                                                low_memory=False)\n",
    "\n",
    "    # Dropping all unnecessary rows leaving only files with 'xml.gz' endings\n",
    "    df_SIFTS_catalog_dropna = df_catalog_listing_everything.dropna()\n",
    "    df_SIFTS_catalog_dropna_xml_gz = df_SIFTS_catalog_dropna[\n",
    "        df_SIFTS_catalog_dropna['file_name'].str.endswith('xml.gz')]\n",
    "    df_SIFTS_catalog_dropna_xml_gz_27kb = df_SIFTS_catalog_dropna_xml_gz[\n",
    "        df_SIFTS_catalog_dropna_xml_gz.Data_size == 27.0]\n",
    "\n",
    "    # creating lists of the SIFTS file_names\n",
    "    list_of_SIFTS_xml_gz_file_names = list()\n",
    "    for SIFTS_file_names in df_SIFTS_catalog_dropna_xml_gz_27kb[\"file_name\"]:\n",
    "        list_of_SIFTS_xml_gz_file_names.append(SIFTS_file_names)\n",
    "\n",
    "    _4Char_list_of_SIFTS_xml_gz_file_names = list()\n",
    "    for SIFTS_file_names_4Char in list_of_SIFTS_xml_gz_file_names:\n",
    "        _4Char_list_of_SIFTS_xml_gz_file_names.append(SIFTS_file_names_4Char[:4])\n",
    "\n",
    "    _4Char_list_of_PDB_ent_gz_file_names = list()\n",
    "    for PDB_file_names_4Char in list_of_PDB_ent_gz_file_names:\n",
    "        _4Char_list_of_PDB_ent_gz_file_names.append(PDB_file_names_4Char[3:7])\n",
    "\n",
    "    _4Char_list_of_mmCIF_cif_gz_file_names = list()\n",
    "    for mmCIF_file_names_4Char in list_of_mmCIF_cif_gz_file_names:\n",
    "        _4Char_list_of_mmCIF_cif_gz_file_names.append(mmCIF_file_names_4Char[:4])\n",
    "\n",
    "    df_list_of_mmCIF_cif_gz_file_names = pd.DataFrame(\n",
    "        zip(list_of_mmCIF_cif_gz_file_names, _4Char_list_of_mmCIF_cif_gz_file_names), columns=[\"mmCIF\", \"4mmCIF\"])\n",
    "    df_list_of_PDB_ent_gz_file_names = pd.DataFrame(\n",
    "        zip(list_of_PDB_ent_gz_file_names, _4Char_list_of_PDB_ent_gz_file_names), columns=[\"PDB\", \"4PDB\"])\n",
    "    df_list_of_SIFTS_xml_gz_file_names = pd.DataFrame(\n",
    "        zip(list_of_SIFTS_xml_gz_file_names, _4Char_list_of_SIFTS_xml_gz_file_names), columns=[\"SIFTS\", \"4SIFTS\"])\n",
    "\n",
    "    merged_df_mmCIF_PDB_file_names = df_list_of_mmCIF_cif_gz_file_names.merge(df_list_of_PDB_ent_gz_file_names,\n",
    "                                                                              left_on='4mmCIF', right_on='4PDB',\n",
    "                                                                              how=\"left\")\n",
    "    merged_df_mmCIF_PDB_SIFTS_file_names = merged_df_mmCIF_PDB_file_names.merge(df_list_of_SIFTS_xml_gz_file_names,\n",
    "                                                                                left_on='4mmCIF', right_on='4SIFTS',\n",
    "                                                                                how=\"left\")\n",
    "\n",
    "    merged_df_mmCIF_PDB_SIFTS_file_names['SIFTS'] = merged_df_mmCIF_PDB_SIFTS_file_names['SIFTS'].replace(np.nan, \"0000\")\n",
    "    merged_df_mmCIF_PDB_SIFTS_file_names['PDB'] = merged_df_mmCIF_PDB_SIFTS_file_names['PDB'].replace(np.nan, \"0000\")\n",
    "\n",
    "    SIFTS_file_names_with_null_if_files_absent = list()\n",
    "    for SIFTS_file_name_null_for_absent in merged_df_mmCIF_PDB_SIFTS_file_names['SIFTS']:\n",
    "        SIFTS_file_names_with_null_if_files_absent.append(SIFTS_file_name_null_for_absent)\n",
    "\n",
    "    PDB_file_names_with_null_if_files_absent = list()\n",
    "    for PDB_file_name_null_for_absent in merged_df_mmCIF_PDB_SIFTS_file_names['PDB']:\n",
    "        PDB_file_names_with_null_if_files_absent.append(PDB_file_name_null_for_absent)\n",
    "\n",
    "    mmCIF_file_names_with_null_if_files_absent = list()\n",
    "    for mmCIF_file_name_null_for_absent in merged_df_mmCIF_PDB_SIFTS_file_names['mmCIF']:\n",
    "        mmCIF_file_names_with_null_if_files_absent.append(mmCIF_file_name_null_for_absent)\n",
    "\n",
    "    return (mmCIF_file_names_with_null_if_files_absent,\n",
    "            PDB_file_names_with_null_if_files_absent,\n",
    "            SIFTS_file_names_with_null_if_files_absent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.download.modules import *\n",
    "from src.download.catalogdownloader import catalog_downloader\n",
    "from src.download.latestcatreader import latest_catalog_reader\n",
    "\n",
    "from src.download.modules import *\n",
    "# from src.download.catalogdownloader import catalog_downloader\n",
    "# from src.download.latestcatreader import latest_catalog_reader\n",
    "# from src.download.lookfilesinside import look_what_is_inside\n",
    "\n",
    "# default_input_path_to_mmCIF = current_directory + \"/mmCIF\"\n",
    "# default_input_path_to_mmCIF_assembly = current_directory + \"/mmCIF_assembly\"\n",
    "# default_input_path_to_PDB = current_directory + \"/PDB\"\n",
    "# default_input_path_to_PDB_assembly = current_directory + \"/PDB_assembly\"\n",
    "# default_input_path_to_SIFTS = current_directory + \"/SIFTS\"\n",
    "# default_output_path_to_mmCIF = current_directory + \"/output_mmCIF\"\n",
    "# default_output_path_to_mmCIF_assembly = current_directory + \"/output_mmCIF_assembly\"\n",
    "# default_output_path_to_PDB = current_directory + \"/output_PDB\"\n",
    "# default_output_path_to_PDB_assembly = current_directory + \"/output_PDB_assembly\"\n",
    "\n",
    "\n",
    "def download_pdb_assemblies_list_with_lxml():\n",
    "    for _ in range(5):\n",
    "        session = requests.Session()\n",
    "        # rcsb = \"https://files.rcsb.org/pub/pdb/data/biounit/PDB/all/\"\n",
    "        wwpdb = \"https://ftp.wwpdb.org/pub/pdb/data/biounit/PDB/all/\"\n",
    "        links = set()\n",
    "        try:\n",
    "            with session.get(wwpdb, stream=True, timeout=100) as r:\n",
    "                dom = html.fromstring(r.content)\n",
    "                for link in dom.xpath('//a/@href'):\n",
    "                    if \".gz\" in link:\n",
    "                        links.add(wwpdb + link)\n",
    "            return links\n",
    "        except requests.exceptions.RequestException:\n",
    "            pass\n",
    "\n",
    "\n",
    "def url_formation_for_pool(format_to_download=\"mmCIF\", list_of_file_names=(),\n",
    "                           default_input_path_to_mmCIF=current_directory + \"/mmCIF\",\n",
    "                           default_input_path_to_PDB=current_directory + \"/PDB\",\n",
    "                           default_input_path_to_SIFTS=current_directory + \"/SIFTS\",\n",
    "                           default_input_path_to_mmCIF_assembly=current_directory + \"/mmCIF_assembly\",\n",
    "                           default_input_path_to_PDB_assembly=current_directory + \"/PDB_assembly\"):\n",
    "    urls_to_target_files = list()\n",
    "    for file_name in list_of_file_names:\n",
    "        if file_name == \"0000\":\n",
    "            continue\n",
    "        if len(file_name) >= 4:\n",
    "            if format_to_download == \"mmCIF\" or format_to_download == \"all\":\n",
    "                if not os.path.exists(default_input_path_to_mmCIF):\n",
    "                    os.makedirs(default_input_path_to_mmCIF)\n",
    "                if \"ent\" in file_name and file_name.startswith('pdb'):\n",
    "                    target_name = file_name[3:7] + \".cif.gz\"\n",
    "                else:\n",
    "                    target_name = file_name[0:4] + \".cif.gz\"\n",
    "                urls_to_target_files.append(\"https://files.rcsb.org/pub/pdb/data/structures/all/mmCIF/\" + target_name)\n",
    "\n",
    "            if format_to_download == \"PDB\" or format_to_download == \"all\":\n",
    "                if not os.path.exists(default_input_path_to_PDB):\n",
    "                    os.makedirs(default_input_path_to_PDB)\n",
    "                if \"ent\" in file_name and file_name.startswith('pdb'):\n",
    "                    target_name = \"pdb\" + file_name[3:7] + \".ent.gz\"\n",
    "                else:\n",
    "                    target_name = \"pdb\" + file_name[0:4] + \".ent.gz\"\n",
    "                urls_to_target_files.append(\"https://files.rcsb.org/pub/pdb/data/structures/all/pdb/\" + target_name)\n",
    "\n",
    "            if format_to_download == \"SIFTS\" or format_to_download == \"all\":\n",
    "                if not os.path.exists(default_input_path_to_SIFTS):\n",
    "                    os.makedirs(default_input_path_to_SIFTS)\n",
    "                if \"ent\" in file_name and file_name.startswith('pdb'):\n",
    "                    target_name = file_name[3:7] + \".xml.gz\"\n",
    "                else:\n",
    "                    target_name = file_name[0:4] + \".xml.gz\"\n",
    "                urls_to_target_files.append(\"http://ftp.ebi.ac.uk/pub/databases/msd/sifts/xml/\" + target_name)\n",
    "\n",
    "            if format_to_download == \"mmCIF_assembly\" or format_to_download == \"all\":\n",
    "                if not os.path.exists(default_input_path_to_mmCIF_assembly):\n",
    "                    os.makedirs(default_input_path_to_mmCIF_assembly)\n",
    "                if \"ent\" in file_name and file_name.startswith('pdb'):\n",
    "                    target_name = file_name[3:7] + \".cif.gz\"\n",
    "                else:\n",
    "                    target_name = file_name[0:4] + \".cif.gz\"\n",
    "                urls_to_target_files.append(\"https://www.ebi.ac.uk/pdbe/static/entry/\" + target_name[:4] + \"-assembly.xml\")\n",
    "\n",
    "            if format_to_download == \"PDB_assembly\":\n",
    "                if not os.path.exists(default_input_path_to_PDB_assembly):\n",
    "                    os.makedirs(default_input_path_to_PDB_assembly)\n",
    "                urls_to_target_files.append(\"https://ftp.wwpdb.org/pub/pdb/data/biounit/PDB/all/\" + file_name)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Input file names list is not correct!!! It cannot be less than 4 characters\")\n",
    "\n",
    "    return urls_to_target_files\n",
    "\n",
    "\n",
    "def download_with_pool(urls_to_target_files=(),\n",
    "                       default_input_path_to_mmCIF=current_directory + \"/mmCIF\",\n",
    "                       default_input_path_to_PDB=current_directory + \"/PDB\",\n",
    "                       default_input_path_to_SIFTS=current_directory + \"/SIFTS\",\n",
    "                       default_input_path_to_mmCIF_assembly=current_directory + \"/mmCIF_assembly\",\n",
    "                       default_input_path_to_PDB_assembly=current_directory + \"/PDB_assembly\"):\n",
    "    try:\n",
    "        file_name_start_pos = urls_to_target_files.rfind(\"/\") + 1\n",
    "        format_start_pos = file_name_start_pos - 4\n",
    "        file_name = urls_to_target_files[file_name_start_pos:]\n",
    "        format_of_db = urls_to_target_files[format_start_pos:format_start_pos + 3]\n",
    "\n",
    "        r = requests.get(urls_to_target_files, stream=True)\n",
    "\n",
    "        if format_of_db == \"CIF\":\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(default_input_path_to_mmCIF + \"/\" + file_name, 'wb') as f:\n",
    "                    for data in r:\n",
    "                        f.write(data)\n",
    "\n",
    "        if format_of_db == \"pdb\":\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(default_input_path_to_PDB + \"/\" + file_name, 'wb') as f:\n",
    "                    for data in r:\n",
    "                        f.write(data)\n",
    "\n",
    "        if format_of_db == \"xml\":\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(default_input_path_to_SIFTS + \"/\" + file_name, 'wb') as f:\n",
    "                    for data in r:\n",
    "                        f.write(data)\n",
    "\n",
    "        if format_of_db == \"all\":\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                with open(default_input_path_to_PDB_assembly + \"/\" + file_name, 'wb') as f:\n",
    "                    for data in r:\n",
    "                        f.write(data)\n",
    "\n",
    "        if format_of_db == \"try\":\n",
    "            if r.status_code == requests.codes.ok:\n",
    "                root = ET.fromstring(r.text)\n",
    "                for n in root:\n",
    "                    compos_ID_list = list(n.attrib.items())\n",
    "                    if compos_ID_list[1][0] == \"id\":\n",
    "                        req_child = requests.get(\n",
    "                            \"https://www.ebi.ac.uk/pdbe/static/entry/\" + file_name[0:4] + \"-assembly-\" + compos_ID_list[1][1] + \".cif.gz\",\n",
    "                            stream=True)\n",
    "                        if req_child.status_code == requests.codes.ok:\n",
    "                            with open(default_input_path_to_mmCIF_assembly + \"/\" + file_name[0:4] + \"-assembly-\" + compos_ID_list[1][1] + \".cif.gz\",\n",
    "                                      'wb') as f:\n",
    "                                for data in req_child:\n",
    "                                    f.write(data)\n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "\n",
    "\n",
    "def run_downloads_with_ThreadPool(format_to_download=\"mmCIF\", urls_to_target=(),\n",
    "                                  default_input_path_to_mmCIF=current_directory + \"/mmCIF\",\n",
    "                                  default_input_path_to_PDB=current_directory + \"/PDB\",\n",
    "                                  default_input_path_to_SIFTS=current_directory + \"/SIFTS\",\n",
    "                                  default_input_path_to_mmCIF_assembly=current_directory + \"/mmCIF_assembly\",\n",
    "                                  default_input_path_to_PDB_assembly=current_directory + \"/PDB_assembly\"):\n",
    "    \n",
    "\n",
    "    executor = ThreadPoolExecutor()\n",
    "    partial_download_with_pool = partial(download_with_pool,\n",
    "                                         default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "                                         default_input_path_to_PDB=default_input_path_to_PDB,\n",
    "                                         default_input_path_to_SIFTS=default_input_path_to_SIFTS,\n",
    "                                         default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly,\n",
    "                                         default_input_path_to_PDB_assembly=default_input_path_to_PDB_assembly)\n",
    "\n",
    "    jobs = [executor.submit(partial_download_with_pool, url) for url in urls_to_target]\n",
    "\n",
    "    for _ in tqdm.tqdm(as_completed(jobs), total=len(jobs), position=0, leave=True, desc=\"Downloading \" + format_to_download + \" files\"):\n",
    "        pass\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     all_files = latest_catalog_reader()\n",
    "#     all_mmCIF_files = all_files[0]\n",
    "#     all_PDB_files = all_files[1]\n",
    "#     all_SIFTS_files = all_files[2]\n",
    "#\n",
    "#     urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", all_mmCIF_files)\n",
    "#     urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", all_PDB_files)\n",
    "#     urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", all_SIFTS_files)\n",
    "#\n",
    "#     run_downloads_with_ThreadPool(urls_to_target_mmCIF_files)\n",
    "#     run_downloads_with_ThreadPool(urls_to_target_PDB_files)\n",
    "#     run_downloads_with_ThreadPool(urls_to_target_SIFTS_files)\n",
    "#\n",
    "#     download_pdb_assemblies = download_pdb_assemblies_list_with_lxml()\n",
    "#     run_downloads_with_ThreadPool(\"pdb_assembly\", download_pdb_assemblies)\n",
    "#\n",
    "#     urls_to_target_mmCIF_assembly_files = url_formation_for_pool(\"mmCIF_assembly\", all_mmCIF_files)\n",
    "#     run_downloads_with_ThreadPool(\"mmCIF_assembly\", urls_to_target_mmCIF_assembly_files)\n",
    "\n",
    "# uniprot_sprot = gzip.open(\"uniprot_sprot.fasta.gz\", 'rt')\n",
    "# uniprot_translation = dict()\n",
    "# for line in uniprot_sprot:\n",
    "#     if line.startswith(\">sp\"):\n",
    "#         Computer_readble = line.split(\"|\", 2)[1]\n",
    "#         Human_readble = line.split(\"|\", 2)[2].split(\" \", 1)[0]\n",
    "#         uniprot_translation[Computer_readble] = Human_readble\n",
    "        \n",
    "# uniprot_trembl = gzip.open(\"uniprot_trembl.fasta.gz\", 'rt')\n",
    "# uniprot_trembl_translation = dict()\n",
    "# for line in uniprot_trembl:\n",
    "#     if line.startswith(\">tr\"):\n",
    "#         Computer_readble = line.split(\"|\", 2)[1]\n",
    "#         Human_readble = line.split(\"|\", 2)[2].split(\" \", 1)[0]\n",
    "#         uniprot_trembl_translation[Computer_readble] = Human_readble\n",
    "\n",
    "# from UniProt webserver\n",
    "# exception list\n",
    "# GFP_AEQVI\n",
    "# Primary (citable) accession number: P42212\n",
    "# Secondary accession number(s): Q17104, Q27903, Q93125\n",
    "\n",
    "# GCN4_YEAST\n",
    "# Primary (citable) accession number: P03069\n",
    "# Secondary accession number(s): D3DLN9, Q96UT3\n",
    "\n",
    "# C562_ECOLX\n",
    "# Primary (citable) accession number: P0ABE7\n",
    "# Secondary accession number(s): P00192, P76805, Q8XCE3\n",
    "\n",
    "# ENLYS_BPT4\n",
    "# Primary (citable) accession number: P00720\n",
    "# Secondary accession number(s): Q38170, Q94N07\n",
    "\n",
    "# MALE_ECOLI\n",
    "# Primary (citable) accession number: P0AEX9\n",
    "# Secondary accession number(s): P02928, Q2M6S0\n",
    "\n",
    "# exception_AccessionIDs = [\"P42212\", \"Q17104\", \"Q27903\", \"Q93125\", \"P03069\", \"D3DLN9\", \"Q96UT3\", \"P0ABE7\", \"P00192\", \"P76805\", \"Q8XCE3\", \"P00720\", \"Q38170\", \"Q94N07\", \"P0AEX9\", \"P02928\", \"Q2M6S0\"]\n",
    "# GFP_AEQVI GCN4_YEAST C562_ECOLX ENLYS_BPT4 MALE_ECOLI\n",
    "\n",
    "# output_mmCIF_files_were_found = look_what_is_inside('output_mmCIF', default_output_path_to_mmCIF = default_output_path_to_mmCIF)\n",
    "# no_change_list = list()\n",
    "# no_SIFTS_list = list()\n",
    "# no_UniProt_in_SIFTS_list = list()\n",
    "# for n in output_mmCIF_files_were_found:\n",
    "#     if \"_no_change_out\" in n:\n",
    "#         no_change_list.append(n)\n",
    "#     if \"_no_SIFTS\" in n:\n",
    "#         no_SIFTS_list.append(n)\n",
    "#     if \"_no_UniProt\" in n:\n",
    "#         no_UniProt_in_SIFTS_list.append(n)\n",
    "\n",
    "# changed = len(output_mmCIF_files_were_found) - len(no_change_list) - len(no_SIFTS_list) - len(no_UniProt_in_SIFTS_list)\n",
    "# len(no_change_list)\n",
    "# len(no_SIFTS_list)\n",
    "# len(no_UniProt_in_SIFTS_list)\n",
    "# len(output_mmCIF_files_were_found)\n",
    "# changed * 100 / len(output_mmCIF_files_were_found)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry, nothing to read from. Try catalog_downloader() command first.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    all_files = latest_catalog_reader()\n",
    "#     all_mmCIF_files = all_files[0]\n",
    "#     all_PDB_files = all_files[1]\n",
    "#     all_SIFTS_files = all_files[2]\n",
    "\n",
    "# #     urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", all_mmCIF_files)\n",
    "#     urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", all_PDB_files)\n",
    "# #     urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", all_SIFTS_files)\n",
    "\n",
    "# #     run_downloads_with_ThreadPool(urls_to_target_mmCIF_files)\n",
    "#     run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files)\n",
    "# #     run_downloads_with_ThreadPool(urls_to_target_SIFTS_files)\n",
    "#     if not os.path.exists(default_input_path_to_PDB_assembly):\n",
    "#         os.makedirs(default_input_path_to_PDB_assembly)\n",
    "#     download_pdb_assemblies = download_pdb_assemblies_list_with_lxml()\n",
    "#     run_downloads_with_ThreadPool(\"pdb_assembly\", download_pdb_assemblies)\n",
    "\n",
    "#     urls_to_target_mmCIF_assembly_files = url_formation_for_pool(\"mmCIF_assembly\", all_mmCIF_files)\n",
    "#     run_downloads_with_ThreadPool(\"mmCIF_assembly\", urls_to_target_mmCIF_assembly_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading mmCIF/SIFTS catalogs ls-lR: 0.00B [01:58, ?B/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-341fe6c60907>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcatalog_downloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/app/PDBrenum/src/download/catalogdownloader.py\u001b[0m in \u001b[0;36mcatalog_downloader\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere_the_file_goes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere_the_file_goes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mdownloader_for_catalog_with_urllib\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftp_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhere_the_file_goes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere_the_file_goes\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/ls-lR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/app/PDBrenum/src/download/catalogdownloader.py\u001b[0m in \u001b[0;36mdownloader_for_catalog_with_urllib\u001b[0;34m(ftp_to_download, where_the_file_goes)\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munit_scale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Downloading mmCIF/SIFTS catalogs \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlast_slash\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0mreporthook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                 \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mftp_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwhere_the_file_goes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlast_slash\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreporthook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreporthook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    246\u001b[0m     \u001b[0murl_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplittype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 544\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    545\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    502\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m             \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'I'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36mconnect_ftp\u001b[0;34m(self, user, passwd, host, port, dirs, timeout)\u001b[0m\n\u001b[1;32m   1585\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1586\u001b[0m         return ftpwrapper(user, passwd, host, port, dirs, timeout,\n\u001b[0;32m-> 1587\u001b[0;31m                           persistent=False)\n\u001b[0m\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1589\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mCacheFTPHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFTPHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user, passwd, host, port, dirs, timeout, persistent)\u001b[0m\n\u001b[1;32m   2406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeepalive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2407\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2409\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2410\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbusy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2416\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mftplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFTP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2417\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2418\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasswd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m         \u001b[0m_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/ftplib.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, host, port, timeout, source_address)\u001b[0m\n\u001b[1;32m    150\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msource_address\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         self.sock = socket.create_connection((self.host, self.port), self.timeout,\n\u001b[0;32m--> 152\u001b[0;31m                                              source_address=self.source_address)\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/PDBrenum/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    712\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    714\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "catalog_downloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13.7M/13.7M [00:09<00:00, 1.45MiB/s]\n",
      "44.7MiB [00:58, 769kiB/s]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from tqdm import tqdm\n",
    "import gzip\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_and_load_json_gz(url, filename):\n",
    "    # Download the .gz file\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    \n",
    "    if total_size != 0 and progress_bar.n != total_size:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "    \n",
    "    # Decompress the .gz file and load the JSON content\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        json_bytes = f.read()  # Read the entire file\n",
    "    \n",
    "    # Parse the JSON content\n",
    "    json_str = json_bytes.decode('utf-8')\n",
    "    json_data = json.loads(json_str)\n",
    "    \n",
    "    return json_data\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "url = \"https://files.wwpdb.org/pub/pdb/holdings/current_file_holdings.json.gz\"\n",
    "filename = \"current_file_holdings.json.gz\"\n",
    "holdings = download_and_load_json_gz(url, filename)\n",
    "\n",
    "mmcif_l = []\n",
    "pdb_l = []\n",
    "assembly_mmcif_l = []\n",
    "assembly_pdb_l = []\n",
    "\n",
    "for pdbid in holdings.keys():\n",
    "    try:\n",
    "        mmcif_l.extend(holdings[pdbid][\"mmcif\"])\n",
    "        pdb_l.extend(holdings[pdbid][\"pdb\"])\n",
    "        assembly_mmcif_l.extend(holdings[pdbid][\"assembly_mmcif\"])\n",
    "        assembly_pdb_l.extend(holdings[pdbid][\"assembly_pdb\"])\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "def download_file_with_progress_bar(url, filename):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL with a progress bar.\n",
    "\n",
    "    Parameters:\n",
    "    - url: The URL of the file to download.\n",
    "    - filename: The filename (with path) where to save the downloaded file.\n",
    "    \"\"\"\n",
    "    # Send a HTTP request to the server and fetch meta information about the file\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Total size in bytes.\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "    \n",
    "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    \n",
    "    if total_size != 0 and progress_bar.n != total_size:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "        \n",
    "download_file_with_progress_bar(\"https://ftp.ebi.ac.uk/pub/databases/msd/sifts/xml\", 'xml')\n",
    "\n",
    "\n",
    "with open(\"xml\") as xml:\n",
    "    xmlr = xml.read()\n",
    "    \n",
    "# Sample HTML content\n",
    "html_content = xmlr\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'lxml')  # or 'html.parser' if lxml is not available\n",
    "\n",
    "# Extract the .xml.gz links\n",
    "xml_gz_files = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.xml.gz')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_with_progress_bar(url, filename):\n",
    "    \"\"\"\n",
    "    Downloads a file from the given URL with a progress bar.\n",
    "\n",
    "    Parameters:\n",
    "    - url: The URL of the file to download.\n",
    "    - filename: The filename (with path) where to save the downloaded file.\n",
    "    \"\"\"\n",
    "    # Send a HTTP request to the server and fetch meta information about the file\n",
    "    response = requests.get(url, stream=True)\n",
    "    \n",
    "    # Total size in bytes.\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024  # 1 Kibibyte\n",
    "    \n",
    "    progress_bar = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
    "    \n",
    "    with open(filename, 'wb') as file:\n",
    "        for data in response.iter_content(block_size):\n",
    "            progress_bar.update(len(data))\n",
    "            file.write(data)\n",
    "    progress_bar.close()\n",
    "    \n",
    "    if total_size != 0 and progress_bar.n != total_size:\n",
    "        print(\"ERROR, something went wrong\")\n",
    "        \n",
    "download_file_with_progress_bar(\"https://ftp.ebi.ac.uk/pub/databases/msd/sifts/xml\", 'xml')\n",
    "\n",
    "\n",
    "with open(\"xml\") as xml:\n",
    "    xmlr = xml.read()\n",
    "    \n",
    "# Sample HTML content\n",
    "html_content = xmlr\n",
    "\n",
    "# Parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'lxml')  # or 'html.parser' if lxml is not available\n",
    "\n",
    "# Extract the .xml.gz links\n",
    "xml_gz_files = [a['href'] for a in soup.find_all('a', href=True) if a['href'].endswith('.xml.gz')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################################################################################################################################\n",
    "# DEPENDENCIES #\n",
    "#####################################################################################################################################################\n",
    "\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from src.download import lefttorenumber\n",
    "from src.download.inputtextfileparser import input_text_file_parser\n",
    "from src.download.shortusagemessage import short_usage_messenger\n",
    "from src.download.longusagemessage import long_usage_messenger\n",
    "from src.download.supremedownloader import supreme_download_master\n",
    "from src.download.lookfilesinside import look_what_is_inside\n",
    "\n",
    "\n",
    "from src.renum.PDB.renumPDB import master_PDB_renumber_function\n",
    "from src.renum.mmCIF.ProcessPool_run_renum import ProcessPool_run_renum\n",
    "from src.renum.write_log import log_writer\n",
    "from downloadwithThreadPool import run_downloads_with_ThreadPool, url_formation_for_pool\n",
    "\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "exception_AccessionIDs = [\"P42212\", \"Q17104\", \"Q27903\", \"Q93125\", \"P03069\", \"D3DLN9\", \"Q96UT3\", \"P0ABE7\", \"P00192\",\n",
    "                          \"P76805\", \"Q8XCE3\", \"P00720\", \"Q38170\", \"Q94N07\", \"P0AEX9\", \"P02928\", \"Q2M6S0\"]\n",
    "\n",
    "#####################################################################################################################################################\n",
    "# ARGUMENTS #\n",
    "#####################################################################################################################################################\n",
    "\n",
    "\n",
    "argpar = argparse.ArgumentParser(usage=short_usage_messenger(), add_help=False)\n",
    "\n",
    "argpar.add_argument(\"-h\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"--help\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-rftf\", \"--renumber_from_text_file\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-rfla\", \"--renumber_from_list_of_arguments\", metavar=\"6dbp 3v03 2jit\", nargs=\"*\", type=str, help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-dftf\", \"--download_from_text_file\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-dfla\", \"--download_from_list_of_arguments\", metavar=\"6dbp 3v03 2jit\", nargs=\"+\", type=str, help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-redb\", \"--renumber_entire_database\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-dall\", \"--download_entire_database\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-refr\", \"--refresh_entire_database\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-PDB\", \"--PDB_format_only\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-mmCIF\", \"--mmCIF_format_only\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-PDB_assembly\", \"--PDB_assembly_format_only\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-mmCIF_assembly\", \"--mmCIF_assembly_format_only\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-all\", \"--all_formats\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-sipm\", \"--set_default_input_path_to_mmCIF\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sipma\", \"--set_default_input_path_to_mmCIF_assembly\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sipp\", \"--set_default_input_path_to_PDB\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sippa\", \"--set_default_input_path_to_PDB_assembly\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sips\", \"--set_default_input_path_to_SIFTS\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sopm\", \"--set_default_output_path_to_mmCIF\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sopma\", \"--set_default_output_path_to_mmCIF_assembly\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sopp\", \"--set_default_output_path_to_PDB\", type=str, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-soppa\", \"--set_default_output_path_to_PDB_assembly\", type=str, help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-sdmn\", \"--set_default_mmCIF_num\", type=int, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-sdpn\", \"--set_default_PDB_num\", type=int, help=argparse.SUPPRESS)\n",
    "\n",
    "argpar.add_argument(\"-nproc\", \"--set_number_of_processes\", type=int, help=argparse.SUPPRESS)\n",
    "argpar.add_argument(\"-offz\", \"--set_to_off_mode_gzip\", action=\"store_true\", help=argparse.SUPPRESS)\n",
    "\n",
    "\n",
    "args = argpar.parse_args()\n",
    "\n",
    "\n",
    "#####################################################################################################################################################\n",
    "# FLAGS #\n",
    "#####################################################################################################################################################\n",
    "\n",
    "\n",
    "if args.help:\n",
    "    print(long_usage_messenger())\n",
    "\n",
    "if args.h:\n",
    "    print(short_usage_messenger())\n",
    "\n",
    "if args.set_default_input_path_to_mmCIF:\n",
    "    default_input_path_to_mmCIF = args.set_default_input_path_to_mmCIF\n",
    "else:\n",
    "    default_input_path_to_mmCIF = current_directory + \"/mmCIF\"\n",
    "\n",
    "if args.set_default_input_path_to_mmCIF_assembly:\n",
    "    default_input_path_to_mmCIF_assembly = args.set_default_input_path_to_mmCIF_assembly\n",
    "else:\n",
    "    default_input_path_to_mmCIF_assembly = current_directory + \"/mmCIF_assembly\"\n",
    "\n",
    "if args.set_default_input_path_to_PDB:\n",
    "    default_input_path_to_PDB = args.set_default_input_path_to_PDB\n",
    "else:\n",
    "    default_input_path_to_PDB = current_directory + \"/PDB\"\n",
    "\n",
    "if args.set_default_input_path_to_PDB_assembly:\n",
    "    default_input_path_to_PDB_assembly = args.set_default_input_path_to_PDB_assembly\n",
    "else:\n",
    "    default_input_path_to_PDB_assembly = current_directory + \"/PDB_assembly\"\n",
    "\n",
    "if args.set_default_input_path_to_SIFTS:\n",
    "    default_input_path_to_SIFTS = args.set_default_input_path_to_SIFTS\n",
    "else:\n",
    "    default_input_path_to_SIFTS = current_directory + \"/SIFTS\"\n",
    "\n",
    "if args.set_default_output_path_to_mmCIF:\n",
    "    default_output_path_to_mmCIF = args.set_default_output_path_to_mmCIF\n",
    "else:\n",
    "    default_output_path_to_mmCIF = current_directory + \"/output_mmCIF\"\n",
    "\n",
    "if args.set_default_output_path_to_mmCIF_assembly:\n",
    "    default_output_path_to_mmCIF_assembly = args.set_default_output_path_to_mmCIF_assembly\n",
    "else:\n",
    "    default_output_path_to_mmCIF_assembly = current_directory + \"/output_mmCIF_assembly\"\n",
    "\n",
    "if args.set_default_output_path_to_PDB:\n",
    "    default_output_path_to_PDB = args.set_default_output_path_to_PDB\n",
    "else:\n",
    "    default_output_path_to_PDB = current_directory + \"/output_PDB\"\n",
    "\n",
    "if args.set_default_output_path_to_PDB_assembly:\n",
    "    default_output_path_to_PDB_assembly = args.set_default_output_path_to_PDB_assembly\n",
    "else:\n",
    "    default_output_path_to_PDB_assembly = current_directory + \"/output_PDB_assembly\"\n",
    "\n",
    "if args.set_default_mmCIF_num:\n",
    "    default_mmCIF_num = args.set_default_mmCIF_num\n",
    "else:\n",
    "    default_mmCIF_num = 50000\n",
    "\n",
    "if args.set_default_PDB_num:\n",
    "    default_PDB_num = args.set_default_PDB_num\n",
    "else:\n",
    "    default_PDB_num = 5000\n",
    "\n",
    "if args.set_to_off_mode_gzip:\n",
    "    gzip_mode = \"off\"\n",
    "else:\n",
    "    gzip_mode = \"on\"\n",
    "\n",
    "if args.set_number_of_processes:\n",
    "    nproc = args.set_number_of_processes\n",
    "else:\n",
    "    nproc = None\n",
    "\n",
    "\n",
    "#####################################################################################################################################################\n",
    "# PARTIAL DB WORK #\n",
    "#####################################################################################################################################################\n",
    "\n",
    "# RENUMBER\n",
    "# RENUMBER FROM TEXT FILE or RENUMBER FROM LIST OF ARGUMENTS\n",
    "if args.renumber_from_text_file or args.renumber_from_list_of_arguments:\n",
    "    if args.renumber_from_text_file:\n",
    "        parsed_input_text = (input_text_file_parser(args.renumber_from_text_file))\n",
    "    else:\n",
    "        parsed_input_text = args.renumber_from_list_of_arguments\n",
    "\n",
    "    if args.all_formats:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", parsed_input_text, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", parsed_input_text, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        passed_as_arg_file_4Char_PDB = list()\n",
    "        for file_name in parsed_input_text:\n",
    "            passed_as_arg_file_4Char_PDB.append(file_name[:4])\n",
    "\n",
    "        input_PDB_files_were_found = look_what_is_inside(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        target_files_list_PDB = list()\n",
    "        for file_name in input_PDB_files_were_found:\n",
    "            if file_name[3:7] in passed_as_arg_file_4Char_PDB:\n",
    "                target_files_list_PDB.append(file_name)\n",
    "        master_PDB_renumber_function(target_files_list_PDB, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "                                     default_output_path_to_PDB, default_PDB_num, gzip_mode)\n",
    "\n",
    "        input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        passed_as_arg_file_4Char_mmCIF = list()\n",
    "        for file_name in parsed_input_text:\n",
    "            passed_as_arg_file_4Char_mmCIF.append(file_name[:4])\n",
    "\n",
    "        target_files_list_mmCIF = list()\n",
    "        for file_name in input_mmCIF_files_were_found:\n",
    "            if file_name[:4] in passed_as_arg_file_4Char_mmCIF:\n",
    "                target_files_list_mmCIF.append(file_name)\n",
    "\n",
    "        if not os.path.exists(default_output_path_to_mmCIF):\n",
    "            os.makedirs(default_output_path_to_mmCIF)\n",
    "        res = ProcessPool_run_renum(\"mmCIF\", target_files_list_mmCIF, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "    elif args.PDB_format_only:\n",
    "        urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", parsed_input_text, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        input_PDB_files_were_found = look_what_is_inside(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        passed_as_arg_file_4Char_PDB = list()\n",
    "        for file_name in parsed_input_text:\n",
    "            passed_as_arg_file_4Char_PDB.append(file_name[:4])\n",
    "\n",
    "        target_files_list_PDB = list()\n",
    "        for file_name in input_PDB_files_were_found:\n",
    "            if file_name[3:7] in passed_as_arg_file_4Char_PDB:\n",
    "                target_files_list_PDB.append(file_name)\n",
    "        master_PDB_renumber_function(target_files_list_PDB, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "                                     default_output_path_to_PDB, default_PDB_num, gzip_mode)\n",
    "\n",
    "    elif args.mmCIF_assembly_format_only:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF_assembly\", parsed_input_text,\n",
    "                                                            default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"mmCIF_assembly\", urls_to_target_mmCIF_files,\n",
    "                                      default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF_assembly\",\n",
    "                                                           default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "        passed_as_args_files_list_4Char = list()\n",
    "        for file_name in parsed_input_text:\n",
    "            passed_as_args_files_list_4Char.append(file_name[:4])\n",
    "\n",
    "        target_files_list = list()\n",
    "        for file_name in input_mmCIF_files_were_found:\n",
    "            if file_name[:4] in passed_as_args_files_list_4Char:\n",
    "                target_files_list.append(file_name)\n",
    "\n",
    "        if not os.path.exists(default_output_path_to_mmCIF_assembly):\n",
    "            os.makedirs(default_output_path_to_mmCIF_assembly)\n",
    "        res = ProcessPool_run_renum(\"mmCIF_assembly\", target_files_list, default_input_path_to_mmCIF_assembly, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF_assembly, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "    else:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", parsed_input_text, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        passed_as_args_files_list_4Char = list()\n",
    "        for file_name in parsed_input_text:\n",
    "            passed_as_args_files_list_4Char.append(file_name[:4])\n",
    "\n",
    "        target_files_list = list()\n",
    "        for file_name in input_mmCIF_files_were_found:\n",
    "            if file_name[:4] in passed_as_args_files_list_4Char:\n",
    "                target_files_list.append(file_name)\n",
    "\n",
    "        if not os.path.exists(default_output_path_to_mmCIF):\n",
    "            os.makedirs(default_output_path_to_mmCIF)\n",
    "        res = ProcessPool_run_renum(\"mmCIF\", target_files_list, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "# RENUMBER FROM LIST OF ARGUMENTS\n",
    "# if args.renumber_from_list_of_arguments:\n",
    "#     if args.all_formats:\n",
    "#\n",
    "#         urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\",  args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "#         urls_to_target_PDB_files = url_formation_for_pool(\"PDB\",  args.renumber_from_list_of_arguments,\n",
    "#                                                           default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "#         urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\",  args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#\n",
    "#         run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "#         run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "#         run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#\n",
    "#         input_PDB_files_were_found = look_what_is_inside(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "#         passed_as_arg_file_4Char_PDB = list()\n",
    "#         for file_name in args.renumber_from_list_of_arguments:\n",
    "#             passed_as_arg_file_4Char_PDB.append(file_name[:4])\n",
    "#\n",
    "#         target_files_list_PDB = list()\n",
    "#         for file_name in input_PDB_files_were_found:\n",
    "#             if file_name[3:7] in passed_as_arg_file_4Char_PDB:\n",
    "#                 target_files_list_PDB.append(file_name)\n",
    "#         master_PDB_renumber_function(target_files_list_PDB, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "#                                      default_output_path_to_PDB, default_PDB_num, gzip_mode)\n",
    "#\n",
    "#         input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "#         passed_as_arg_file_4Char_mmCIF = list()\n",
    "#         for file_name in args.renumber_from_list_of_arguments:\n",
    "#             passed_as_arg_file_4Char_mmCIF.append(file_name[:4])\n",
    "#\n",
    "#         target_files_list_mmCIF = list()\n",
    "#         for file_name in input_mmCIF_files_were_found:\n",
    "#             if file_name[:4] in passed_as_arg_file_4Char_mmCIF:\n",
    "#                 target_files_list_mmCIF.append(file_name)\n",
    "#\n",
    "#         if not os.path.exists(default_output_path_to_mmCIF):\n",
    "#             os.makedirs(default_output_path_to_mmCIF)\n",
    "#         res = ProcessPool_run_renum(\"mmCIF\", target_files_list_mmCIF, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "#                                     default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "#         log_writer(res)\n",
    "#\n",
    "#     elif args.PDB_format_only:\n",
    "#         urls_to_target_PDB_files = url_formation_for_pool(\"PDB\",  args.renumber_from_list_of_arguments,\n",
    "#                                                           default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "#         urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\",  args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#\n",
    "#         run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "#         run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#\n",
    "#         input_PDB_files_were_found = look_what_is_inside(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "#         passed_as_arg_file_4Char_PDB = list()\n",
    "#         for file_name in args.renumber_from_list_of_arguments:\n",
    "#             passed_as_arg_file_4Char_PDB.append(file_name[:4])\n",
    "#\n",
    "#         target_files_list_PDB = list()\n",
    "#         for file_name in input_PDB_files_were_found:\n",
    "#             if file_name[3:7] in passed_as_arg_file_4Char_PDB:\n",
    "#                 target_files_list_PDB.append(file_name)\n",
    "#         master_PDB_renumber_function(target_files_list_PDB, default_input_path_to_PDB, default_input_path_to_SIFTS,\n",
    "#                                      default_output_path_to_PDB, default_PDB_num, gzip_mode)\n",
    "#\n",
    "#     elif args.mmCIF_assembly_format_only:\n",
    "#         urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF_assembly\", args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "#         urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#\n",
    "#         run_downloads_with_ThreadPool(\"mmCIF_assembly\", urls_to_target_mmCIF_files,\n",
    "#                                       default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "#         run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#\n",
    "#         input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF_assembly\",\n",
    "#                                                            default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly)\n",
    "#         passed_as_args_files_list_4Char = list()\n",
    "#         for file_name in args.renumber_from_list_of_arguments:\n",
    "#             passed_as_args_files_list_4Char.append(file_name[:4])\n",
    "#\n",
    "#         target_files_list = list()\n",
    "#         for file_name in input_mmCIF_files_were_found:\n",
    "#             if file_name[:4] in passed_as_args_files_list_4Char:\n",
    "#                 target_files_list.append(file_name)\n",
    "#\n",
    "#         if not os.path.exists(default_output_path_to_mmCIF_assembly):\n",
    "#             os.makedirs(default_output_path_to_mmCIF_assembly)\n",
    "#         res = ProcessPool_run_renum(\"mmCIF_assembly\", target_files_list, default_input_path_to_mmCIF_assembly, default_input_path_to_SIFTS,\n",
    "#                                     default_output_path_to_mmCIF_assembly, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "#         log_writer(res)\n",
    "#\n",
    "#     else:\n",
    "#         urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\",  args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "#         urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\",  args.renumber_from_list_of_arguments,\n",
    "#                                                             default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#         run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "#         run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "#         input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "#         passed_as_args_files_list_4Char = list()\n",
    "#         for file_name in args.renumber_from_list_of_arguments:\n",
    "#             passed_as_args_files_list_4Char.append(file_name[:4])\n",
    "#\n",
    "#         target_files_list = list()\n",
    "#         for file_name in input_mmCIF_files_were_found:\n",
    "#             if file_name[:4] in passed_as_args_files_list_4Char:\n",
    "#                 target_files_list.append(file_name)\n",
    "#\n",
    "#         if not os.path.exists(default_output_path_to_mmCIF):\n",
    "#             os.makedirs(default_output_path_to_mmCIF)\n",
    "#         res = ProcessPool_run_renum(\"mmCIF\", target_files_list, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "#                                     default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "#         log_writer(res)\n",
    "\n",
    "# DOWNLOAD\n",
    "# DOWNLOAD FROM TEXT FILE\n",
    "if args.download_from_text_file:\n",
    "    parsed_input_text = (input_text_file_parser(args.download_from_text_file))\n",
    "    if args.all_formats:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", parsed_input_text, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", parsed_input_text, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    elif args.PDB_format_only:\n",
    "        urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", parsed_input_text, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    else:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", parsed_input_text, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", parsed_input_text, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "# DOWNLOAD FROM LIST\n",
    "if args.download_from_list_of_arguments:\n",
    "    if args.all_formats:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\",  args.download_from_list_of_arguments,\n",
    "                                                            default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        urls_to_target_PDB_files = url_formation_for_pool(\"PDB\",  args.download_from_list_of_arguments,\n",
    "                                                          default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\",  args.download_from_list_of_arguments,\n",
    "                                                            default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    elif args.PDB_format_only:\n",
    "        urls_to_target_PDB_files = url_formation_for_pool(\"PDB\", args.download_from_list_of_arguments,\n",
    "                                                          default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", args.download_from_list_of_arguments,\n",
    "                                                            default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        run_downloads_with_ThreadPool(\"PDB\", urls_to_target_PDB_files, default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    else:\n",
    "        urls_to_target_mmCIF_files = url_formation_for_pool(\"mmCIF\", args.download_from_list_of_arguments,\n",
    "                                                            default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        urls_to_target_SIFTS_files = url_formation_for_pool(\"SIFTS\", args.download_from_list_of_arguments,\n",
    "                                                            default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        run_downloads_with_ThreadPool(\"mmCIF\", urls_to_target_mmCIF_files, default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        run_downloads_with_ThreadPool(\"SIFTS\", urls_to_target_SIFTS_files, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "\n",
    "#####################################################################################################################################################\n",
    "# WHOLE DB WORK #\n",
    "#####################################################################################################################################################\n",
    "\n",
    "# RENUMBER ENTIRE DB\n",
    "if args.renumber_entire_database:\n",
    "    if args.all_formats:\n",
    "        print(\"Starting to renumber tree databases...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF, \n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        input_mmCIF_files_were_found = look_what_is_inside(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF)\n",
    "        mmCIF_files_left_to_renumber = lefttorenumber.left_to_renumber_mmCIF()\n",
    "\n",
    "        res = ProcessPool_run_renum(\"mmCIF\", mmCIF_files_left_to_renumber, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "        supreme_download_master(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        input_PDB_files_were_found = look_what_is_inside(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        PDB_files_left_to_renumber = lefttorenumber.left_to_renumber_PDB()\n",
    "\n",
    "        master_PDB_renumber_function(PDB_files_left_to_renumber, default_input_path_to_PDB, default_input_path_to_SIFTS, default_output_path_to_PDB,\n",
    "                                     default_PDB_num, gzip_mode)\n",
    "    elif args.PDB_format_only:\n",
    "        print(\"Starting to renumber entire PDB database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB, default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        input_PDB_files_were_found = look_what_is_inside(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB)\n",
    "        PDB_files_left_to_renumber = lefttorenumber.left_to_renumber_PDB()\n",
    "\n",
    "        master_PDB_renumber_function(PDB_files_left_to_renumber, default_input_path_to_PDB, default_input_path_to_SIFTS, default_output_path_to_PDB,\n",
    "                                     default_PDB_num, gzip_mode)\n",
    "\n",
    "    elif args.mmCIF_assembly_format_only:\n",
    "        print(\"Starting to renumber entire mmCIF_assembly database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"mmCIF_assembly\", default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly,\n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        mmCIF_assembly_left_to_renumber = lefttorenumber.left_to_renumber_mmCIF(default_input_path_to_mmCIF=default_input_path_to_mmCIF_assembly,\n",
    "                                                                                default_output_path_to_mmCIF=default_output_path_to_mmCIF_assembly)\n",
    "\n",
    "        res = ProcessPool_run_renum(\"mmCIF_assembly\", mmCIF_assembly_left_to_renumber, default_input_path_to_mmCIF_assembly,\n",
    "                                    default_input_path_to_SIFTS, default_output_path_to_mmCIF_assembly, default_mmCIF_num,\n",
    "                                    exception_AccessionIDs, gzip_mode, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "    else:\n",
    "        print(\"Starting to renumber entire mmCIF database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF, \n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        mmCIF_files_left_to_renumber = lefttorenumber.left_to_renumber_mmCIF(default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "                                                                             default_output_path_to_mmCIF=default_output_path_to_mmCIF)\n",
    "\n",
    "        res = ProcessPool_run_renum(\"mmCIF\", mmCIF_files_left_to_renumber, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "# DOWNLOAD ENTIRE DB\n",
    "if args.download_entire_database:\n",
    "    if args.all_formats:\n",
    "        print(\"Starting to download tree databases...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"all\", default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "                                default_input_path_to_PDB=default_input_path_to_PDB,\n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS,\n",
    "                                default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly,\n",
    "                                default_input_path_to_PDB_assembly=default_input_path_to_PDB_assembly)\n",
    "\n",
    "    elif args.PDB_format_only:\n",
    "        print(\"Starting to download entire PDB database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"PDB\", default_input_path_to_PDB=default_input_path_to_PDB,\n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    elif args.mmCIF_assembly_format_only:\n",
    "        print(\"Starting to download entire mmCIF_assembly database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"mmCIF_assembly\", default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly,\n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    elif args.PDB_assembly_format_only:\n",
    "        print(\"Starting to download entire PDB_assembly database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"PDB_assembly\", default_input_path_to_PDB_assembly=default_input_path_to_PDB_assembly,\n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "    else:\n",
    "        print(\"Starting to download entire mmCIF database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        supreme_download_master(\"mmCIF\", default_input_path_to_mmCIF=default_input_path_to_mmCIF, \n",
    "                                default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "# REFRESH ENTIRE DB\n",
    "if args.refresh_entire_database:\n",
    "    if args.all_formats:\n",
    "        print(\"Starting to refresh tree databases...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        left_to_refresh = supreme_download_master(\"all\", \"refresh\", default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "                                                  default_input_path_to_PDB=default_input_path_to_PDB,\n",
    "                                                  default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        left_to_refresh_mmCIF = left_to_refresh[0]\n",
    "        left_to_refresh_PDB = left_to_refresh[1]\n",
    "        master_PDB_renumber_function(left_to_refresh_PDB, default_input_path_to_PDB, default_input_path_to_SIFTS, default_output_path_to_PDB,\n",
    "                                     default_PDB_num, gzip_mode)\n",
    "        res = ProcessPool_run_renum(\"mmCIF\", left_to_refresh_mmCIF, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "    elif args.PDB_format_only:\n",
    "        print(\"Starting to refresh entire PDB database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        left_to_refresh_PDB = supreme_download_master(\"PDB\", \"refresh\", default_input_path_to_PDB=default_input_path_to_PDB,\n",
    "                                                      default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        master_PDB_renumber_function(left_to_refresh_PDB, default_input_path_to_PDB, default_input_path_to_SIFTS, default_output_path_to_PDB,\n",
    "                                     default_PDB_num, gzip_mode)\n",
    "\n",
    "    elif args.PDB_assembly_format_only:\n",
    "        print(\"Starting to refresh entire PDB database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        left_to_refresh_PDB_assembly = supreme_download_master(\"PDB_assembly\", \"refresh\",\n",
    "                                                               default_input_path_to_PDB_assembly=default_input_path_to_PDB_assembly,\n",
    "                                                               default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        master_PDB_renumber_function(left_to_refresh_PDB_assembly, default_input_path_to_PDB_assembly, default_input_path_to_SIFTS,\n",
    "                                     default_output_path_to_PDB_assembly, default_PDB_num, gzip_mode)\n",
    "\n",
    "    elif args.mmCIF_assembly_format_only:\n",
    "        print(\"Starting to refresh entire mmCIF_assembly database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        left_to_refresh_mmCIF_assembly = supreme_download_master(\"mmCIF_assembly\", \"refresh\",\n",
    "                                                                 default_input_path_to_mmCIF_assembly=default_input_path_to_mmCIF_assembly,\n",
    "                                                                 default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "\n",
    "        res = ProcessPool_run_renum(\"mmCIF_assembly\", left_to_refresh_mmCIF_assembly, default_input_path_to_mmCIF_assembly,\n",
    "                                    default_input_path_to_SIFTS, default_output_path_to_mmCIF_assembly, default_mmCIF_num,\n",
    "                                    gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n",
    "\n",
    "    else:\n",
    "        print(\"Starting to refresh entire mmCIF database...\")\n",
    "        print(\"Please, be patient...\")\n",
    "        left_to_refresh_mmCIF = supreme_download_master(\"mmCIF\", \"refresh\", default_input_path_to_mmCIF=default_input_path_to_mmCIF,\n",
    "                                                        default_input_path_to_SIFTS=default_input_path_to_SIFTS)\n",
    "        res = ProcessPool_run_renum(\"mmCIF\", left_to_refresh_mmCIF, default_input_path_to_mmCIF, default_input_path_to_SIFTS,\n",
    "                                    default_output_path_to_mmCIF, default_mmCIF_num, gzip_mode, exception_AccessionIDs, nproc)\n",
    "        log_writer(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
